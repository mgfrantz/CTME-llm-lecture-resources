{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mgfrantz/CTME-llm-lecture-resources/blob/main/labs/fine_tuning_and_inference_with_axolotl.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine tuning and inference\n",
        "\n",
        "In this lab, we're going to demonstrate the process of fine tuning.\n",
        "Note that this **does not mean we will have a better, cheaper, or faster model than API providers.**\n",
        "Given the small amnount of data we have and the time/resource constraints of this lab setting, we can't shoot for a highly performant model.\n",
        "But we will have a **general design pattern for fine tuning**, and we will see that the fine tuned model is not as bad as the base model for fine tuning.\n",
        "\n",
        "[Axolotl](https://github.com/axolotl-ai-cloud/axolotl) is a convenient library that helps fine tune text generation models.\n",
        "In this notebook, we will use `axolotl` to fine tune a small LLM on a dataset we've created.\n",
        "\n",
        "We will be using the small and open Llama 3.2 1b. model today.\n",
        "Here's are agenda:\n",
        "\n",
        "- Run the model in the notebook to demonstrate that it cannot do anything we want it to do out of the box.\n",
        "- Load the conversations we generated yesterday and prepare them for training by converting them into the ChatML format and tokenizing them.\n",
        "- Fine-tune the llama model using QLoRA\n",
        "- Export the model to .gguf so we can run it anywhere with `ollama` or `llama.cpp`\n",
        "- Test our model in our agent to demonstrate that it is better than the base model\n",
        "- Push our model artifacts to Huggingface so they can be run anywhere"
      ],
      "metadata": {
        "id": "KCr4O0fXR_rJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "Nnjlha4NSwDN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ollama\n",
        "\n",
        "We will be using ollama for local inference.\n",
        "To set it up, open the Colab terminal â†™.\n",
        "Then, run the following commands:\n",
        "\n",
        "```\n",
        "curl -fsSL https://ollama.com/install.sh | sh # install ollama\n",
        "ollama serve &                                # start the ollama server and return the terminal\n",
        "ollama pull llama3.2:3b                       # pull llama3.2:1b (the model we'll be fine-tuning)\n",
        "```\n",
        "\n",
        "This will pull the model we'll be testing against."
      ],
      "metadata": {
        "id": "412bg_zxSfm1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installs"
      ],
      "metadata": {
        "id": "Eet-v8uMdQuD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qqqq \\\n",
        "    huggingface_hub \\\n",
        "    bitsandbytes \\\n",
        "    accelerate \\\n",
        "    \"transformers[torch]\" \\\n",
        "    llama-cpp-python \\\n",
        "    vllm \\\n",
        "    \"llama-index>=0.11.17\" \\\n",
        "    \"llama-index-core>=0.10.43\" \\\n",
        "    \"openinference-instrumentation-llama-index>=2\" \\\n",
        "    \"opentelemetry-proto>=1.12.0\" \\\n",
        "    arize-phoenix-otel \\\n",
        "    nest-asyncio \\\n",
        "    llama-index-callbacks-arize-phoenix \\\n",
        "    llama-index-readers-database \\\n",
        "    llama-index-llms-openai \\\n",
        "    llama-index-embeddings-fastembed \\\n",
        "    llama-index-readers-database \\\n",
        "    fastembed-gpu \\\n",
        "    llama-index-llms-ollama \\\n",
        "    llama-index-agent-openai \\\n",
        "    --progress-bar off\n",
        "\n",
        "# Clone llama.cpp for conversion to gguf\n",
        "!git clone https://github.com/ggerganov/llama.cpp.git"
      ],
      "metadata": {
        "id": "eO4yVPCjs3Ej"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "UPDY2usLePC7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sqlite3\n",
        "import pandas as pd\n",
        "from IPython.display import display\n",
        "from rich import print\n",
        "from typing import Literal, List\n",
        "from pydantic import BaseModel, Field\n",
        "from time import sleep\n",
        "from sqlalchemy import create_engine\n",
        "from google.colab import userdata\n",
        "import json\n",
        "import os\n",
        "import phoenix as px\n",
        "from openinference.instrumentation.llama_index import LlamaIndexInstrumentor\n",
        "from openinference.instrumentation import using_metadata\n",
        "from phoenix.otel import register\n",
        "from enum import Enum\n",
        "from tqdm.auto import tqdm\n",
        "from llama_index.core import VectorStoreIndex, Document\n",
        "from llama_index.core.tools import FunctionTool, QueryEngineTool, RetrieverTool\n",
        "from llama_index.core.agent import ReActAgent\n",
        "from llama_index.core.llms import ChatMessage\n",
        "from llama_index.core import PromptTemplate\n",
        "from llama_index.readers.database import DatabaseReader\n",
        "from llama_index.embeddings.fastembed import FastEmbedEmbedding\n",
        "from llama_index.core.program import LLMTextCompletionProgram\n",
        "from llama_index.llms.ollama import Ollama\n",
        "# from llama_index.llms.huggingface import HuggingFaceLLM\n",
        "# from llama_index.llms.llama_cpp import LlamaCPP\n",
        "# from llama_index.llms.vllm import Vllm\n",
        "from transformers import BitsAndBytesConfig\n",
        "import asyncio\n",
        "import nest_asyncio\n",
        "\n",
        "token = userdata.get('HF_TOKEN')\n",
        "os.environ['HF_TOKEN'] = token"
      ],
      "metadata": {
        "id": "JQIM4p7FdPl_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## All the code to set up our agent ðŸ‘‡\n",
        "\n",
        "Since we have all the code we used to run our agent yesterday, let's bring that over so we can use it in this notebook.\n",
        "Here is the solution code, but if your code differs feel free to replace it with your own."
      ],
      "metadata": {
        "id": "bsyhE9pt0I2x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if not os.path.exists('drive/MyDrive/CTME-LLM-labs/ecommerce.db'):\n",
        "    print(\"The ecommerce.db database does not exist. Please make sure you're connected to Google Drive or upload it to the Colab notebook or re-run lesson 1.\")\n",
        "!cp drive/MyDrive/CTME-LLM-labs/ecommerce.db ."
      ],
      "metadata": {
        "id": "loBqq6IZz80c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def query(q:str, db:str='ecommerce.db') -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Executes a SQL query against the SQLite database and returns the result as a pandas DataFrame.\n",
        "    Use this function when you want to query a database and return results.\n",
        "\n",
        "    Args:\n",
        "        q (str): The SQL query to execute.\n",
        "        db (str, optional): The path to the SQLite database file. Defaults to 'ecommerce.db'.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The result of the SQL query as a pandas DataFrame.\n",
        "    \"\"\"\n",
        "    connection = sqlite3.connect(db)\n",
        "    cursor = connection.cursor()\n",
        "    cursor.execute(q)\n",
        "    result = cursor.fetchall()\n",
        "    df = pd.DataFrame(result)\n",
        "    df.columns = [i[0] for i in cursor.description]\n",
        "    connection.close()\n",
        "    return df\n",
        "\n",
        "def execute(q:str, db:str='ecommerce.db') -> None:\n",
        "    \"\"\"\n",
        "    Executes an SQL query against the SQLite database.\n",
        "    Use this when you want to run commands like updates, inserts, or deletes that don't return results.\n",
        "\n",
        "    Args:\n",
        "        q (str): The SQL query to execute.\n",
        "        db (str, optional): The path to the SQLite database file. Defaults to 'ecommerce.db'.\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    connection = sqlite3.connect(db)\n",
        "    cursor = connection.cursor()\n",
        "    cursor.execute(q)\n",
        "    connection.commit()\n",
        "    connection.close()\n",
        "\n",
        "def instrument():\n",
        "    \"\"\"\n",
        "    Starts a poenix session.\n",
        "\n",
        "    Returns:\n",
        "        session: The phoenix session object.\n",
        "    \"\"\"\n",
        "    session = px.launch_app()\n",
        "    tracer_provider = register(endpoint=\"http://127.0.0.1:6006/v1/traces\")\n",
        "    LlamaIndexInstrumentor().instrument(skip_dep_check=True, tracer_provider=tracer_provider)\n",
        "    return session\n",
        "\n",
        "def end_session(session):\n",
        "    \"\"\"\n",
        "    Ends a phoenix session.\n",
        "\n",
        "    Args:\n",
        "        session: The phoenix session object.\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    !rm {session.database_url.replace('sqlite:///', '')}\n",
        "    session.end()"
      ],
      "metadata": {
        "id": "F77zrrgjypDN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "AGENT_SYSTEM_PROMPT = PromptTemplate(\"\"\"\\\n",
        "You are a helpul customer service assistant for MikeCorp, an ecommerce company selling electronics. \\\n",
        "You are designed to help with a variety of problems a customer may have, including account management, order management, and product-related queries. \\\n",
        "If you are ever unsure what to do, please escalate.\n",
        "\n",
        "## Tools\n",
        "\n",
        "You have access to a wide variety of tools. You are responsible for using the tools in any sequence you deem\n",
        "appropriate to complete the task at hand.\n",
        "This may require breaking the task into subtasks and using different tools to complete each subtask.\n",
        "\n",
        "You have access to the following tools:\n",
        "{tool_desc}\n",
        "\n",
        "\n",
        "## Output Format\n",
        "\n",
        "Please answer in English using the following format:\n",
        "\n",
        "```\n",
        "Thought: I need to use a tool to help me answer the question.\n",
        "Action: tool name (one of {tool_names}) if using a tool.\n",
        "Action Input: the input to the tool, in a JSON format representing the kwargs (e.g. {{\"input\": \"hello world\", \"num_beams\": 5}})\n",
        "```\n",
        "\n",
        "Please ALWAYS start with a Thought.\n",
        "\n",
        "NEVER surround your response with markdown code markers. \\\n",
        "You may use code markers within your response if you need to.\n",
        "\n",
        "Please use a valid JSON format for the Action Input. Do NOT do this {{'input': 'hello world', 'num_beams': 5}}.\n",
        "\n",
        "If this format is used, the user will respond in the following format:\n",
        "\n",
        "```\n",
        "Observation: tool response\n",
        "```\n",
        "\n",
        "You should keep repeating the above format till you have enough information \\\n",
        "to answer the question without using any more tools. \\\n",
        "At that point, you MUST respond in the one of the following two formats:\n",
        "\n",
        "```\n",
        "Thought: I can answer without using any more tools. I'll use the user's language to answer\n",
        "Answer:\n",
        "```\n",
        "\n",
        "```\n",
        "Thought: I cannot answer the question with the provided tools.\n",
        "Answer:\n",
        "```\n",
        "\n",
        "## Current Conversation\n",
        "\n",
        "Below is the current conversation consisting of interleaving human and assistant messages. \\\n",
        "Conversation:\n",
        "\"\"\")\n",
        "\n",
        "def get_random_id():\n",
        "    return query(\"SELECT customer_id FROM customers ORDER BY random() LIMIT 1;\").iloc[0,0]\n",
        "\n",
        "def create_agent(llm, tools, system_prompt=AGENT_SYSTEM_PROMPT, verbose=False):\n",
        "    agent = ReActAgent.from_tools(tools, llm=llm, verbose=verbose)\n",
        "    prompt_dict = agent.get_prompts()\n",
        "    prompt_dict['agent_worker:system_prompt'] = system_prompt\n",
        "    agent.update_prompts(prompt_dict)\n",
        "    return agent"
      ],
      "metadata": {
        "id": "IZiV5U_7zAgH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def does_id_exist(id:int)-> bool:\n",
        "    df = query(f\"SELECT customer_id FROM customers WHERE customer_id = {id}\")\n",
        "    if len(df) == 0:\n",
        "        return False\n",
        "    else:\n",
        "        return True\n",
        "\n",
        "def get_update_pin(customer_id:int) -> str:\n",
        "    if not does_id_exist(customer_id):\n",
        "        raise ValueError(\"Customer id not found on file.\")\n",
        "    def update_pin(new_pin:str) -> str:\n",
        "        \"\"\"Use when you want to update the customer's pin.\n",
        "\n",
        "        Args:\n",
        "            new_pin (str): The new pin.\n",
        "\n",
        "        Returns:\n",
        "            str: A message indicating the success or failure of the update.\n",
        "        \"\"\"\n",
        "        execute(f\"UPDATE customers SET pin = '{new_pin}' WHERE customer_id = {customer_id}\")\n",
        "        return \"Pin updated successfully.\"\n",
        "\n",
        "    return FunctionTool.from_defaults(update_pin)\n",
        "\n",
        "def get_update_address(customer_id:int):\n",
        "    if not does_id_exist(customer_id):\n",
        "        raise ValueError(\"Customer id not found on file.\")\n",
        "    def update_address(street:str, city:str, state:str, zip:str, country:str) -> str:\n",
        "        \"\"\"Use when you want to update the customer's address.\n",
        "\n",
        "        Args:\n",
        "            street (str): The street address.\n",
        "            city (str): The city.\n",
        "            state (str): The state.\n",
        "            zip (str): The zip code.\n",
        "            country (str): The country.\n",
        "\n",
        "        Returns:\n",
        "            str: A message indicating the success or failure of the update.\n",
        "        \"\"\"\n",
        "        execute(f\"UPDATE customers SET street_address = '{street}', city = '{city}', state = '{state}', zip_code = '{zip}', country = '{country}' WHERE customer_id = {customer_id}\")\n",
        "        return \"Address updated successfully.\"\n",
        "\n",
        "    return FunctionTool.from_defaults(update_address)\n",
        "\n",
        "def get_update_phone_number(customer_id:int):\n",
        "    if not does_id_exist(customer_id):\n",
        "        raise ValueError(\"Customer id not found on file.\")\n",
        "    def update_phone_number(phone:str) -> str:\n",
        "        \"\"\"Use when you want to update the customer's phone number.\n",
        "\n",
        "        Args:\n",
        "            customer_id (int): The customer ID.\n",
        "            phone (str): The new phone number.\n",
        "\n",
        "        Returns:\n",
        "            str: A message indicating the success or failure of the update.\n",
        "        \"\"\"\n",
        "        execute(f\"UPDATE customers SET phone = '{phone}' WHERE customer_id = {customer_id}\")\n",
        "        return \"Phone number updated successfully.\"\n",
        "\n",
        "    return FunctionTool.from_defaults(update_phone_number)\n",
        "\n",
        "def get_user_management_tools(customer_id=get_random_id()):\n",
        "    return [\n",
        "        get_update_pin(customer_id),\n",
        "        get_update_address(customer_id),\n",
        "        get_update_phone_number(customer_id)\n",
        "    ]\n",
        "\n",
        "def get_list_orders(customer_id:int):\n",
        "    if not does_id_exist(customer_id):\n",
        "        raise ValueError(\"Customer id not found on file.\")\n",
        "    def list_orders() -> List[dict]:\n",
        "        \"\"\"Use when you want to list all order data for a customer.\n",
        "\n",
        "        Args:\n",
        "            customer_id (int): The customer ID.\n",
        "\n",
        "        Returns:\n",
        "            List[dict]: A list of dictionaries containing the order data.\n",
        "        \"\"\"\n",
        "        df = query(f\"\"\"\n",
        "        SELECT o.order_id, i.name, o.ordered_date, o.status, o.estimated_delivery, o.shipping_carrier, o.tracking_number, o.shipping_address\n",
        "        FROM orders o\n",
        "        INNER JOIN customers c ON c.customer_id = o.customer_id\n",
        "        INNER JOIN items i ON i.item_id = o.item_id\n",
        "        WHERE c.customer_id = {customer_id}\n",
        "        \"\"\")\n",
        "        return df.to_dict(orient='records')\n",
        "    return FunctionTool.from_defaults(list_orders)\n",
        "\n",
        "def does_customer_have_order_id(customer_id, order_id):\n",
        "    df = query(f\"\"\"\n",
        "    SELECT order_id\n",
        "    FROM orders\n",
        "    WHERE customer_id = {customer_id} AND order_id = {order_id}\n",
        "    \"\"\")\n",
        "    if len(df) == 0:\n",
        "        return False\n",
        "    else:\n",
        "        return True\n",
        "\n",
        "def get_cancel_order(customer_id:int):\n",
        "    if not does_id_exist(customer_id):\n",
        "        raise ValueError(\"Customer id not found on file.\")\n",
        "    def cancel_order(order_id:int) -> str:\n",
        "        \"\"\"Use when you want to cancel an order.\n",
        "\n",
        "        Args:\n",
        "            order_id (int): the order ID.\n",
        "\n",
        "        Returns:\n",
        "            str\n",
        "        \"\"\"\n",
        "        if not does_customer_have_order_id(customer_id, order_id):\n",
        "            return \"Order does not belong to this customer.\"\n",
        "        execute(f\"UPDATE orders SET status = 'cancelled' WHERE order_id = '{order_id}'\")\n",
        "        return \"Order cancelled successfully.\"\n",
        "    return FunctionTool.from_defaults(cancel_order)\n",
        "\n",
        "def get_update_order_address(customer_id:int):\n",
        "    if not does_id_exist(customer_id):\n",
        "        raise ValueError(\"Customer id not found on file.\")\n",
        "    def update_order_address(order_id:int, new_street:str, new_city:str, new_zip:str, new_state:str, new_country:str) -> str:\n",
        "        \"\"\"Use when you want to update the shipping address of an order.\n",
        "\n",
        "        Args:\n",
        "            order_id (int): The order ID.\n",
        "            new_street (str): The new street address.\n",
        "            new_city (str): The new city.\n",
        "            new_zip (str): The new zip code.\n",
        "            new_state (str): The new state.\n",
        "            new_country (str): The new country.\n",
        "\n",
        "        Returns:\n",
        "            str: A message indicating the success or failure of the update.\n",
        "        \"\"\"\n",
        "        if not does_customer_have_order_id(customer_id, order_id):\n",
        "            return \"Order does not belong to this customer.\"\n",
        "        new_address = f\"{new_street}, {new_city}, {new_state}, {new_zip}, {new_country}\"\n",
        "        execute(f\"UPDATE orders SET shipping_address = '{new_address}' WHERE order_id = '{order_id}'\")\n",
        "        return \"Shipping address updated successfully.\"\n",
        "    return FunctionTool.from_defaults(update_order_address)\n",
        "\n",
        "def get_order_tools(customer_id=get_random_id()):\n",
        "    return [\n",
        "        get_list_orders(customer_id),\n",
        "        get_cancel_order(customer_id),\n",
        "        get_update_order_address(customer_id)\n",
        "    ]\n",
        "\n",
        "# Load documents\n",
        "engine = create_engine('sqlite:///ecommerce.db')\n",
        "docs = DatabaseReader(engine=engine).load_data(query=\"SELECT item_id, description, price, quantity, name AS text FROM items\")\n",
        "embed_model = FastEmbedEmbedding(model_name='mixedbread-ai/mxbai-embed-large-v1')\n",
        "index = VectorStoreIndex.from_documents(docs, embed_model=embed_model, show_progress=True)\n",
        "inventory_tools = [\n",
        "    RetrieverTool.from_defaults(index.as_retriever(), description=\"Useful when you need to answer a question by searching items.\", name='search_items'),\n",
        "]\n",
        "\n",
        "def get_all_tools(customer_id=get_random_id()):\n",
        "    return (\n",
        "        get_base_tools(customer_id)\n",
        "        + get_user_management_tools(customer_id)\n",
        "        + get_order_tools(customer_id)\n",
        "        + inventory_tools\n",
        "    )"
      ],
      "metadata": {
        "id": "SdJlJnpTy7Iz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Does our agent work at all with the un finetuned model?\n",
        "\n",
        "Let's use the `Ollama` LLM class to run `llama3.2:3b`.\n",
        "Make sure to run `ollama pull llama3.2:3b` in the Colab terminal before loading the model - otherwise it will give you an error.\n",
        "\n",
        "Once you've loaded the model, let's run a few basic things we'd expect our agent to do.\n",
        "Does it work?"
      ],
      "metadata": {
        "id": "jVsiwscL0Dn8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = Ollama('llama3.2:3b')"
      ],
      "metadata": {
        "id": "zeXujn64hqDh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "customer_id = get_random_id()\n",
        "customer_info = get_customer_information(customer_id)\n",
        "print(customer_info)"
      ],
      "metadata": {
        "id": "H_AaZFIMh6PQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent = create_agent(llm, get_all_tools(customer_id), verbose=True)"
      ],
      "metadata": {
        "id": "xPF2NcV-xsB4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent.chat(\"Look up my information using my id and tell me my address.\")"
      ],
      "metadata": {
        "id": "L2ITMguN0UTK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Discussion:\n",
        "\n",
        "* Based on the model's outputs, what do you think the training process was?\n",
        "* Did it give any of the right information?\n",
        "* Did it call any functions? Why/why not do you think this happened?\n",
        "* Do you think the model learning our previous conversations might help?"
      ],
      "metadata": {
        "id": "1o-I-eEbVo_Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine tuning: Environment setup and imports\n",
        "\n",
        "In this section, we install and set up `axolotl`, the framework we will use to configure our fine tuning.\n",
        "We will also configure our `HF_TOKEN` here, so we can upload our model to ðŸ¤— once we're done fine-tuning."
      ],
      "metadata": {
        "id": "YSBVexvhTeoZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install axolotl\n",
        "import os\n",
        "if os.path.exists(\"axolotl\"):\n",
        "  !rm -rf axolotl\n",
        "!git clone https://github.com/axolotl-ai-cloud/axolotl\n",
        "# This handles a mismatch between xformers torch requirements and that of other dependencies\n",
        "with open('/content/axolotl/requirements.txt', 'r') as file:\n",
        "    requirements = file.read()\n",
        "    # replace xformers==0.0.27 with xformers\n",
        "    requirements = requirements.replace('xformers==0.0.27', 'xformers')\n",
        "with open('/content/axolotl/requirements.txt', 'w') as file:\n",
        "    file.write(requirements)\n",
        "!pip install -qqqq ninja packaging mlflow==\"2.13.0\" --progress-bar off\n",
        "!cd axolotl && pip install -qqqq -e \".[flash-attn,deepspeed]\" --progress-bar off"
      ],
      "metadata": {
        "id": "liriqWCjtFfa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jSYMsvJR214f"
      },
      "outputs": [],
      "source": [
        "# Set the `HF_TOKEN` env variable\n",
        "from google.colab import userdata\n",
        "import os\n",
        "token = userdata.get('HF_TOKEN')\n",
        "os.environ['HF_TOKEN'] = token"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Pull in our train.jsonl and eval.jsonl from Google drive\n",
        "if not os.path.exists(\"/content/drive/MyDrive/CTME-LLM-labs/train.jsonl\"):\n",
        "    raise ValueError(\"data.jsonl does not exist. Please make sure you've connected to google drive and run the first two lab notebooks.\")\n",
        "else:\n",
        "    try:\n",
        "        !rm -r data\n",
        "    except:\n",
        "        pass\n",
        "    !mkdir data\n",
        "    !cp /content/drive/MyDrive/CTME-LLM-labs/*.jsonl data/"
      ],
      "metadata": {
        "id": "U9S0gnK4XQUa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Observe 1 row from the training data.\n",
        "!head data/train.jsonl -n 1 | python -m json.tool"
      ],
      "metadata": {
        "id": "mJjI_NJ_k-3Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Axolotl configuration\n",
        "\n",
        "In this section, we define everything about how we want to fine tune the model, including what model we want to fine tune, where the data is, what template we want to use, and where to export the model.\n",
        "\n",
        "This config was mostly copied from [Axolotl's repositlry of examples](https://github.com/axolotl-ai-cloud/axolotl/tree/main/examples).\n",
        "You can spend a lot of time fiddling aroudn with hyperparameters, but these examples are pretty good and relatively easy to modify for anything you want to do.\n",
        "Your time is *much* better spent curating data and making sure your data is properly formatted rather than messing around with hyperparameters.\n",
        "There are also model-specific quirks that mean it's challenging to apply one fine tuning configuration to another model.\n",
        "For example, the modules targeted for LoRA adaptes may be named differently in different model families (llama, gemma, mistral, etc.).\n",
        "Do yourself a favor and start with something that works!"
      ],
      "metadata": {
        "id": "s7BQ-DosToZW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wWZ4a8_hu9yk"
      },
      "outputs": [],
      "source": [
        "%%writefile axolotl.yaml\n",
        "base_model: meta-llama/Llama-3.2-3B\n",
        "\n",
        "load_in_8bit: false\n",
        "load_in_4bit: true\n",
        "strict: false\n",
        "adapter: qlora\n",
        "\n",
        "# Data config\n",
        "dataset_prepared_path: data\n",
        "chat_template: chatml\n",
        "datasets:\n",
        "  - path: data/train.jsonl\n",
        "    ds_type: json\n",
        "    data_files:\n",
        "      - data/train.jsonl\n",
        "    conversation: alpaca\n",
        "    type: sharegpt\n",
        "\n",
        "test_datasets:\n",
        "  - path: data/eval.jsonl\n",
        "    ds_type: json\n",
        "    # You need to specify a split. For \"json\" datasets the default split is called \"train\".\n",
        "    split: train\n",
        "    type: sharegpt\n",
        "    conversation: alpaca\n",
        "    data_files:\n",
        "      - data/eval.jsonl\n",
        "\n",
        "sequence_len: 4096\n",
        "sample_packing: true\n",
        "eval_sample_packing: true\n",
        "pad_to_sequence_len: true\n",
        "\n",
        "lora_r: 32\n",
        "lora_alpha: 16\n",
        "lora_dropout: 0.05\n",
        "lora_target_linear: true\n",
        "lora_fan_in_fan_out:\n",
        "lora_target_modules:\n",
        "  - gate_proj\n",
        "  - down_proj\n",
        "  - up_proj\n",
        "  - q_proj\n",
        "  - v_proj\n",
        "  - k_proj\n",
        "  - o_proj\n",
        "\n",
        "wandb_project:\n",
        "wandb_entity:\n",
        "wandb_watch:\n",
        "wandb_name:\n",
        "wandb_log_model:\n",
        "\n",
        "gradient_accumulation_steps: 4\n",
        "micro_batch_size: 2\n",
        "num_epochs: 2\n",
        "optimizer: adamw_bnb_8bit\n",
        "lr_scheduler: cosine\n",
        "learning_rate: 0.0002\n",
        "\n",
        "train_on_inputs: false\n",
        "group_by_length: false\n",
        "bf16: auto\n",
        "fp16:\n",
        "tf32: false\n",
        "\n",
        "gradient_checkpointing: true\n",
        "early_stopping_patience:\n",
        "resume_from_checkpoint:\n",
        "local_rank:\n",
        "logging_steps: 1\n",
        "xformers_attention:\n",
        "flash_attention: true\n",
        "\n",
        "loss_watchdog_threshold: 5.0\n",
        "loss_watchdog_patience: 3\n",
        "\n",
        "warmup_steps: 10\n",
        "evals_per_epoch: 4\n",
        "eval_table_size:\n",
        "eval_max_new_tokens: 128\n",
        "saves_per_epoch: 1\n",
        "debug:\n",
        "deepspeed:\n",
        "weight_decay: 0.0\n",
        "fsdp:\n",
        "fsdp_config:\n",
        "special_tokens:\n",
        "  pad_token: \"<|end_of_text|>\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data preparation\n",
        "\n",
        "Currently, our data is in the `sharegpt` format, we have an array of conversations with a `from` and a `value` field that can't directly be used to fine-tune the model.\n",
        "We need to convert it to a template.\n",
        "In this case, we will use the popular ChatML template.\n",
        "Below is the `jinja` template for ChatML ([source](https://huggingface.co/docs/transformers/main/en/chat_templating#what-template-should-i-use)):\n",
        "\n",
        "```python\n",
        "{%- for message in messages %}\n",
        "    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n' }}\n",
        "{%- endfor %}\n",
        "```\n",
        "\n",
        "When we pipe our chat messages through this template, the actual text will be formatted like so ([source](https://huggingface.co/docs/transformers/main/en/chat_templating#what-are-generation-prompts)):\n",
        "\n",
        "```\n",
        "<|im_start|>user\n",
        "Hi there!<|im_end|>\n",
        "<|im_start|>assistant\n",
        "Nice to meet you!<|im_end|>\n",
        "<|im_start|>user\n",
        "Can I ask a question?<|im_end|>\n",
        "```\n",
        "\n",
        "We also need to tokenize our data - turn it into tensrs of integers that the model can read and learn from.\n",
        "`axolotl` will also help us tokenize the data and perfom several optimizations such as sample packing (putting multiple smaller samples in the same training example to reduce the number of padding tokens) and creating masks so we don't train on system prompts and user messages.\n",
        "\n",
        "Thankfully, `axolotl` handles all these complex configs!"
      ],
      "metadata": {
        "id": "KudCtoIsYngV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the data\n",
        "!python -m axolotl.cli.preprocess /content/axolotl.yaml"
      ],
      "metadata": {
        "id": "Vu_Zf2PUYphn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine tuning\n",
        "\n",
        "Wow, that was tough!\n",
        "Now let's do another hard thing, fine tune the model.\n",
        "The following command will launch our fine tuning run.\n",
        "It will save the LoRA adapters in the output folder."
      ],
      "metadata": {
        "id": "N7PnQChITtHs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hYHoF8rtdlCp"
      },
      "outputs": [],
      "source": [
        "# By using the ! the comand will be executed as a bash command\n",
        "!accelerate launch -m axolotl.cli.train /content/axolotl.yaml"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Merge weights\n",
        "\n",
        "In this cell, we will merge the LoRA weights with the original model."
      ],
      "metadata": {
        "id": "ERoc4N_oUA4H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 -m axolotl.cli.merge_lora axolotl.yaml"
      ],
      "metadata": {
        "id": "wHlICPMmPzs5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Export to .gguf\n",
        "\n",
        "We have decided we want to run this model with `ollama`, so we need to export to `.gguf`.\n",
        "Thankfully, `llama.cpp` comes with a handy script that helps us export our ðŸ¤— `transformers` - style model to `.gguf`."
      ],
      "metadata": {
        "id": "LDPWrxZlUFFw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to .gguf\n",
        "!python llama.cpp/convert_hf_to_gguf.py /content/model-out/merged \\\n",
        "  --outfile /content/model-out/customer-service-agent-merged.gguf \\\n",
        "  --outtype q8_0"
      ],
      "metadata": {
        "id": "b2epaeMpuD67"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Upload to ðŸ¤—\n",
        "\n",
        "Our final step is to upload the model to Huggingface.\n",
        "Make sure you have the `HF_TOKEN` environment variable set, then run the next several cells.\n",
        "Make sure to populate your hf.co username and the name of the repo you want to upload to.\n",
        "This will upload the merged file and the `.gguf` file."
      ],
      "metadata": {
        "id": "eKMILeuZUJWO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login --token $HF_TOKEN --add-to-git-credential"
      ],
      "metadata": {
        "id": "VFwW8nIDWjS8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "username = None\n",
        "repo_name = None\n",
        "if not username or not repo_name:\n",
        "    username = input(\"Username: \")\n",
        "    repo_name = input(\"Repo name: \")\n",
        "os.environ['HF_HUB_ENABLE_HF_TRANSFER'] = '1'\n",
        "!huggingface-cli upload {username}/{repo_name} /content/model-out/ ."
      ],
      "metadata": {
        "id": "eBROYTQ2V98K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Use your fine tune\n",
        "\n",
        "Ok! Finally, you have your fine tuned model.\n",
        "Let's go back up to the top of the notebook and use it instead of our original model and see if there was any improvement.\n",
        "When you instantiate the `Ollama` LLM class, replace the model we originally used with `\"hf.co/{your_username}/{your_repo_name}\"`.\n",
        "Then, run the agent normally as we did before!"
      ],
      "metadata": {
        "id": "AMos9-3KE7fj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Discussion: Next steps\n",
        "\n",
        "If you were to try and improve on this solution, what steps would you take?"
      ],
      "metadata": {
        "id": "m7cZQK6QFY6J"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OIhi6DRVHddb"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": [],
      "machine_shape": "hm",
      "collapsed_sections": [
        "Eet-v8uMdQuD",
        "UPDY2usLePC7"
      ],
      "toc_visible": true,
      "mount_file_id": "1PRrY3qfKwipvU1bXWLGBWw1u_WXQsLoy",
      "authorship_tag": "ABX9TyNcE91nvLUwlUFWzfBm3/wc",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}