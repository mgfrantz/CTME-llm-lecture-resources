{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "1hFroQXIb_0cs2AaJjkHg_caJWtItHGVL",
      "authorship_tag": "ABX9TyO99w3Qyx4bpKftilxXAQZI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mgfrantz/CTME-llm-lecture-resources/blob/main/labs/getting_started_with_LLMs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Getting started with LLMs"
      ],
      "metadata": {
        "id": "TqvGA4D4tx5g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup\n",
        "\n",
        "In the labs over the next several days, we'll need a few API keys.\n",
        "Let's get all the account setup out of the way.\n",
        "We will make 3 API keys and add them to the secrets manager as shown below ðŸ‘‡.\n",
        "\n",
        "\n",
        "![image.png](https://github.com/mgfrantz/CTME-llm-lecture-resources/blob/main/images/colabSecrets.png?raw=true)\n",
        "\n",
        "\n",
        "API key checklist:\n",
        "- [ ] Create a [Gemini api key](https://aistudio.google.com/) and add it as a secret (`GEMINI_API_KEY`)\n",
        "- [ ] Create a [Huggingface](https://huggingface.co/) account and and API key (`HF_TOKEN`)\n",
        "- [ ] Create an [OpenAI](https://platform.openai.com/) API key (`OPENAI_API_KEY`)"
      ],
      "metadata": {
        "id": "f6xP2mC_tpG1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup\n",
        "\n",
        "Most of the time, you will interact with LLMs via an API.\n",
        "OpenAI has an API standard that's been adopted by most other LLM providers.\n",
        "Today, we'll be using `litellm`, a package that wraps multiple LLM providers with a common interface, for low-level LLM work.\n",
        "We'll also be usign the `llama-index` framework to see when high-level frameworks can help us be a bit more productive or write less code.\n",
        "\n",
        "In the code below, we install requirements, set environment variables, and"
      ],
      "metadata": {
        "id": "8vRuUpH6w90-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Installs\n",
        "!pip install -qqqq --progress-bar off --no-cache \\\n",
        "    google-generativeai \\\n",
        "    llama-index-llms-gemini \\\n",
        "    llama-index \\\n",
        "    litellm \\\n",
        "    pydantic \\\n",
        "    gradio \\\n",
        "    fuzzywuzzy \\\n",
        "    python-levenshtein"
      ],
      "metadata": {
        "id": "O52hPjF-t_Ub"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Environment variables\n",
        "from google.colab import userdata\n",
        "import os\n",
        "os.environ['GEMINI_API_KEY'] = userdata.get('GEMINI_API_KEY') # expected for gemini by LiteLLM\n",
        "os.environ['GOOGLE_API_KEY'] = userdata.get('GEMINI_API_KEY') # expected for gemini by LlamaIndex\n",
        "os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY') # globally expected for OpenAI"
      ],
      "metadata": {
        "id": "vz7goJmQtvL7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "import asyncio\n",
        "import itertools\n",
        "import json\n",
        "import os\n",
        "import time\n",
        "from datetime import date, datetime, timedelta\n",
        "from typing import List, Optional, Union, Literal\n",
        "from itertools import combinations\n",
        "from tqdm.auto import tqdm\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "\n",
        "import gradio as gr\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import yfinance as yf\n",
        "from fuzzywuzzy import fuzz\n",
        "from ipywidgets import interact\n",
        "from IPython.display import Markdown, display\n",
        "from llama_index.core.llms import ChatMessage\n",
        "from llama_index.llms.openai import OpenAI\n",
        "from llama_index.llms.gemini import Gemini\n",
        "from llama_index.core.tools import FunctionTool\n",
        "from llama_index.core.program import LLMTextCompletionProgram, FunctionCallingProgram\n",
        "import litellm\n",
        "from litellm import acompletion, completion, get_supported_openai_params\n",
        "from pydantic import BaseModel, Field\n",
        "from rich import print"
      ],
      "metadata": {
        "id": "UJy7NlluNzTT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The basics\n",
        "\n",
        "Let's start by using the LLM `gemini-1.5-flash`'s free tier to learn about how to query LLMs.\n",
        "\n",
        "In the cell below ðŸ‘‡, we import `completion` from `litellm`.\n",
        "This is the core function we'll be using to chat with models in this section."
      ],
      "metadata": {
        "id": "1gkcSOtYt6jv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Making requests\n",
        "\n",
        "This model happens to be trained for chat.\n",
        "Chat expects a `list` of messages.\n",
        "Each message is a dictionary with a `role` field specifying the type of message, and a `content` field specifying the content of the message.\n",
        "Three `role` values are supported:\n",
        "\n",
        "- `system`, for giving general instructions to the LLM for how to behave\n",
        "- `user`, for a user message, and\n",
        "- `assistant`, for the LLM's response.\n",
        "\n",
        "Let's give the LLM a simple prompt and see what it returns."
      ],
      "metadata": {
        "id": "L8UakE552zHd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "GEMINI_MODEL = 'gemini/gemini-1.5-flash-002'"
      ],
      "metadata": {
        "id": "NPFMR_3wUhj6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Reverse the following string: 'abcde'. Let's think step by step:\"},\n",
        "]\n",
        "response = completion(model=GEMINI_MODEL, messages=messages)\n",
        "print(response)"
      ],
      "metadata": {
        "id": "EUJhBsmPue4y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The response actually has a lot of useful information.\n",
        "It has the model, a timestamp, other identifiers, and model-specific metadata like safety ratings in several different capacities.\n",
        "But the real content is in the `choices` field.\n",
        "Currently, the `choices` field only has one element containing the message returned by the LLM.\n",
        "Let's access the LLM's response:"
      ],
      "metadata": {
        "id": "qEnTwuRy34vR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "id": "7FIjwRtfufe9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extending the chat history\n",
        "\n",
        "In a chat setting, we also want to persist the response so we can continue the chat.\n",
        "we can add it to the `messages` so we can ask a new question in the same dialog, assuming we want to create a message history."
      ],
      "metadata": {
        "id": "st4sP6TQ4drQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "assistant_message = response.choices[0].message.dict()\n",
        "messages.append(assistant_message)\n",
        "print(assistant_message)"
      ],
      "metadata": {
        "id": "uaxmhLLA3Tyk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since the whole history of messages is in-context and passed to the LLM, it can perform logical extensions of the original task as seen here:"
      ],
      "metadata": {
        "id": "9asE8Hwo4_iE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_message = {'role': 'user', 'content': \"Now reverse it back to the same order.\"}\n",
        "messages.append(new_message)\n",
        "\n",
        "response = completion(model=GEMINI_MODEL, messages=messages)\n",
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "id": "kC-BgGWY4yqm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Streaming responses\n",
        "\n",
        "Another popular feature is streaming.\n",
        "Since we're generating 1 token at a time, the API can also return text as it's generated.\n",
        "This can be extra useful when you have long responses, and is nice in a user interface."
      ],
      "metadata": {
        "id": "FBbGkOByWfu1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# streaming\n",
        "response = completion(model=GEMINI_MODEL, messages=messages, stream=True)\n",
        "for chunk in response:\n",
        "    print(chunk.choices[0].delta.content, end='')"
      ],
      "metadata": {
        "id": "C_UubCeUWGec"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Async responses\n",
        "\n",
        "When we make API calls to these models, the time it takes to make a request is relatively short but the inference time can be long.\n",
        "Calling these APIs asynchronously lets us do other things while we're waiting for a response.\n",
        "Without async, we are waiting for the first API call to be complete to begin making the second call.\n",
        "Using async, we can make all the calls and `await` responses as they come in.\n",
        "\n",
        "Here's an example of an async request ðŸ‘‡"
      ],
      "metadata": {
        "id": "J57Bv03WW7MU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# async\n",
        "response = await acompletion(model=GEMINI_MODEL, messages=messages)\n",
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "id": "fv2crtB6W2wS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's demonstrate that we can make multiple simultaneous requests with async than we can with blocking requests."
      ],
      "metadata": {
        "id": "iXRB_rGFYkkO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# With blocking requests\n",
        "start = time.time()\n",
        "for i in range(5):\n",
        "    print(f\"Making request {i}\")\n",
        "    response = completion(model=GEMINI_MODEL, messages=messages)\n",
        "total_seconds = time.time() - start\n",
        "print(f'Total time: {total_seconds} seconds')"
      ],
      "metadata": {
        "id": "5PYZ1bLnYw3m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "async def _acomplete(model, messages, i):\n",
        "    print(f\"Making request {i}\")\n",
        "    response = await acompletion(model=model, messages=messages)\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "start = time.time()\n",
        "tasks = [_acomplete(GEMINI_MODEL, messages, i) for i in range(5)]\n",
        "responses = await asyncio.gather(*tasks)\n",
        "total_seconds = time.time() - start\n",
        "print(f'Total time: {total_seconds} seconds')"
      ],
      "metadata": {
        "id": "iL5btG25Y5h6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Exercise 1: Simple chatbot\n",
        "\n",
        "So far, we've gone over how to make basic API calls using a Python SDK.\n",
        "Let's put what we've learned to use and make a basic chatbot using the popular `gradio` library.\n",
        "All we have to do is define a function for that handles a user message and extends the chat history.\n",
        "Documentation on `gradio.ChatInterface` can be found [here](https://www.gradio.app/guides/creating-a-chatbot-fast).\n",
        "Follow this guide to make a chatbot.\n",
        "\n",
        "**Bonus**: figure out how to add a system prompt and stream resopnses."
      ],
      "metadata": {
        "id": "S2tqIr-S5Tm4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def chat(message, history):\n",
        "    raise NotImplementedError(\"Please implement this function\")\n",
        "\n",
        "gr.ChatInterface(chat, type='messages').launch()"
      ],
      "metadata": {
        "id": "11orerYm5tbH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generation hyperparameters\n",
        "\n",
        "Each model has different supported hyperparameters.\n",
        "Let's evaluate the hyperparameters here and get a good feeling for what they do.\n",
        "`litellm` has a handy function to tell us what parameters the model expects."
      ],
      "metadata": {
        "id": "ulxH8d6UatUx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "get_supported_openai_params(model=GEMINI_MODEL)"
      ],
      "metadata": {
        "id": "ZqBkbaHba9Pv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## `temperature`\n",
        "\n",
        "Let's recall that temperature alters the softmax function to skew the probability distribution of tokens during decoding.\n",
        "To illustrate, let's look at the interactive plot below ðŸ‘‡.\n",
        "We have a small probability distribution representing the top probabilities at a single decoding step.\n",
        "There are 5 tokens, `A`, `B`, `C`, `D`, and `E`, where `A` is the most probable with a score of 0.5 and `D` and `E` are at the bottom with a score of 0.05."
      ],
      "metadata": {
        "id": "X64EAcnCawtW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 2: `softmax_with_temperature` visualization\n",
        "\n",
        "Please fill in the `softmax_with_temperature` function.\n",
        "Since we're not doing any backpropagation, let's use `numpy`.\n",
        "\n",
        "The softmax with temperature function is as follows:\n",
        "\n",
        "$\\text{softmax}(p, T) = \\frac{\\exp\\left(\\frac{\\log(p_i)}{T}\\right)}{\\sum_{j}\n",
        "\\exp\\left(\\frac{\\log(p_j)}{T}\\right)}$\n",
        "\n",
        "Where:\n",
        "- $p_i$  represents the original probability values.\n",
        "- $T$  is the temperature parameter.\n",
        "- $\\exp$  is the exponential function.\n",
        "\n",
        "Once you've implemented the python function, run the cell and play with the widget.\n",
        "If temperature is smaller, how do you think that will affect text generation?\n",
        "What about when temperature is large?\n",
        "Most LLM providers cap temperature at 2.0.\n",
        "Why might you do this?"
      ],
      "metadata": {
        "id": "PxCRX4aiStox"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a small probability distribution\n",
        "probs = np.array([0.5, 0.3, 0.1, 0.05, 0.05])\n",
        "\n",
        "# Softmax function with temperature parameter\n",
        "def softmax_with_temperature(probs, temperature):\n",
        "    raise NotImplementedError(\"Please implement this function\")\n",
        "\n",
        "# Plot the distribution with numbers on top of each bar\n",
        "def plot_distribution(temperature):\n",
        "    adjusted_probs = softmax_with_temperature(probs, temperature)\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    bars = plt.bar(range(len(probs)), adjusted_probs, tick_label=['A', 'B', 'C', 'D', 'E'])\n",
        "    plt.ylim(0, 1)\n",
        "    plt.title(f'Softmax with Temperature = {temperature:.2f}')\n",
        "    plt.ylabel('Probability')\n",
        "    plt.xlabel('Categories')\n",
        "\n",
        "    # Add numbers on top of each bar\n",
        "    for bar in bars:\n",
        "        yval = bar.get_height()\n",
        "        plt.text(bar.get_x() + bar.get_width()/2, yval, f'{yval:.2f}', ha='center', va='bottom')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# Interactive widget\n",
        "interact(plot_distribution, temperature=(0.1, 2.0, 0.1));"
      ],
      "metadata": {
        "id": "rJwYX-MGQvp9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we've got a good feeling for what `temperature` does to the probability distribution, let's examine some of the outputs.\n",
        "In addition to qualitatively observing the outputs, we will calculate the similarity between each pair of generations using `fuzzywuzzy`, a library that calculates similarity using levenshtein distance.\n",
        "Roughly, it's based on the number of characters we'd have to change to turn string 1 into string 2.\n",
        "100 is exact match, and 0 is no match.\n"
      ],
      "metadata": {
        "id": "rnZpDQ9WUe8h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# perfect match, 50% matc, 0 match\n",
        "fuzz.ratio('a', 'a'), fuzz.ratio('ab', 'bb'), fuzz.ratio('a', 'b')"
      ],
      "metadata": {
        "id": "Id-GuGWlWhVZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    {'role': 'user', 'content': 'write a limerick about python'}\n",
        "]"
      ],
      "metadata": {
        "id": "S4LX3fCvcIcd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "contents = []\n",
        "for choice in completion(model=GEMINI_MODEL, messages=messages, temperature=0, n=5).choices:\n",
        "    content = choice.message.content\n",
        "    contents.append(content)\n",
        "combos = combinations(contents, 2)\n",
        "ratios = [fuzz.ratio(c1, c2) for c1, c2 in combos]\n",
        "print(f\"Average text similarity (fuzz.ratio): {np.mean(ratios)}\")\n",
        "for c in contents:\n",
        "    print(c)\n",
        "    print('='*40)"
      ],
      "metadata": {
        "id": "r1kP4mDOba5N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "contents = []\n",
        "for choice in completion(model=GEMINI_MODEL, messages=messages, temperature=1.9, n=5).choices:\n",
        "    content = choice.message.content\n",
        "    contents.append(content)\n",
        "combos = combinations(contents, 2)\n",
        "ratios = [fuzz.ratio(c1, c2) for c1, c2 in combos]\n",
        "print(f\"Average text similarity (fuzz.ratio): {np.mean(ratios)}\")\n",
        "for c in contents:\n",
        "    print(c)\n",
        "    print('='*40)"
      ],
      "metadata": {
        "id": "ENoxOMcUbr_i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## `top_p` (nucleus sampling)\n",
        "\n",
        "Let's re-visit top-p sampling.\n",
        "We definie the cumulative sum of the probabilitity distribution we want to allow for decoding.\n",
        "Let's formally define it:\n",
        "\n",
        "\n",
        "$\\text{Top-p sampling:} \\quad P(S) = \\{x_i \\mid \\sum_{x_j \\in S} P(x_j) \\geq p \\}$\n",
        "\n",
        "Where:\n",
        "\n",
        "- $P(x_i)$  is the probability of token  x_i .\n",
        "- $S$  is the set of selected tokens.\n",
        "- $p$  is the cumulative probability threshold (top-p value).\n",
        "\n",
        "In short, we take the set of all tokens such that the sum of the token probabilities is greater than or equal to the parameter $p$.\n",
        "Let's illustrate this in anoher widget.\n"
      ],
      "metadata": {
        "id": "O8hP8SG8a3_-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a small probability distribution (can simulate a language model's logits)\n",
        "probs = np.array([0.4, 0.2, 0.15, 0.1, 0.08, 0.05, 0.02])\n",
        "\n",
        "# Function to apply top-p filtering with a minimum of one token selected\n",
        "def top_p_filter(probs, p):\n",
        "    sorted_probs = np.sort(probs)[::-1]\n",
        "    cumulative_probs = np.cumsum(sorted_probs)\n",
        "\n",
        "    # Ensure at least one token is selected\n",
        "    if p < sorted_probs[0]:\n",
        "        cutoff = 1\n",
        "    else:\n",
        "        cutoff = np.argmax(cumulative_probs >= p) + 1\n",
        "\n",
        "    filtered_probs = sorted_probs[:cutoff]\n",
        "    return filtered_probs, cutoff\n",
        "\n",
        "# Plot the distribution with top-p filtering\n",
        "def plot_top_p(p):\n",
        "    filtered_probs, cutoff = top_p_filter(probs, p)\n",
        "    normalized_probs = filtered_probs / np.sum(filtered_probs)  # Normalize the selected probabilities\n",
        "\n",
        "    # Create two subplots\n",
        "    fig, axs = plt.subplots(1, 2, figsize=(12, 5))\n",
        "    labels = ['A', 'B', 'C', 'D', 'E', 'F', 'G']\n",
        "\n",
        "    # Plot 1: Original distribution with top-p filtering\n",
        "    bars1 = axs[0].bar(range(len(probs)), np.sort(probs)[::-1], tick_label=labels)\n",
        "    axs[0].set_ylim(0, 1)\n",
        "    axs[0].set_title(f'Top-p Sampling (p = {p:.2f}) - Original Probabilities')\n",
        "    axs[0].set_ylabel('Probability')\n",
        "    axs[0].set_xlabel('Categories')\n",
        "\n",
        "    # Highlight selected and unselected probabilities\n",
        "    for i, bar in enumerate(bars1):\n",
        "        if i >= cutoff:\n",
        "            bar.set_color('gray')  # Color the bars outside top-p as gray\n",
        "        else:\n",
        "            bar.set_color('blue')  # Highlight the selected probabilities\n",
        "\n",
        "    # Add numbers on top of each bar for original distribution\n",
        "    for bar in bars1:\n",
        "        yval = bar.get_height()\n",
        "        axs[0].text(bar.get_x() + bar.get_width()/2, yval, f'{yval:.2f}', ha='center', va='bottom')\n",
        "\n",
        "    # Plot 2: Normalized probabilities of the selected tokens\n",
        "    bars2 = axs[1].bar(range(len(filtered_probs)), normalized_probs, tick_label=labels[:len(filtered_probs)])\n",
        "    axs[1].set_ylim(0, 1)\n",
        "    axs[1].set_title(f'Normalized Probabilities of Selected Tokens (p = {p:.2f})')\n",
        "    axs[1].set_ylabel('Normalized Probability')\n",
        "    axs[1].set_xlabel('Selected Categories')\n",
        "\n",
        "    # Add numbers on top of each bar for normalized probabilities\n",
        "    for bar in bars2:\n",
        "        yval = bar.get_height()\n",
        "        axs[1].text(bar.get_x() + bar.get_width()/2, yval, f'{yval:.2f}', ha='center', va='bottom')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Interactive widget\n",
        "interact(plot_top_p, p=(0.01, 1.0, 0.05));"
      ],
      "metadata": {
        "id": "fv_aR67dyaod"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 3: Relationship between `top_p` and sequence similarity\n",
        "\n",
        "Just like `temperature`, `top_p` is another lever to control how text is generated.\n",
        "I this exercise, let's actually visualize that relationship.\n",
        "Please create a plot wher the x-axis is the `top-p` value, and the y-axis is the average similarity (using `fuzz.ratio`) of all of the generations for the same prompt."
      ],
      "metadata": {
        "id": "L4c7p2NZa6B7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code here"
      ],
      "metadata": {
        "id": "MURTGbua1BMy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Structured outputs\n",
        "\n",
        "Many times, we want to use LLMs as middleware in other systems.\n",
        "That means they may have to do things like extract structured data, populate parameters to API calls, etc.\n",
        "This is really hard to do with plain text LLM responses - you have to do lots of \"begging\" to get the LLM to output a consistent structure.\n",
        "\n",
        "For this next section, we're going to switch to OpenAI's API since it's much better than gemini at producing structured outputs.\n",
        "We will be using `gpt-4o-mini`.\n",
        "\n",
        "The most convenient way to declare what you want to return is with `pydantic` models.\n",
        "These are really useful because you can define exactly the values, formats, and types you expect back.\n",
        "As you can see in the `EventDetails` class below, we've specified all the information we want to know about an event.\n",
        "We can even describe complex objects, like lists or dicts of other objects."
      ],
      "metadata": {
        "id": "dYOI0k9WbPbp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "OPENAI_MODEL = 'openai/gpt-4o-mini'"
      ],
      "metadata": {
        "id": "p1C7FnAVbSVr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EventDetails(BaseModel):\n",
        "    attendees: List[str] = Field(..., description=\"List of attendees\")\n",
        "    date: str = Field(..., description=\"Date of the event, in the YYYY-MM-DD format\")\n",
        "    eventName: str = Field(..., description=\"Name of the event\")"
      ],
      "metadata": {
        "id": "GeFElBvpbnww"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Jan and Peter are going to have dinner on 7/15/2024\"\n",
        "resp = completion(model=OPENAI_MODEL, messages=[{\"role\": \"user\", \"content\": text}], response_format=EventDetails)"
      ],
      "metadata": {
        "id": "V2uhFO5Nb--A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Response text:\\n\", c:=resp.choices[0].message.content)\n",
        "print(\"Loaded JSON:\", j:=json.loads(c))\n",
        "print(\"Pydantic object:\", o:=EventDetails(**j))"
      ],
      "metadata": {
        "id": "qctGF9iic-Fn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Function calling"
      ],
      "metadata": {
        "id": "3Ib4kKQToAOt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"{GEMINI_MODEL} supports function calling: \", litellm.supports_function_calling(model=GEMINI_MODEL))\n",
        "print(f\"{OPENAI_MODEL} supports function calling: \", litellm.supports_function_calling(model=OPENAI_MODEL))"
      ],
      "metadata": {
        "id": "N-zmtwsiolTY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_stock_data(\n",
        "        symbols:Union[str, List[str]],\n",
        "        start_date:Optional[str]=str(date.today() - timedelta(days=7)),\n",
        "        end_date:Optional[str] = str(date.today())\n",
        "):\n",
        "    result = yf.download(symbols, start=start_date, end=end_date)\n",
        "    return result.to_markdown()"
      ],
      "metadata": {
        "id": "bkuetfoVqBHb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(Markdown(get_stock_data('MSFT')))"
      ],
      "metadata": {
        "id": "cv9sc-Z1rceF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "function_definition = {\n",
        "    \"type\": \"function\",\n",
        "    \"function\": {\n",
        "        \"name\": \"get_stock_data\",\n",
        "        \"description\": \"Get historical stock data for a symbol or list of symbols. Returns markdown table.\",\n",
        "        \"parameters\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"symbols\": {\n",
        "                    \"type\": \"string\",\n",
        "                    \"description\": \"Symbol or list of symbols to get data for\",\n",
        "                    \"required\": [\"symbols\"]\n",
        "                    },\n",
        "                \"start_date\": {\n",
        "                    \"type\": \"string\",\n",
        "                    \"description\": \"Start date in YYYY-MM-DD format. Should be at least 1 day before the end_date.\",\n",
        "                },\n",
        "                \"end_date\": {\n",
        "                    \"type\": \"string\",\n",
        "                    \"description\": \"End date in YYYY-MM-DD format\",\n",
        "                }\n",
        "            },\n",
        "            \"required\": [\"symbols\"]\n",
        "        }\n",
        "    }\n",
        "}\n",
        "functions = [function_definition]"
      ],
      "metadata": {
        "id": "hbnRfBKbsIEM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [{\"role\": \"user\", \"content\": f\"What's the most recent day's high and low for MSFT?\"}]\n",
        "resp = completion(\n",
        "    model=OPENAI_MODEL,\n",
        "    messages=messages,\n",
        "    tools=functions,\n",
        "    tool_choice='auto'\n",
        ")"
      ],
      "metadata": {
        "id": "iMr0-8X6st9s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "message = resp.choices[0].message\n",
        "message"
      ],
      "metadata": {
        "id": "FvlW7YaEwYr1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tool_call = message.tool_calls[0]\n",
        "tool_call"
      ],
      "metadata": {
        "id": "z1zqM7RctLUm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "func = tool_call.function.name\n",
        "args = json.loads(tool_call.function.arguments)\n",
        "output_str = eval(f\"{func}(**{args})\")\n",
        "display(Markdown(output_str))"
      ],
      "metadata": {
        "id": "HxuoG3i5taF0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages.append(message.dict())\n",
        "messages.append(\n",
        "    {\n",
        "        \"tool_call_id\": tool_call.id,\n",
        "        \"role\": \"tool\",\n",
        "        \"name\": func,\n",
        "        \"content\": output_str,\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "id": "chhfkQmCwOUC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(messages)"
      ],
      "metadata": {
        "id": "R1YRle7owQOD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resp = completion(\n",
        "    model=OPENAI_MODEL,\n",
        "    messages=messages\n",
        ")"
      ],
      "metadata": {
        "id": "m-f5B2xPtq_z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(resp.choices[0].message.content)"
      ],
      "metadata": {
        "id": "l_b6H36muXug"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 4: Fix this stock retriever\n",
        "\n",
        "Do you notice anything weird about the response here?\n",
        "LLMs have a knowledge cutoff, meaning that they don't know anything after a certain date.\n",
        "To solve this, we need to give the LLM the current date.\n",
        "What are some ways to do this?\n",
        "Please edit the above code to fix the error."
      ],
      "metadata": {
        "id": "7wVMoM1Vu6lK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# High-level frameworks: `llama-index`\n",
        "\n",
        "So far, we've been working with the pretty basic OpenAI API specification.\n",
        "It's really good to have experience with this because it's a pretty universal API standard.\n",
        "However, sometimes it can take a lot of time to write certain functionalities such as structured outputs, function calls, etc.\n",
        "They're not a panacea, but high-level LLM libraries can really help take some of the gruntwork out of working with LLMs.\n",
        "\n",
        "In this couse, we'll be using [`llama-index`](https://docs.llamaindex.ai/en/stable/) as our high-level library, but there are other popular options like `langchain` and `DSPy`.\n",
        "\n",
        "Let's start by replicating some of the text generation we did previously."
      ],
      "metadata": {
        "id": "iGEhm_kUTYxR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "openai_llm = OpenAI(model='gpt-4o-mini')\n",
        "gemini_llm = Gemini(model='models/gemini-1.5-flash-002')"
      ],
      "metadata": {
        "id": "EQBWqcbDykr8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate text\n",
        "\n",
        "Text generation using `llama-index` is pretty straightforward.\n",
        "Once you've created an instance of an LLM object, you can call `.complete`, `chat`, `acomplete`, or `achat` directly on the object."
      ],
      "metadata": {
        "id": "2uAFTh2zihKQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    ChatMessage(role='user', content='Write a limerick about python')\n",
        "]"
      ],
      "metadata": {
        "id": "lb99grFWYlXs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resp = openai_llm.chat(messages=messages, n=5, top_p=0.9, temperature=0.9) # sync\n",
        "# resp = await llm.achat(messages=messages, n=5, top_p=0.9, temperature=0.9) # async"
      ],
      "metadata": {
        "id": "j6wZU__XzjW3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for c in resp.raw.choices:\n",
        "    print(c.message.content)\n",
        "    print('='*40)"
      ],
      "metadata": {
        "id": "I6NfMuS50bJy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate structured outputs\n",
        "\n",
        "Structured outputs are much the same - you define a `pydantic` object, and instruct the LLM to generate that object.\n",
        "Here, we're going to get a bit more complex with the pydantic objects we're creating."
      ],
      "metadata": {
        "id": "BJCew0q_i0EF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Person(BaseModel):\n",
        "    name: str = Field(..., description=\"Name of the person\")\n",
        "    age: int = Field(..., description=\"Age of the person\", ge=1, le=100)\n",
        "\n",
        "class People(BaseModel):\n",
        "    people: List[Person] = Field(..., description=\"List of people\")\n",
        "\n",
        "message = ChatMessage.from_str('Please create 3 people')\n",
        "\n",
        "structured_llm = openai_llm.as_structured_llm(output_cls=People)\n",
        "\n",
        "structured_output = structured_llm.chat([message])"
      ],
      "metadata": {
        "id": "GbeESBPHVPCH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "structured_output.raw"
      ],
      "metadata": {
        "id": "RScymx88g_t2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Call functions\n",
        "\n",
        "Calling functions can also be easier with frameworks.\n",
        "If you still want to use tools like `litellm`, high-level frameworks can still make life a bit easier by creating that nasty function json for you.\n",
        "You can also use them to call functions.\n",
        "Tomorrow when we work with agents, we will be using even more advanced function calling usage."
      ],
      "metadata": {
        "id": "XIXeHhXuiYfe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_stock_data(\n",
        "        symbols:Union[str, List[str]] = Field(..., description=\"Symbol or list of symbols\"),\n",
        "        start_date:Optional[str] = Field(default=str(date.today() - timedelta(days=7)), description=\"Start date in YYYY-MM-DD format. Should be at least 1 day before the end_date.\"),\n",
        "        end_date:Optional[str] = Field(default=str(date.today()), description=\"End date in YYYY-MM-DD format\")\n",
        "):\n",
        "    \"\"\"\n",
        "    Get historical stock data for a symbol or list of symbols. Returns markdown table.\n",
        "\n",
        "    Args:\n",
        "        symbols (Union[str, List[str]]): Symbol or list of symbols to get data for.\n",
        "        start_date (Optional[str], optional): Start date in YYYY-MM-DD format. Defaults to 7 days before today. Must be at least 1 day before end_date.\n",
        "        end_date (Optional[str], optional): End date in YYYY-MM-DD format. Defaults to today. Must be at least 1 day after start_date.\n",
        "    \"\"\"\n",
        "    result = yf.download(symbols, start=start_date, end=end_date)\n",
        "    return result.to_markdown()"
      ],
      "metadata": {
        "id": "tLCGeS-Fi6bp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llama_index_function = FunctionTool.from_defaults(fn=get_stock_data)"
      ],
      "metadata": {
        "id": "a5YaRX3TOjC4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llama_index_function.metadata.to_openai_tool()"
      ],
      "metadata": {
        "id": "34JOnGQwOlGS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "message = ChatMessage.from_str(f\"What's the most recent week's high and low for MSFT? Today's date is {date.today()}.\")\n",
        "resp = openai_llm.chat_with_tools([llama_index_function], user_msg=message)"
      ],
      "metadata": {
        "id": "bIWvFo4MOtev"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(resp)"
      ],
      "metadata": {
        "id": "munQLf83QlSB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fn_args = json.loads(resp.raw.choices[0].message.tool_calls[0].function.arguments)\n",
        "fn_args"
      ],
      "metadata": {
        "id": "frOfG0HNPWcl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(Markdown(get_stock_data(**fn_args)))"
      ],
      "metadata": {
        "id": "PwtNjuVFQETp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Practicum: Generate an ecommerce DB\n",
        "\n",
        "### Problem statement:\n",
        "\n",
        "You are helping an immaginary online retailer build an customer support agent.\n",
        "But they don't want to share any of their data with you, only the schema of their database.\n",
        "We need to create some reasonable data for a proof of concept.\n",
        "Over the next several days, we will be using this data to build a customer support agent.\n",
        "\n",
        "\n",
        "### Generating the data\n",
        "\n",
        "Use what you've learned about generating structured data to help fill the following tables.\n",
        "When we're done, your database should contain:\n",
        "* Generate 10-20 customers in the `customers` table\n",
        "* Generate 50 products in the `items` table\n",
        "* A purchase history for each customer ranging from 1 to 10 items in the `orders` table\n",
        "\n",
        "Remember to think critically about what the llm *should* and *should not* generate.\n",
        "One example is IDs - we've gone ahead and created database tables with auto-incrementing IDs.\n",
        "\n",
        "Below, we have code to create the tables, as well as an example of how to add data to the tables using `df.to_sql`."
      ],
      "metadata": {
        "id": "eoez9OrRREqf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sqlite3\n",
        "from sqlalchemy import create_engine"
      ],
      "metadata": {
        "id": "ok042MwHTRSB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_tables():\n",
        "    if os.path.exists('ecommerce.db'):\n",
        "        os.remove('ecommerce.db')\n",
        "    conn = sqlite3.connect('ecommerce.db')\n",
        "    cursor = conn.cursor()\n",
        "\n",
        "    # Create Customers Table\n",
        "    cursor.execute(\"\"\"\n",
        "        CREATE TABLE IF NOT EXISTS customers (\n",
        "            customer_id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "            name TEXT NOT NULL,\n",
        "            email TEXT UNIQUE NOT NULL,\n",
        "            phone TEXT,\n",
        "            street_address TEXT,\n",
        "            city TEXT,\n",
        "            state TEXT,\n",
        "            zip_code TEXT,\n",
        "            country TEXT,\n",
        "            account_status TEXT,\n",
        "            pin TEXT\n",
        "        );\n",
        "    \"\"\")\n",
        "\n",
        "    # Create Items Table\n",
        "    cursor.execute(\"\"\"\n",
        "        CREATE TABLE IF NOT EXISTS items (\n",
        "            item_id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "            name TEXT NOT NULL,\n",
        "            description TEXT,\n",
        "            price REAL NOT NULL,\n",
        "            quantity INTEGER NOT NULL\n",
        "        );\n",
        "    \"\"\")\n",
        "\n",
        "    # Create Orders Table\n",
        "    cursor.execute(\"\"\"\n",
        "        CREATE TABLE IF NOT EXISTS orders (\n",
        "            order_id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "            customer_id TEXT NOT NULL,\n",
        "            item_id TEXT NOT NULL,\n",
        "            status TEXT,\n",
        "            ordered_date TEXT,\n",
        "            estimated_delivery TEXT,\n",
        "            shipping_carrier TEXT,\n",
        "            tracking_number TEXT,\n",
        "            shipping_address TEXT,\n",
        "            FOREIGN KEY (customer_id) REFERENCES customers(customer_id),\n",
        "            FOREIGN KEY (item_id) REFERENCES items(item_id)\n",
        "        );\n",
        "    \"\"\")\n",
        "\n",
        "    conn.commit()\n",
        "    conn.close()\n",
        "    print(\"Tables created successfully.\")"
      ],
      "metadata": {
        "id": "iqDT_M2aTe4s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def query(sql:str) -> pd.DataFrame:\n",
        "    conn = sqlite3.connect('ecommerce.db')\n",
        "    cursor = conn.cursor()\n",
        "    cursor.execute(sql)\n",
        "    results = cursor.fetchall()\n",
        "    columns = [column[0] for column in cursor.description]\n",
        "    conn.close()\n",
        "    return pd.DataFrame(results, columns=columns)\n",
        "\n",
        "def execute(sql:str) -> None:\n",
        "    conn = sqlite3.connect('ecommerce.db')\n",
        "    cursor = conn.cursor()\n",
        "    cursor.execute(sql)\n",
        "    conn.commit()\n",
        "    conn.close()"
      ],
      "metadata": {
        "id": "n6XIeHzPUGPZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "create_tables()\n",
        "engine = create_engine('sqlite:///ecommerce.db')\n",
        "one_customer = pd.DataFrame([\n",
        "    {\n",
        "        'name': 'mike',\n",
        "        'email': 'mike@gmail.com',\n",
        "        'phone': '555-555-5555',\n",
        "        'street_address': '123 Main St',\n",
        "        'city': 'Anytown',\n",
        "        'state': 'CA',\n",
        "        'zip_code': '12345',\n",
        "        'country': 'USA',\n",
        "        'account_status': 'active',\n",
        "        'pin': '1234'\n",
        "    }\n",
        "])\n",
        "\n",
        "one_customer.to_sql('customers', engine, if_exists='append', index=False)\n",
        "\n",
        "query('SELECT * FROM customers')"
      ],
      "metadata": {
        "id": "_Pb3Q6pSUY1h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "create_tables()"
      ],
      "metadata": {
        "id": "KwXIX2cwWY-S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate 10-20 customers\n",
        "\n",
        "Use this section to practice creating structured outputs with the `openai_llm` or `gemini_llm` object we created earlier.\n",
        "Use `pydantic` to define the schema of the data you want to generate, then use an LLM to generate them.\n",
        "Once you generate your customers, insert them into the database.\n",
        "Make sure that the values you're generating make sense!\n",
        "\n",
        "**NOTE:** If you perform multiple LLM calls to generate customers, make sure you are not generating the same customers.\n",
        "Some LLM providers will cache responses, so the same prompt will give you the same output."
      ],
      "metadata": {
        "id": "b3LH4WOAXwkH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "customer_statuses = Literal['active', 'inactive', 'suspended', 'canceled']\n",
        "\n",
        "class Customer(BaseModel):\n",
        "    name: str = Field(..., description=\"Name of the customer\")\n",
        "    email: str = Field(..., description=\"Email of the customer\")\n",
        "    phone: str = Field(..., description=\"Phone number of the customer\")\n",
        "    street_address: str = Field(..., description=\"Street address of the customer\")\n",
        "    city: str = Field(..., description=\"City of the customer\")\n",
        "    state: str = Field(..., description=\"State of the customer (US Only)\")\n",
        "    zip_code: str = Field(..., description=\"Zip code of the customer\")\n",
        "    country: str = Field(default='US', description=\"Country of the customer\")\n",
        "    account_status: customer_statuses = Field(..., description=\"Status of the customer account\")\n",
        "    pin: str = Field(default_factory=lambda x: f\"{np.random.randint(0, 1000):04}\", description=\"PIN for the customer\")\n",
        "\n",
        "class Customers(BaseModel):\n",
        "    customers: List[Customer] = Field(..., description=\"List of customers\")"
      ],
      "metadata": {
        "id": "FWGrv2jHJ_OJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "customers = []\n",
        "prompt = \"Please generate 5 customers\"\n",
        "messages = [ChatMessage.from_str(prompt)]\n",
        "structured_llm = openai_llm.as_structured_llm(output_cls=Customers)\n",
        "resp = structured_llm.chat(messages)\n",
        "for c in resp.raw.customers:\n",
        "    customers.append(c.model_dump())\n",
        "for _ in range(3):\n",
        "    messages.append(resp.message)\n",
        "    messages.append(ChatMessage.from_str(\"Please generate 5 more customers\"))\n",
        "    resp = structured_llm.chat(messages)\n",
        "    for c in resp.raw.customers:\n",
        "        customers.append(c.model_dump())"
      ],
      "metadata": {
        "id": "fs49aU4NLR3n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "customer_df = pd.DataFrame(customers)\n",
        "customer_df.sample(5)"
      ],
      "metadata": {
        "id": "dEh8Lb8xNXmm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "engine = create_engine('sqlite:///ecommerce.db')\n",
        "customer_df.to_sql('customers', engine, if_exists='append', index=False)"
      ],
      "metadata": {
        "id": "kNCSYQgeN7kT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query('SELECT * FROM customers ORDER BY random() LIMIT 5;')"
      ],
      "metadata": {
        "id": "O4d4iyb-ORmq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate 50 products/items\n",
        "\n",
        "Let's take a slightly different approach.\n",
        "Another way to generate structured outputs with `llama-index` is using the LLM pydantic program.\n",
        "Please read the documentation [here](https://docs.llamaindex.ai/en/stable/examples/output_parsing/llm_program/) and use `LLMTextCompletionProgram` to create data to populate the `items` table.\n",
        "\n",
        "**Hint:** Use the pydantic's `Field` to set some sensible defaults.\n",
        "You can set parameters like `ge` (greater than or equal to) or `le` (less than or equal to), or `default_factory` to use a function to generate default values."
      ],
      "metadata": {
        "id": "nlsiUxzPYBbW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code here"
      ],
      "metadata": {
        "id": "CtNmdZkr1Dor"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query('SELECT * FROM items ORDER BY random() LIMIT 5;')"
      ],
      "metadata": {
        "id": "fIogt-AtRqh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Order history\n",
        "\n",
        "Now let's generate the order history for each customer.\n",
        "Since we will have many orders per customer, we are potentially generating hundreds of orders.\n",
        "This is a time when we want to use async processing.\n",
        "For each customer, please generate between 0 and 20 orders using `acall` (`LLMTextCompletionProgram`) or `achat` (structured LLM).\n",
        "\n",
        "Try to think about typical patterns you'd see in ordering.\n",
        "For example, customers are probably shipping to themselves *most* of the time, but sometimes buy items shipped to a work address or to a friend's home as a gift.\n",
        "Think about what information you can pull from the database and what should be generated by the LLM to help set some sensible defaults."
      ],
      "metadata": {
        "id": "oik8psRGYEr5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code here"
      ],
      "metadata": {
        "id": "7ZF-cCMN1JjX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query('SELECT * FROM orders ORDER BY random() LIMIT 5;')"
      ],
      "metadata": {
        "id": "L-MrNDJdUMhO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Wrapping up\n",
        "\n",
        "We will use `ecommerce.db` in the lab tomorrow.\n",
        "There are two options:\n",
        "\n",
        "1. Download `ecommerce.db` to your local device, and we'll upload it to the colab notebook during the next lab\n",
        "2. Save it to your Google drive. To save to your Google drive, click the google drive folder as shown below, then run the cell below.\n",
        "Then we will be able to access this database tomorrow!\n",
        "\n",
        "![image.png](data:image/png;base64,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)\n",
        "\n"
      ],
      "metadata": {
        "id": "Rq5J2OmVUOji"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if not os.path.exists('drive/MyDrive/CTME-LLM-labs'):\n",
        "    !mkdir drive/MyDrive/CTME-LLM-labs\n",
        "!cp ecommerce.db drive/MyDrive/CTME-LLM-labs/"
      ],
      "metadata": {
        "id": "JfsVhrI9XVRZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "S-42qJP8XcfZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}