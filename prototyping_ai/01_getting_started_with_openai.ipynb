{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "4HDOGmN6OmF0",
        "uZDWgZfZQGcU",
        "XfqCJ9bmLDG2",
        "e-_7WDTwPyZo",
        "JTQ-yAmFQVOM",
        "-bHAfnK9YizZ"
      ],
      "authorship_tag": "ABX9TyOxn0FYv6wgNpdYFyM7LkBF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mgfrantz/CTME-llm-lecture-resources/blob/main/prototyping_ai/01_getting_started_with_openai.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Making our first LLM API call\n",
        "\n"
      ],
      "metadata": {
        "id": "4HDOGmN6OmF0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ],
      "metadata": {
        "id": "uZDWgZfZQGcU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JYQyB8QUIhOm"
      },
      "outputs": [],
      "source": [
        "!pip install -Uqqqq openai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "from rich import print"
      ],
      "metadata": {
        "id": "z7bXGANBPZk-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting our `OPENAI_API_KEY` environment variable\n",
        "\n",
        "When we use any LLM provider like OpenAI, Anthropic, or Google, we need some way to tell them who we are making the request.\n",
        "Today, we'll be using OpenAI.\n",
        "The most straightforward way to provide this key is through the environment variable `OPENAI_API_KEY`.\n",
        "The OpenAI python client looks for this environment variable to use in authentication.\n",
        "\n",
        "In the cell below, we load it from the Google Colab secrets manager on the left ðŸ‘ˆ.\n",
        "Before runnign this code, make sure your API key is set as shown below:\n",
        "\n",
        "![](https://github.com/mgfrantz/CTME-llm-lecture-resources/blob/main/images/colabSecrets.png?raw=true)"
      ],
      "metadata": {
        "id": "XfqCJ9bmLDG2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the OPENAI_API_KEY environment variable\n",
        "from google.colab import userdata # import the environment variables from secrets\n",
        "import os\n",
        "os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY') # Set the OPENAI_API_KEY environmnet variable"
      ],
      "metadata": {
        "id": "N2nPVwzrIpPQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Under the hood: `curl`\n",
        "\n",
        "Almost all of the interactions we will have with LLMs are through API calls.\n",
        "Below is one of the most low-level ways we can call an LLM, using the `curl` command.\n",
        "This command gives us a lot of information about how the API request is structured.\n",
        "We pass a JSON with an authorization header containing our `OPENAI_API_KEY`.\n",
        "We also pass the model we want to call, the chat messages, and hyperparameters such as `temperature` that help control how text is generated."
      ],
      "metadata": {
        "id": "e-_7WDTwPyZo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make the API call to OpenAI and store the response in test.json\n",
        "!curl https://api.openai.com/v1/chat/completions \\\n",
        "  -H \"Content-Type: application/json\" \\\n",
        "  -H \"Authorization: Bearer $OPENAI_API_KEY\" \\\n",
        "  -d '{ \\\n",
        "     \"model\": \"gpt-4o-mini\", \\\n",
        "     \"messages\": [{\"role\": \"user\", \"content\": \"Say: This is a test!\"}], \\\n",
        "     \"temperature\": 0.7 \\\n",
        "    }' > test.json\n",
        "# Show the output of test.json formatted nicely\n",
        "!cat test.json | python -m json.tool"
      ],
      "metadata": {
        "id": "s47r5FtcOyLG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using the OpenAI Python client\n",
        "\n",
        "While the `curl` command shows us how the API call is made, it's not something that we can easily use in more complex applications.\n",
        "One thing we can use is the OpenAI python client.\n",
        "We can do the exact same thing, but the API call is a bit more abstracted from the developer.\n",
        "Let's see how to perform the exact same API call using the OpenAI clinet ðŸ‘‡:"
      ],
      "metadata": {
        "id": "DPRgn2_IP3w6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI # import OpenAI\n",
        "client = OpenAI() # Create the OpenAI client"
      ],
      "metadata": {
        "id": "fPRvearuIz86"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the messages (same as above)\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"Say: This is a test!\"},\n",
        "]\n",
        "\n",
        "# Make the API call\n",
        "chat_completion = client.chat.completions.create(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    messages=messages,\n",
        "    # stream=True\n",
        ")"
      ],
      "metadata": {
        "id": "vYbGuKC8I3na"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the output\n",
        "print(chat_completion)"
      ],
      "metadata": {
        "id": "2gd5AitgI9kU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build our first chatbots\n",
        "\n",
        "In this section, we'll go through several demos.\n",
        "By the end of this section, you should be able to:\n",
        "- Build a basic chatbot with the popular `gradio` Python library\n",
        "- Understand key hyperparameters like `temperature`, `top_p`, and `top_k`\n",
        "- Build an advanced chatbot with hyperparameter controls"
      ],
      "metadata": {
        "id": "yJ0qYsS0Y4DZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise: Building a basic chatbot with `gradio`\n",
        "\n",
        "User interfaces (UIs) are a great way to demo work in AI.\n",
        "In the next several lessons, we will be using the `gradio` framework to demonstrate our growing skillset.\n",
        "In this exercise, we will get a gentle introduction to creating chatbots with `gradio`.\n",
        "\n",
        "Please follow the [ChatInterface](https://www.gradio.app/docs/gradio/chatinterface) documentation and the [Creating a chatbot fast](https://www.gradio.app/guides/creating-a-chatbot-fast) guide to make your first AI chatbot.\n",
        "Your chatbot must:\n",
        "- respond to messages\n",
        "\n",
        "If this too easy, try to:\n",
        "- add a system prompt\n",
        "- use `stream=True` in your chat function"
      ],
      "metadata": {
        "id": "JTQ-yAmFQVOM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -Uqqqq gradio"
      ],
      "metadata": {
        "id": "qOq34PZ-QjYO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr"
      ],
      "metadata": {
        "id": "4_tJtXNMRHA6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def chat(message, history):\n",
        "    if not history:\n",
        "        history = [\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant who speaks like a pirate.\"},\n",
        "        ]\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        messages=history,\n",
        "        stream=True,\n",
        "    )\n",
        "    partial_message = \"\"\n",
        "    for chunk in response:\n",
        "        if chunk.choices[0].delta.content is not None:\n",
        "              partial_message = partial_message + chunk.choices[0].delta.content\n",
        "              yield partial_message"
      ],
      "metadata": {
        "id": "DL3d7elhRkj3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gr.ChatInterface(chat, type='messages').launch()"
      ],
      "metadata": {
        "id": "zy5lPNy9R7ur"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text generation hyperparameters\n",
        "\n",
        "There are several hyperparameters we can play with that determine how text is generated.\n",
        "For each token, the model outputs a score distribution over words, and that distribution is normalized using the softmax function to sum to 1.0.\n",
        "We have several options to modify this probability distribution in ways that affect the way text is generated."
      ],
      "metadata": {
        "id": "-bHAfnK9YizZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### `temperature`\n",
        "\n",
        "The softmax function is shown below:\n",
        "\n",
        "$$\n",
        "\\text{softmax}(p) = \\frac{e^{x_i}}{\\sum_{j=0}^ne^{x_j}}\n",
        "$$\n",
        "\n",
        "The softmax function is defined in python below:\n"
      ],
      "metadata": {
        "id": "pibw6nM-YncC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def softmax(p):\n",
        "    return np.exp(p) / np.sum(np.exp(p))\n",
        "\n",
        "# Example usage\n",
        "p = np.array([1, 2, 3, 4, 5])\n",
        "print(softmax(p))"
      ],
      "metadata": {
        "id": "bIskN3MLMdmx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `temperature` paramater allows us to make the most probable words more probable (temperature < 1) or less probable (temperature > 1) than vanilla softmax (temperature = 1).\n",
        "The formula for softmax with temperature is show below:\n",
        "\n",
        "$$\n",
        "\\text{softmax}(p, T) = \\frac{e^{\\frac{x_i}{T}}}{\\sum_{j=0}^ne^{\\frac{x_j}{T}}}\n",
        "$$\n",
        "\n",
        "All you do is divide everything by T before taking the exponent; larger values of $T$ flatten the distribution, while smaller values of $T$ skew the distribution towards the most probable tokens."
      ],
      "metadata": {
        "id": "3kdVu2JRp1Yi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Demo: Softmax with temperature"
      ],
      "metadata": {
        "id": "qNo9GVX_r9g_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Softmax function with temperature parameter\n",
        "def softmax_with_temperature(probs, temperature):\n",
        "    exp_probs = np.exp(np.log(probs) / temperature)\n",
        "    return exp_probs / np.sum(exp_probs)"
      ],
      "metadata": {
        "id": "Wt5DWn9psCqD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from ipywidgets import interact\n",
        "\n",
        "# Define a small probability distribution\n",
        "probs = np.array([0.5, 0.3, 0.1, 0.05, 0.05])\n",
        "\n",
        "# Plot the distribution with numbers on top of each bar\n",
        "def plot_distribution(temperature):\n",
        "    adjusted_probs = softmax_with_temperature(probs, temperature)\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    bars = plt.bar(range(len(probs)), adjusted_probs, tick_label=['A', 'B', 'C', 'D', 'E'])\n",
        "    plt.ylim(0, 1)\n",
        "    plt.title(f'Softmax with Temperature = {temperature:.2f}')\n",
        "    plt.ylabel('Probability')\n",
        "    plt.xlabel('Categories')\n",
        "\n",
        "    # Add numbers on top of each bar\n",
        "    for bar in bars:\n",
        "        yval = bar.get_height()\n",
        "        plt.text(bar.get_x() + bar.get_width()/2, yval, f'{yval:.2f}', ha='center', va='bottom')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# Interactive widget\n",
        "interact(plot_distribution, temperature=(0.1, 2.0, 0.1));"
      ],
      "metadata": {
        "id": "pz-eytfxLA6I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Demo: Alter the `temperature` parameter\n",
        "\n",
        "In this demo, we generate several messages from the same prompt.\n",
        "If we lower the temperature to 0, what do you notice about the results?\n",
        "What if we raise it above 1.0?"
      ],
      "metadata": {
        "id": "QiAOWzFosPZ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"Write a python function that reverses as tring. Tell a joke in the docstring!\"},\n",
        "]\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    messages=messages,\n",
        "    temperature=2,\n",
        "    n=3\n",
        ")"
      ],
      "metadata": {
        "id": "pOnm4SUoKSFu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for choice in response.choices:\n",
        "    print(choice.message.content)\n",
        "    print('\\n\\n' + '='*50 + '\\n')"
      ],
      "metadata": {
        "id": "IJNXwiCmKhJW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### `top_k`"
      ],
      "metadata": {
        "id": "WxkYh5ZkYrb-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### `top_p`"
      ],
      "metadata": {
        "id": "d-x9f33bYvPN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Advanced chatbot with hyperparameter controls"
      ],
      "metadata": {
        "id": "ktB3q7OrYW72"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def user(message:str, history:list, system_message:str):\n",
        "    if not history and system_message:\n",
        "        history.append(\n",
        "            {\"role\": \"system\", \"content\": system_message},\n",
        "        )\n",
        "    # Append the user's message to the conversation history\n",
        "    history.append({\"role\": \"user\", \"content\": message})\n",
        "    return \"\", history\n",
        "\n",
        "def bot(history, temperature, top_p):\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        messages=history,\n",
        "        temperature=temperature,\n",
        "        top_p=top_p,\n",
        "        stream=True,\n",
        "    )\n",
        "    history.append({\"role\": \"assistant\", \"content\": \"\"})\n",
        "    for chunk in response:\n",
        "        if chunk.choices[0].delta.content is not None:\n",
        "              history[-1]['content'] += chunk.choices[0].delta.content\n",
        "              yield history\n",
        "\n",
        "def clear():\n",
        "    return \"\", []"
      ],
      "metadata": {
        "id": "ozv16NX5WLzT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with gr.Blocks() as demo:\n",
        "    with gr.Row():\n",
        "        with gr.Column(scale=1):\n",
        "            temperature = gr.Slider(minimum=0, maximum=1, value=0.7, label=\"Temperature\")\n",
        "            top_p = gr.Slider(minimum=0, maximum=1, value=1, label=\"Top p\")\n",
        "            system_message = gr.Textbox(label=\"System message\", value=\"You are a helpful assistant who speaks like a pirate.\")\n",
        "            clear_button = gr.Button(\"Clear messages\")\n",
        "        with gr.Column(scale=3):\n",
        "            chatbot = gr.Chatbot(type='messages')\n",
        "            input = gr.Textbox(label=\"Message\", value=\"\")\n",
        "            input.submit(user, [input, chatbot, system_message], [input, chatbot]).then(\n",
        "                bot, [chatbot, temperature, top_p], chatbot\n",
        "            )\n",
        "            clear_button.click(clear, [], [input, chatbot])\n",
        "\n",
        "demo.launch()\n",
        "\n"
      ],
      "metadata": {
        "id": "S_qTM-_-UkG6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#"
      ],
      "metadata": {
        "id": "3Uj86BPRVHLm"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3EY-BXuHYfzc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prompt engineering 1: few-shot promting"
      ],
      "metadata": {
        "id": "0vnpPQUOZpPX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Embeddings and vector stores"
      ],
      "metadata": {
        "id": "tfedNl_UZyWj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prompt engineering 2: dynamic few-shot prompting"
      ],
      "metadata": {
        "id": "whSrknBwZ1hA"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cdM9igtyZ4ZA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}