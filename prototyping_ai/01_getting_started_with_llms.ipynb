{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "gpuType": "L4",
      "authorship_tag": "ABX9TyPfpGyH7H5hp7EBOWDd9owb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mgfrantz/CTME-llm-lecture-resources/blob/main/prototyping_ai/01_getting_started_with_llms.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Making our first LLM API call\n",
        "\n"
      ],
      "metadata": {
        "id": "4HDOGmN6OmF0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ],
      "metadata": {
        "id": "uZDWgZfZQGcU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JYQyB8QUIhOm"
      },
      "outputs": [],
      "source": [
        "!pip install -Uqqqq \\\n",
        "    openai \\\n",
        "    chromadb \\\n",
        "    sentence-transformers \\\n",
        "    llama-index \\\n",
        "    llama-index-llms-openai \\\n",
        "    gradio \\\n",
        "    datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Basic imports\n",
        "from rich import print\n",
        "import gradio as gr\n",
        "import numpy as np\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from ipywidgets import interact\n",
        "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix, classification_report, cohen_kappa_score, mean_absolute_error\n",
        "\n",
        "# OpenAI\n",
        "from openai import OpenAI"
      ],
      "metadata": {
        "id": "z7bXGANBPZk-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting our `OPENAI_API_KEY` environment variable\n",
        "\n",
        "When we use any LLM provider like OpenAI, Anthropic, or Google, we need some way to tell them who we are making the request.\n",
        "Today, we'll be using OpenAI.\n",
        "The most straightforward way to provide this key is through the environment variable `OPENAI_API_KEY`.\n",
        "The OpenAI python client looks for this environment variable to use in authentication.\n",
        "\n",
        "In the cell below, we load it from the Google Colab secrets manager on the left 👈.\n",
        "Before runnign this code, make sure your API key is set as shown below:\n",
        "\n",
        "![](https://github.com/mgfrantz/CTME-llm-lecture-resources/blob/main/images/colabSecrets.png?raw=true)"
      ],
      "metadata": {
        "id": "XfqCJ9bmLDG2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the OPENAI_API_KEY environment variable\n",
        "from google.colab import userdata # import the environment variables from secrets\n",
        "os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY') # Set the OPENAI_API_KEY environmnet variable"
      ],
      "metadata": {
        "id": "N2nPVwzrIpPQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Under the hood: `curl`\n",
        "\n",
        "Almost all of the interactions we will have with LLMs are through API calls.\n",
        "Below is one of the most low-level ways we can call an LLM, using the `curl` command.\n",
        "This command gives us a lot of information about how the API request is structured.\n",
        "We pass a JSON with an authorization header containing our `OPENAI_API_KEY`.\n",
        "We also pass the model we want to call, the chat messages, and hyperparameters such as `temperature` that help control how text is generated."
      ],
      "metadata": {
        "id": "e-_7WDTwPyZo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make the API call to OpenAI and store the response in test.json\n",
        "!curl https://api.openai.com/v1/chat/completions \\\n",
        "  -H \"Content-Type: application/json\" \\\n",
        "  -H \"Authorization: Bearer $OPENAI_API_KEY\" \\\n",
        "  -d '{ \\\n",
        "     \"model\": \"gpt-4o-mini\", \\\n",
        "     \"messages\": [{\"role\": \"user\", \"content\": \"Say: This is a test!\"}], \\\n",
        "     \"temperature\": 0.7 \\\n",
        "    }' > test.json\n",
        "# Show the output of test.json formatted nicely\n",
        "!cat test.json | python -m json.tool"
      ],
      "metadata": {
        "id": "s47r5FtcOyLG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using the OpenAI Python client\n",
        "\n",
        "While the `curl` command shows us how the API call is made, it's not something that we can easily use in more complex applications.\n",
        "One thing we can use is the OpenAI python client.\n",
        "We can do the exact same thing, but the API call is a bit more abstracted from the developer.\n",
        "Let's see how to perform the exact same API call using the OpenAI clinet 👇:"
      ],
      "metadata": {
        "id": "DPRgn2_IP3w6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "client = OpenAI() # Create the OpenAI client"
      ],
      "metadata": {
        "id": "fPRvearuIz86"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the messages (same as above)\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"Say: This is a test!\"},\n",
        "]\n",
        "\n",
        "# Make the API call\n",
        "chat_completion = client.chat.completions.create(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    messages=messages,\n",
        "    # stream=True\n",
        ")"
      ],
      "metadata": {
        "id": "vYbGuKC8I3na"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the output\n",
        "print(chat_completion)"
      ],
      "metadata": {
        "id": "2gd5AitgI9kU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build our first chatbots\n",
        "\n",
        "In this section, we'll go through several demos.\n",
        "By the end of this section, you should be able to:\n",
        "- Build a basic chatbot with the popular `gradio` Python library\n",
        "- Understand key hyperparameters like `temperature`, `top_p`, and `top_k`\n",
        "- Build an advanced chatbot with hyperparameter controls"
      ],
      "metadata": {
        "id": "yJ0qYsS0Y4DZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise: Building a basic chatbot with `gradio`\n",
        "\n",
        "User interfaces (UIs) are a great way to demo work in AI.\n",
        "In the next several lessons, we will be using the `gradio` framework to demonstrate our growing skillset.\n",
        "In this exercise, we will get a gentle introduction to creating chatbots with `gradio`.\n",
        "\n",
        "Please follow the [ChatInterface](https://www.gradio.app/docs/gradio/chatinterface) documentation and the [Creating a chatbot fast](https://www.gradio.app/guides/creating-a-chatbot-fast) guide to make your first AI chatbot.\n",
        "Your chatbot must:\n",
        "- respond to messages\n",
        "\n",
        "If this too easy, try to:\n",
        "- add a system prompt\n",
        "- use `stream=True` in your chat function"
      ],
      "metadata": {
        "id": "JTQ-yAmFQVOM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code here"
      ],
      "metadata": {
        "id": "zy5lPNy9R7ur"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text generation hyperparameters\n",
        "\n",
        "There are several hyperparameters we can play with that determine how text is generated.\n",
        "For each token, the model outputs a score distribution over words, and that distribution is normalized using the softmax function to sum to 1.0.\n",
        "We have several options to modify this probability distribution in ways that affect the way text is generated."
      ],
      "metadata": {
        "id": "-bHAfnK9YizZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### `temperature`\n",
        "\n",
        "The softmax function is shown below:\n",
        "\n",
        "$$\n",
        "\\text{softmax}(p) = \\frac{e^{x_i}}{\\sum_{j=0}^ne^{x_j}}\n",
        "$$\n",
        "\n",
        "The softmax function is defined in python below:\n"
      ],
      "metadata": {
        "id": "pibw6nM-YncC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax(p):\n",
        "    return np.exp(p) / np.sum(np.exp(p))\n",
        "\n",
        "# Example usage\n",
        "p = np.array([1, 2, 3, 4, 5])\n",
        "print(softmax(p))"
      ],
      "metadata": {
        "id": "bIskN3MLMdmx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `temperature` paramater allows us to make the most probable words more probable (temperature < 1) or less probable (temperature > 1) than vanilla softmax (temperature = 1).\n",
        "The formula for softmax with temperature is show below:\n",
        "\n",
        "$$\n",
        "\\text{softmax}(p, T) = \\frac{e^{\\frac{x_i}{T}}}{\\sum_{j=0}^ne^{\\frac{x_j}{T}}}\n",
        "$$\n",
        "\n",
        "All you do is divide everything by T before taking the exponent; larger values of $T$ flatten the distribution, while smaller values of $T$ skew the distribution towards the most probable tokens."
      ],
      "metadata": {
        "id": "3kdVu2JRp1Yi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Demo: Softmax with temperature"
      ],
      "metadata": {
        "id": "qNo9GVX_r9g_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Softmax function with temperature parameter\n",
        "def softmax_with_temperature(probs, temperature):\n",
        "    exp_probs = np.exp(np.log(probs) / temperature)\n",
        "    return exp_probs / np.sum(exp_probs)"
      ],
      "metadata": {
        "id": "Wt5DWn9psCqD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a small probability distribution\n",
        "probs = np.array([0.5, 0.3, 0.1, 0.05, 0.05])\n",
        "\n",
        "# Plot the distribution with numbers on top of each bar\n",
        "def plot_distribution(temperature):\n",
        "    adjusted_probs = softmax_with_temperature(probs, temperature)\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    bars = plt.bar(range(len(probs)), adjusted_probs, tick_label=['A', 'B', 'C', 'D', 'E'])\n",
        "    plt.ylim(0, 1)\n",
        "    plt.title(f'Softmax with Temperature = {temperature:.2f}')\n",
        "    plt.ylabel('Probability')\n",
        "    plt.xlabel('Tokens')\n",
        "\n",
        "    # Add numbers on top of each bar\n",
        "    for bar in bars:\n",
        "        yval = bar.get_height()\n",
        "        plt.text(bar.get_x() + bar.get_width()/2, yval, f'{yval:.2f}', ha='center', va='bottom')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# Interactive widget\n",
        "interact(plot_distribution, temperature=(0.1, 2.0, 0.1));"
      ],
      "metadata": {
        "id": "pz-eytfxLA6I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Demo: Alter the `temperature` parameter\n",
        "\n",
        "In this demo, we generate several messages from the same prompt.\n",
        "If we lower the temperature to 0, what do you notice about the results?\n",
        "What if we raise it above 1.0?"
      ],
      "metadata": {
        "id": "QiAOWzFosPZ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"Write a python function that reverses as tring. Tell a joke in the docstring!\"},\n",
        "]\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    messages=messages,\n",
        "    temperature=1.0, # change this number between 0 and 2 to see the outcome\n",
        "    n=3 # generate 3 choices\n",
        ")"
      ],
      "metadata": {
        "id": "pOnm4SUoKSFu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for choice in response.choices:\n",
        "    print(choice.message.content)\n",
        "    print('\\n\\n' + '='*50 + '\\n')"
      ],
      "metadata": {
        "id": "IJNXwiCmKhJW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Demo: `top_k`\n",
        "\n",
        "In top k sampling, we define the number of tokens we want to consider to sample from.\n",
        "For example if `top_k = 3`, we will take the scores of the top 3 tokens and apply the softmax to only those 3 scores.\n",
        "Run the code block below to see how `top_k` normalizes the scores at different values."
      ],
      "metadata": {
        "id": "WxkYh5ZkYrb-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a small probability distribution\n",
        "probs = np.array([0.5, 0.3, 0.1, 0.05, 0.05])\n",
        "\n",
        "# Plot the distribution with numbers on top of each bar\n",
        "def plot_top_k(top_k):\n",
        "    ticks = ['A', 'B', 'C', 'D', 'E']\n",
        "    sorted_probs = np.sort(probs)[::-1]\n",
        "    top_k_probs = sorted_probs[:top_k]\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
        "    adjusted_probs = softmax(probs)\n",
        "    ax = axes[0]\n",
        "    bars = ax.bar(range(len(probs)), adjusted_probs, tick_label=ticks)\n",
        "    ax.set_ylim(0, 1)\n",
        "    ax.set_title('Original Probabilities')\n",
        "    ax.set_ylabel('Probability')\n",
        "    ax.set_xlabel('Token')\n",
        "\n",
        "    # Add numbers on top of each bar\n",
        "    for bar in bars:\n",
        "        yval = bar.get_height()\n",
        "        ax.text(bar.get_x() + bar.get_width()/2, yval, f'{yval:.2f}', ha='center', va='bottom')\n",
        "\n",
        "    ax = axes[1]\n",
        "    if top_k < len(probs):\n",
        "        updated_probs = softmax(probs[:top_k])\n",
        "        bars = ax.bar(range(top_k), updated_probs, tick_label=ticks[:top_k])\n",
        "    else:\n",
        "        bars = ax.bar(range(len(probs)), softmax(probs), tick_label=ticks)\n",
        "    ax.set_ylim(0, 1)\n",
        "    ax.set_title(f'Top {top_k} Probabilities')\n",
        "    ax.set_ylabel('Probability')\n",
        "    ax.set_xlabel('Token')\n",
        "    for bar in bars:\n",
        "        yval = bar.get_height()\n",
        "        ax.text(bar.get_x() + bar.get_width()/2, yval, f'{yval:.2f}', ha='center', va='bottom')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# Interactive widget\n",
        "interact(plot_top_k, top_k=(1, len(probs), 1));"
      ],
      "metadata": {
        "id": "uqptc8mIr0hq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Demo: `top_p` (aka nucleus sampling)\n",
        "\n",
        "`top_p` is similar to `top_k`, but instead of defining the number of tokens to consider, you define a cutoff for cumulative probability.\n",
        "For example, if you have a top_p of 0.6 and score of ('cat', 0.4), ('dog', 0.15), ('llama', 0.1), and ('parakeet', 0.01), you would cut only consider 'cat', 'dog', and 'llama' because 0.4 + 0.15 is less than 0.6, but 0.4 + 0.15 + 0.1 is greater.\n",
        "Because you have a probability cutoff instead of number of tokens, this may mean you have different numbers of tokens considered at each decoding step.\n",
        "\n",
        "Play around with the `top_p` slider below 👇 to get some intuition for how it works."
      ],
      "metadata": {
        "id": "d-x9f33bYvPN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a small probability distribution (can simulate a language model's logits)\n",
        "probs = np.array([0.4, 0.2, 0.15, 0.1, 0.08, 0.05, 0.02])\n",
        "\n",
        "# Function to apply top-p filtering with a minimum of one token selected\n",
        "def top_p_filter(probs, p):\n",
        "    sorted_probs = np.sort(probs)[::-1]\n",
        "    cumulative_probs = np.cumsum(sorted_probs)\n",
        "\n",
        "    # Ensure at least one token is selected\n",
        "    if p < sorted_probs[0]:\n",
        "        cutoff = 1\n",
        "    else:\n",
        "        cutoff = np.argmax(cumulative_probs >= p) + 1\n",
        "\n",
        "    filtered_probs = sorted_probs[:cutoff]\n",
        "    return filtered_probs, cutoff\n",
        "\n",
        "# Plot the distribution with top-p filtering\n",
        "def plot_top_p(p):\n",
        "    filtered_probs, cutoff = top_p_filter(probs, p)\n",
        "    normalized_probs = filtered_probs / np.sum(filtered_probs)  # Normalize the selected probabilities\n",
        "\n",
        "    # Create two subplots\n",
        "    fig, axs = plt.subplots(1, 2, figsize=(12, 5))\n",
        "    labels = ['A', 'B', 'C', 'D', 'E', 'F', 'G']\n",
        "\n",
        "    # Plot 1: Original distribution with top-p filtering\n",
        "    bars1 = axs[0].bar(range(len(probs)), np.sort(probs)[::-1], tick_label=labels)\n",
        "    axs[0].set_ylim(0, 1)\n",
        "    axs[0].set_title(f'Top-p Sampling (p = {p:.2f}) - Original Probabilities')\n",
        "    axs[0].set_ylabel('Probability')\n",
        "    axs[0].set_xlabel('Tokens')\n",
        "\n",
        "    # Highlight selected and unselected probabilities\n",
        "    for i, bar in enumerate(bars1):\n",
        "        if i >= cutoff:\n",
        "            bar.set_color('gray')  # Color the bars outside top-p as gray\n",
        "        else:\n",
        "            bar.set_color('blue')  # Highlight the selected probabilities\n",
        "\n",
        "    # Add numbers on top of each bar for original distribution\n",
        "    for bar in bars1:\n",
        "        yval = bar.get_height()\n",
        "        axs[0].text(bar.get_x() + bar.get_width()/2, yval, f'{yval:.2f}', ha='center', va='bottom')\n",
        "\n",
        "    # Plot 2: Normalized probabilities of the selected tokens\n",
        "    bars2 = axs[1].bar(range(len(filtered_probs)), normalized_probs, tick_label=labels[:len(filtered_probs)])\n",
        "    axs[1].set_ylim(0, 1)\n",
        "    axs[1].set_title(f'Normalized Probabilities of Selected Tokens (p = {p:.2f})')\n",
        "    axs[1].set_ylabel('Normalized Probability')\n",
        "    axs[1].set_xlabel('Selected Tokens')\n",
        "\n",
        "    # Add numbers on top of each bar for normalized probabilities\n",
        "    for bar in bars2:\n",
        "        yval = bar.get_height()\n",
        "        axs[1].text(bar.get_x() + bar.get_width()/2, yval, f'{yval:.2f}', ha='center', va='bottom')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Interactive widget\n",
        "interact(plot_top_p, p=(0.01, 1.0, 0.05));"
      ],
      "metadata": {
        "id": "1nUXAXiPsGlA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise: Advanced chatbot with hyperparameter controls\n",
        "\n",
        "Now that you've learned about roles and generation hyperparameters, let's create a new chatbot that allows you to contol at them.\n",
        "Your chatbot must:\n",
        "- allow for control of at least 1 generation hyperparameter (ex: `temperature`)\n",
        "- allow for user input of a system message\n",
        "\n",
        "If this is too easy, try to:\n",
        "- allow for control over `temperature` and `top_p`\n",
        "- improve the UI by putting all the controls in a sidebar\n",
        "- have a `Clear` button that restarts the conversation\n",
        "- add documentation with markdown\n",
        "- implement streaming responses"
      ],
      "metadata": {
        "id": "ktB3q7OrYW72"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code here"
      ],
      "metadata": {
        "id": "ozv16NX5WLzT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prompt engineering 1: zero-shot prompting\n",
        "\n",
        "To really benchmark how each of these techniques do, we need a baseline.\n",
        "We will use zero-shot prompting to get a base level of performance on our task.\n",
        "\n",
        "So far, we've been using the low-level `openai` library.\n",
        "However, there are several very competent higher-level libraries that provide great abstractions such as `langchain` and `llama-index`.\n",
        "Today, we'll be using `llama-index` to make our LLM calls a bit easier."
      ],
      "metadata": {
        "id": "koHpg9S2jthm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare our dataset"
      ],
      "metadata": {
        "id": "cagMwLN4kKVP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prompt engineering imports\n",
        "from datasets import load_dataset, Dataset\n",
        "from llama_index.core import PromptTemplate\n",
        "from llama_index.core.prompts import ChatMessage\n",
        "from llama_index.llms.openai import OpenAI\n",
        "from pydantic import BaseModel, Field\n",
        "from IPython.display import display\n",
        "import asyncio"
      ],
      "metadata": {
        "id": "IslHF3_ak71j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds = load_dataset('SetFit/amazon_reviews_multi_en')"
      ],
      "metadata": {
        "id": "YAkjCmM4yP7T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_samples_per_class = 50\n",
        "eval_test_samples_per_class = 10\n",
        "train = Dataset.from_pandas(ds['train'].to_pandas().groupby('label').sample(train_samples_per_class, random_state=1234).reset_index(drop=True))\n",
        "valid = Dataset.from_pandas(ds['validation'].to_pandas().groupby('label').sample(eval_test_samples_per_class, random_state=1234).reset_index(drop=True))\n",
        "test = Dataset.from_pandas(ds['test'].to_pandas().groupby('label').sample(eval_test_samples_per_class, random_state=1234).reset_index(drop=True))"
      ],
      "metadata": {
        "id": "lRlkTBciyz7S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.to_pandas().sample(3)"
      ],
      "metadata": {
        "id": "uf6l_lA33OV-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "async def predict_and_evaluate(predict_fn):\n",
        "    labels = [int(x) for x in valid['label']]\n",
        "    tasks = [\n",
        "        predict_fn(text)\n",
        "        for text in valid['text']\n",
        "    ]\n",
        "    predictions = await asyncio.gather(*tasks)\n",
        "    cm = ConfusionMatrixDisplay.from_predictions(labels, predictions, normalize='true')\n",
        "    cr = classification_report(labels, predictions)\n",
        "    kappa = cohen_kappa_score(labels, predictions, weights='quadratic')\n",
        "    mae = mean_absolute_error(labels, predictions)\n",
        "    return labels, predictions, kappa, mae, cm, cr"
      ],
      "metadata": {
        "id": "ocJ1AjZ2mMTx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Zero-shot prompt"
      ],
      "metadata": {
        "id": "FMAydzy6kT2k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_tmpl_str = \"\"\"\\\n",
        "The review text is below.\n",
        "---------------------\n",
        "{review}\n",
        "---------------------\n",
        "Given the review text and not prior knowledge, \\\n",
        "please attempt to predict the review score of the context.\n",
        "\n",
        "Query: What is the rating of this review?\n",
        "Answer: \\\n",
        "\"\"\"\n",
        "\n",
        "prompt_tmpl = PromptTemplate(\n",
        "    prompt_tmpl_str,\n",
        ")"
      ],
      "metadata": {
        "id": "9KNSnSPikdh1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Rating(BaseModel):\n",
        "    rating: int = Field(..., description=\"Rating of the review\", enum=[0, 1, 2, 3, 4])\n",
        "\n",
        "llm = OpenAI(model=\"gpt-4o-mini\")\n",
        "zero_shot_structured_llm = llm.as_structured_llm(Rating)"
      ],
      "metadata": {
        "id": "0rJk61JskmHa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "async def zero_shot_predict(text):\n",
        "    messages = [\n",
        "        ChatMessage.from_str(prompt_tmpl.format(review=text))\n",
        "    ]\n",
        "    response = await zero_shot_structured_llm.achat(messages)\n",
        "    return response.raw.rating"
      ],
      "metadata": {
        "id": "idqg4yf-lPDi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "zero_shot_labels, zero_shot_predictions, zero_shot_kappa, zero_shot_mae, zero_shot_cm, zero_shot_cr = await predict_and_evaluate(zero_shot_predict)\n",
        "print(f\"Cohen's Kappa: {zero_shot_kappa:.04f}, MAE: {zero_shot_mae}\")\n",
        "print(zero_shot_cr)"
      ],
      "metadata": {
        "id": "8bOBOElala6d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prompt engineering 2: few-shot promting"
      ],
      "metadata": {
        "id": "0vnpPQUOZpPX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the following cell, we load and parse the data.\n",
        "The data here is uber reviews"
      ],
      "metadata": {
        "id": "W-_rhqOV7lzY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train.shuffle()[:5]"
      ],
      "metadata": {
        "id": "dozUZ9OS433K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def random_few_shot_examples_fn(**kwargs):\n",
        "    random_examples = train.shuffle()[:5]\n",
        "    result_strs = []\n",
        "    for text, rating in zip(random_examples['text'], random_examples['label']):\n",
        "        result_strs.append(f\"Text: {text}\\nRating: {rating}\")\n",
        "    return \"\\n\\n\".join(result_strs)"
      ],
      "metadata": {
        "id": "OGLdG-Hz3a4h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(random_few_shot_examples_fn())"
      ],
      "metadata": {
        "id": "dYCRyZFT48eM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "few_shot_prompt_tmpl_str = \"\"\"\\\n",
        "The review text is below.\n",
        "---------------------\n",
        "{review}\n",
        "---------------------\n",
        "Given the review text and not prior knowledge, \\\n",
        "please attempt to predict the review score of the context. \\\n",
        "Here are several examples of reviews and their ratings:\n",
        "\n",
        "{random_few_shot_examples}\n",
        "\n",
        "Query: What is the rating of this review?\n",
        "Answer: \\\n",
        "\"\"\"\n",
        "\n",
        "few_shot_prompt_tmpl = PromptTemplate(\n",
        "    few_shot_prompt_tmpl_str,\n",
        "    function_mappings={\"random_few_shot_examples\": random_few_shot_examples_fn},\n",
        ")"
      ],
      "metadata": {
        "id": "GGk-w7fL5XW5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(few_shot_prompt_tmpl.format(review='I loved this product!'))"
      ],
      "metadata": {
        "id": "apaZeBI35qcL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Rating(BaseModel):\n",
        "    rating: int = Field(..., description=\"Rating of the review\", enum=[0, 1, 2, 3, 4])\n",
        "\n",
        "llm = OpenAI(model=\"gpt-4o-mini\")\n",
        "rand_few_shot_structured_llm = llm.as_structured_llm(Rating)"
      ],
      "metadata": {
        "id": "PSGk69O33Hfo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "async def random_few_shot_predict(text):\n",
        "    messages = [\n",
        "        ChatMessage.from_str(few_shot_prompt_tmpl.format(review=text))\n",
        "    ]\n",
        "    response = await rand_few_shot_structured_llm.achat(messages)\n",
        "    return response.raw.rating"
      ],
      "metadata": {
        "id": "TBZuZeBK440q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random_few_shot_labels, random_few_shot_predictions, random_few_shot_kappa, random_few_shot_mae, random_few_shot_cm, random_few_shot_cr = await predict_and_evaluate(random_few_shot_predict)\n",
        "print(f\"Cohen's Kappa: {random_few_shot_kappa:.04f}, MAE: {random_few_shot_mae}\")\n",
        "print(random_few_shot_cr)"
      ],
      "metadata": {
        "id": "QRAMqSGKre1j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Demo: Embeddings and vector stores\n",
        "\n",
        "In the previous demonstration, we saw that providing several randomly-selected examples to the LLM at inference time does decently well - it's decently good at predicting the review score, especially within 1 point of the actual review.\n",
        "In a little bit, we'll see that providing better examples to the model at inference time helps improve these scores.\n",
        "But we need efficient ways of searching over our `train` examples to determine which one to use.\n",
        "\n",
        "This is when you want to use a vector store.\n",
        "Vector stores can be in-memory stors, on-disk stores, database extensions like pgvector for Postgres, or even external APIs like Pinecone.\n",
        "\n",
        "Today, we'll use a popular open-source vectore database called `chromadb`.\n",
        "This tool allows us to ingest our documents and search over them effectively to determine which examples to use.\n",
        "\n",
        "In this demo, we'll go over the basics of how to use ChromaDB.\n",
        "We will also use `sentence-transformers` for embeddings as an example of how to use open-weights embedding models."
      ],
      "metadata": {
        "id": "tfedNl_UZyWj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "from chromadb import Client\n",
        "from chromadb.utils.embedding_functions import SentenceTransformerEmbeddingFunction"
      ],
      "metadata": {
        "id": "ij1wKj_P8hbl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this cell, we declare our embedding function.\n",
        "We will use the small but powerful [BGE-small](https://huggingface.co/BAAI/bge-small-en-v1.5) model to embed our documents."
      ],
      "metadata": {
        "id": "89FH9DtpcI3r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embed_fn = SentenceTransformerEmbeddingFunction('BAAI/bge-small-en-v1.5')"
      ],
      "metadata": {
        "id": "WpQdhTRP92MM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we can create our `chromadb` client and use it to create our collection (think table).\n",
        "Notice that we pass our embedding function.\n",
        "That way, when we add documents to the table, the the text is automatically embedded."
      ],
      "metadata": {
        "id": "WQtyevhXceZ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chroma_client = Client()"
      ],
      "metadata": {
        "id": "gjN4Byls-sl4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reviews = chroma_client.create_collection(\n",
        "    name='reviews',\n",
        "    embedding_function=embed_fn,\n",
        "    get_or_create=True\n",
        ")"
      ],
      "metadata": {
        "id": "TU6DBPIP-tyS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reviews.add(documents=train['text'], metadatas=[{'rating': x} for x in train['label']], ids=train['id'])"
      ],
      "metadata": {
        "id": "dMu6Dqs4_3Bm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once we have created our vector store, we can search over it using plain text.\n",
        "Here are 3 queries - good review, a neutral review, and a bad review.\n",
        "Let's search our train documents and observe the average rating for the closest 5 documents to each fake review."
      ],
      "metadata": {
        "id": "sJuCCU5pcvvU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "queries = [\n",
        "    \"This product is great!\",\n",
        "    \"This product was pretty typical - not good or bad.\",\n",
        "    \"This product was awful\",\n",
        "]"
      ],
      "metadata": {
        "id": "QHhwQkgSZmQW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retrievals = reviews.query(\n",
        "    query_texts=queries,\n",
        "    n_results=5\n",
        ")"
      ],
      "metadata": {
        "id": "iNW6jiGX-1kg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for query, metadatas in zip(queries, retrievals['metadatas']):\n",
        "    ratings = [float(i.get('rating')) for i in metadatas]\n",
        "    print(f\"Review {query}\")\n",
        "    print(f\"Avg rating of retrieved passages: {np.mean(ratings)}\")"
      ],
      "metadata": {
        "id": "226vy3W6Y9kz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reviews.query(query_texts = 'hello!')['documents']"
      ],
      "metadata": {
        "id": "sQOW_7KZ30XH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Discussion: Using vector stores\n",
        "\n",
        "Now that we have our data encoded this way, can anyone tell me how we might use this object to improve the way we classify reviews?"
      ],
      "metadata": {
        "id": "UIRfHOCFdBY_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prompt engineering 3: dynamic few-shot prompting"
      ],
      "metadata": {
        "id": "whSrknBwZ1hA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise: gathering examples\n",
        "\n",
        "Using the examples of few-shot learning and our vector store from the last module, complete the `dynamic_few_shot_examples` function.\n",
        "It should return a string with all the formatted examples retrieved from the vector store."
      ],
      "metadata": {
        "id": "dAANnpVc7QJ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def dynamic_few_shot_examples_fn(**kwargs):\n",
        "    n_examples = kwargs.get('n_examples', 5)\n",
        "    query_text = kwargs.get('review')\n",
        "    raise NotImplementedError(\"Please implement the rest of this function. It should return a string.\")"
      ],
      "metadata": {
        "id": "cdM9igtyZ4ZA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dynamic_few_shot_examples_fn(review=\"This is the best product!\"))"
      ],
      "metadata": {
        "id": "hSjfOxUP1V69"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dynamic_few_shot_examples_fn(review=\"This is the worst product!\", n_examples=2))"
      ],
      "metadata": {
        "id": "n47biWMBqt2t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercose: dynamic few shot prompt template\n",
        "\n",
        "Based on the examples from earlier in this lab, please create the dynamic few shot prompt template string and prompt template."
      ],
      "metadata": {
        "id": "lrBAj6jO8Gqv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dynamic_few_shot_prompt_tmpl_str = None\n",
        "\n",
        "dynamic_few_shot_prompt_tmpl = None"
      ],
      "metadata": {
        "id": "SFqomajG4YyU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dynamic_few_shot_prompt_tmpl.format(review='I loved this product!', n_examples=1))"
      ],
      "metadata": {
        "id": "9UJsdk9Ypknd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Rating(BaseModel):\n",
        "    rating: int = Field(..., description=\"Rating of the review\", enum=[0, 1, 2, 3, 4])\n",
        "\n",
        "llm = OpenAI(model=\"gpt-4o-mini\")\n",
        "dynamic_few_shot_structured_llm = llm.as_structured_llm(Rating)\n",
        "\n",
        "async def dynamic_few_shot_predict(text):\n",
        "    messages = [\n",
        "        ChatMessage.from_str(dynamic_few_shot_prompt_tmpl.format(review=text))\n",
        "    ]\n",
        "    response = await dynamic_few_shot_structured_llm.achat(messages)\n",
        "    return response.raw.rating"
      ],
      "metadata": {
        "id": "3Sg2NzM-1ZXY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dynamic_few_shot_labels, dynamic_few_shot_predictions, dynamic_few_shot_kappa, dynamic_few_shot_mae, dynamic_few_shot_cm, dynamic_few_shot_cr = await predict_and_evaluate(dynamic_few_shot_predict)\n",
        "print(f\"Cohen's Kappa: {dynamic_few_shot_kappa:.04f}, MAE: {dynamic_few_shot_mae}\")\n",
        "print(dynamic_few_shot_cr)"
      ],
      "metadata": {
        "id": "IU7jjKtk4s28"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise: Rating Reviews\n",
        "\n",
        "In today's session, we've learned about:\n",
        "- Chat models, interfaces, and `gradio`\n",
        "- Zero shot prompting\n",
        "- Few shot learning\n",
        "- Embeddings and vector stores\n",
        "- Dynamic few shot learning\n",
        "\n",
        "It's time to combine these principles into our final exercise of the day.\n",
        "Your task is to create a `gradio` app where a user can paste a review from Amazon and the app displays the predicted number of ⭐stars⭐.\n",
        "To complete this task, please:\n",
        "- Create a GradIO app with...\n",
        "  - an input field where a user can submit text\n",
        "  - a submit button and/or functionality to submit the text to the app when the user hits the return key\n",
        "  - an output field to display the predicted result\n",
        "\n",
        "If this is too easy, try to:\n",
        "- Add hyperparameters like the number of examples retrieved\n",
        "- Add details in markdown for how to use the app\n",
        "- Display the prompt and response for inspection\n",
        "- Install the [`gradio-client`](https://pypi.org/project/gradio-client/) library and make requests to your app from this notebook\n",
        "\n",
        "If you're done, and **really** want to challenge yourslef, add a dropdown for a different model.\n",
        "You can follow the `Gemini API keys` button in the 👈secrets🔑 tab of Colab, or follow [this notebook guide](https://github.com/mgfrantz/CTME-llm-lecture-resources/blob/main/resources/ollama.ipynb) on a GPU colab to try doing inference with local LLMs like llama3 (Recommended to restart and use a gpu runtime, runtime > change runtime type. May not work with `.as_structured_llm`, check out [this low-level guide on structured outputs](https://docs.llamaindex.ai/en/stable/examples/output_parsing/llm_program/)).\n"
      ],
      "metadata": {
        "id": "hA84jzIihOaw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!curl -fsSL https://ollama.com/install.sh | sh\n",
        "!pip install -qqqq llama-index-llms-ollama ollama"
      ],
      "metadata": {
        "id": "rKQOLkXjpaF2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ollama serve > ollama.log 2>&1 &"
      ],
      "metadata": {
        "id": "R3BkNtynrJBZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!ollama pull llama3.2:3b"
      ],
      "metadata": {
        "id": "11nHQ43JrVdJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.llms.ollama import Ollama\n",
        "from llama_index.core.program import LLMTextCompletionProgram\n",
        "from llama_index.core.output_parsers import PydanticOutputParser\n",
        "from tenacity import retry, wait_fixed, stop_after_attempt\n",
        "\n",
        "async def openai_predict(text, n_examples):\n",
        "    OpenAI(model=\"gpt-4o-mini\").as_structured_llm(Rating)\n",
        "    messages = [\n",
        "        ChatMessage.from_str(dynamic_few_shot_prompt_tmpl.format(review=text, n_examples=n_examples))\n",
        "    ]\n",
        "    response = await models[model].achat(messages)\n",
        "    return response.raw.rating, messages[0].content, str(response)\n",
        "\n",
        "@retry(wait=wait_fixed(2), stop=stop_after_attempt(3))\n",
        "def ollama_predict(text, n_examples):\n",
        "    llm = Ollama(model=\"llama3.2:3b\")\n",
        "    output_parser = PydanticOutputParser(Rating)\n",
        "    prompt = dynamic_few_shot_prompt_tmpl.format(review=text, n_examples=n_examples)\n",
        "    program = LLMTextCompletionProgram(llm=llm, output_parser=output_parser, output_cls=Rating, prompt=dynamic_few_shot_prompt_tmpl)\n",
        "    response = program(review=text, n_examples=n_examples)\n",
        "    return response.rating, prompt, str(response)\n",
        "\n",
        "\n",
        "def _predict(text, model, n_examples):\n",
        "    if model == 'OpenAI':\n",
        "        return asyncio.run(openai_predict(text, n_examples))\n",
        "    elif model == 'Ollama':\n",
        "        return ollama_predict(text, n_examples)"
      ],
      "metadata": {
        "id": "SxfnpqXrpfZO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with gr.Blocks() as demo:\n",
        "\n",
        "    gr.Markdown(\"\"\"\\\n",
        "# Amazon Rating Classifier\n",
        "\n",
        "Ever wondered if your review matches the rating?\n",
        "Copy and paste any Amazon review into the **Review** box and hit return to see the predicted rating.\n",
        "\"\"\")\n",
        "    with gr.Row():\n",
        "        with gr.Column(scale=1):\n",
        "            model_dropdown = gr.Dropdown(\n",
        "                label=\"Model\",\n",
        "                choices=list(models.keys()),\n",
        "                value=list(models.keys())[0],\n",
        "            )\n",
        "            n_examples = gr.Slider(minimum=1, maximum=10, step=1, value=5, label=\"Number of examples\")\n",
        "        with gr.Column(scale=5):\n",
        "            input = gr.Textbox(label=\"Review\")\n",
        "            output = gr.Textbox(label=\"Rating\")\n",
        "\n",
        "    with gr.Row():\n",
        "        message_display = gr.Textbox(label=\"Prompt\")\n",
        "        response_display = gr.Textbox(label=\"Response\")\n",
        "\n",
        "    input.submit(_predict, [input, model_dropdown, n_examples], [output, message_display, response_display])\n",
        "\n",
        "demo.launch()"
      ],
      "metadata": {
        "id": "AYhKKJevSBvx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ChEPfqP9j6Du"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}