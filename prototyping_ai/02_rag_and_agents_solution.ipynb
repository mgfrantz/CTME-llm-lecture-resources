{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval Augmented Generation\n",
    "\n",
    "![](https://www.dailydoseofds.com/content/images/2024/10/rag.gif)\n",
    "Source: [Daily Dose of DS](https://www.dailydoseofds.com/a-crash-course-on-building-rag-systems-part-1-with-implementations/)\n",
    "\n",
    "Overview:\n",
    "- Introduction to RAG with LlamaIndex\n",
    "- Data ingestion\n",
    "  - PDF\n",
    "  - Web pages\n",
    "  - Code\n",
    "- Data splitting\n",
    "  - Token splitting\n",
    "  - Sentence splitting\n",
    "  - Structured data splitting\n",
    "  - Semantic chunking\n",
    "- Vectorization\n",
    "  - Embeddings\n",
    "  - Vector storage\n",
    "- Retrieval\n",
    "  - Keyword search\n",
    "  - Vector search\n",
    "  - Hybrid search\n",
    "- Advanced methods\n",
    "  - Query rewriting\n",
    "  - Multi-hop retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext rich"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to RAG with LlamaIndex\n",
    "\n",
    "LlamaIndex is a library for working with large language models.\n",
    "One of its main strengths is its ability to ingest documents into a vector index and use them to answer questions.\n",
    "This is known as Retrieval Augmented Generation (RAG).\n",
    "\n",
    "To start, we will use a low-code, high-level abstraction to build a basic PDF question-answering system.\n",
    "We will read in PDFs, split them into chunks, embed them, and store them in a vector database.\n",
    "Then, we will use an abstraction known as a `QueryEngine` that implements RAG to answer questions about the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we're in colab, use userdata to get the OPENAI_API_KEY\n",
    "import os\n",
    "from rich import print\n",
    "from pathlib import Path\n",
    "\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    print(\"Colab detected - setting up environment\")\n",
    "    os.environ[\"OPENAI_API_KEY\"] = userdata.get(\"OPENAI_API_KEY\")\n",
    "    %pip install llama-index \\\n",
    "        llama-index-readers-web \\\n",
    "        thefuzz \\\n",
    "        gradio \\\n",
    "        chromadb \\\n",
    "        llama-index-embeddings-huggingface \\\n",
    "        llama-index-vector-stores-chroma \\\n",
    "        llama-index-retrievers-bm25 \\\n",
    "        llama-index-llms-gemini \\\n",
    "        docling \\\n",
    "        llama-index-readers-docling \\\n",
    "        llama-index-node-parser-docling \\\n",
    "        bm25s \\\n",
    "        PyStemmer\n",
    "        \n",
    "except:\n",
    "    print(\"Not in colab - using local environment variables.\")\n",
    "    from dotenv import load_dotenv\n",
    "    load_dotenv(\"../.env\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "\n",
    "# Create data directory if it doesn't exist\n",
    "data_dir = \"data\"\n",
    "if not os.path.exists(data_dir):\n",
    "    os.makedirs(data_dir)\n",
    "\n",
    "# Download the PDF file\n",
    "pdf_url = \"https://arxiv.org/pdf/2407.21783\"\n",
    "pdf_path = os.path.join(data_dir, \"2407.21783.pdf\")\n",
    "\n",
    "if not os.path.exists(pdf_path):\n",
    "    response = requests.get(pdf_url)\n",
    "    with open(pdf_path, \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "    print(f\"Downloaded PDF to {pdf_path}\")\n",
    "else:\n",
    "    print(f\"PDF already exists at {pdf_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we need to do is to load the data.\n",
    "For general documents like PDFs, LlamaIndex provides a nice abstraction known as a `SimpleDirectoryReader` that can load data from a directory.\n",
    "In the cell below, we use it to load the data from the `data` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.readers import SimpleDirectoryReader\n",
    "documents = SimpleDirectoryReader(data_dir).load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the first document.\n",
    "Most prominently, we get the text of the document.\n",
    "By default, we also get a lot of useful information, like the page number file name, file path, type, size, etc.\n",
    "When we load lots of documents, this type of information becomes important to keep track of."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(documents[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've loaded the data, we need to vectorize it.\n",
    "We will use a combination of an embedding model and a vector database to store the vectors.\n",
    "In the cell below, we use a HuggingFace embedding model to embed the documents.\n",
    "We also use torch to determine the device to use for the embedding model (`mps` for Mac GPUs, `cuda` for Nvidia GPUs, and `cpu` otherwise)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from torch.backends.mps import is_available as is_mps_available\n",
    "from torch.cuda import is_available as is_cuda_available\n",
    "\n",
    "if is_mps_available():\n",
    "    device = \"mps\"\n",
    "elif is_cuda_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-large-en-v1.5\", device=device)\n",
    "index = VectorStoreIndex.from_documents(documents, embed_model=embed_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's set up our RAG query engine.\n",
    "If we want to perform simple question-answering, we can use the `as_query_engine` method.\n",
    "If we want to perform chat with history, we can use the `as_chat_engine` method.\n",
    "We can see both below ðŸ‘‡.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core import Settings\n",
    "llm = OpenAI(model=\"gpt-4o-mini\")\n",
    "Settings.llm = llm # set gpt-4o-mini as the default llm\n",
    "\n",
    "query_engine = index.as_query_engine(llm=llm)\n",
    "chat_engine = index.as_chat_engine(llm=llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how the query enine works.\n",
    "We start by passing it a question, then it uses the retriever to find the most relevant documents.\n",
    "Finally, it uses the LLM to answer the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_engine.query(\"How many new Llama models models are mentioned in the paper?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we see the response text.\n",
    "But there's also some additional information that we can access, including the source nodes.\n",
    "These are the nodes that the retriever used to answer the question.\n",
    "We can see that the retriever found several nodes that are relevant to the question, then the LLM used at least one of them to answer the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking a look at the first source node, we can see that it is a node that contains part of the document that is relevant to the question.\n",
    "It also contains the `score`, which is the similarity between the question and the node.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response.source_nodes[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see if our chat enigine comes up with the same answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_response = chat_engine.chat(\"How many new Llama models models are mentioned in the paper?\")\n",
    "print(chat_response.response)\n",
    "print(chat_response.source_nodes[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Create a Gradio interface for a question-answering system\n",
    "\n",
    "Your goal in this exercise is to create a Gradio interface for a question-answering system.\n",
    "Your application should:\n",
    "- Use the query engine created above to answer questions about the uploaded PDF\n",
    "- Display the question and answer in the UI\n",
    "\n",
    "If you need a challenge:\n",
    "- Use the `gr.File` component to allow the user to upload ANY pdf and ask question about it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data ingestion\n",
    "\n",
    "Data often comes in many different formats.\n",
    "It may come in the form of a PDF, a web page, a code file, etc.\n",
    "We may need some specific processing pipelines to extract the text from these documents, split them correctly, and vectorize them.\n",
    "\n",
    "Luckily, LlamaIndex (and other libraries) provide lots of built-in and add-on tools to help you ingest almost any data type.\n",
    "Instead of loading a PDF, let's load a web page instead.\n",
    "We will use one of the classes provided by [`llama-index-readers-web`](https://llamahub.ai/l/readers/llama-index-readers-web?from=readers) to load data from a web page.\n",
    "\n",
    "In this section, we will:\n",
    "- Load a web page as Markdown\n",
    "- Split it into chunks following the structured format of the Markdown\n",
    "- Embed the chunks\n",
    "- Store the chunks in a vector database\n",
    "- Create a query engine from the vector database and use it to answer a question\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.readers.web import SimpleWebPageReader\n",
    "from llama_index.core.node_parser import MarkdownNodeParser\n",
    "from llama_index.core.ingestion import IngestionPipeline\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "import chromadb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "web_docs = SimpleWebPageReader(html_to_text=True).load_data(['https://en.wikipedia.org/wiki/Wikipedia'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = chromadb.EphemeralClient().create_collection(\"wikipedia\", get_or_create=True)\n",
    "vector_store = ChromaVectorStore(collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = IngestionPipeline(\n",
    "    transformations=[\n",
    "        MarkdownNodeParser.from_defaults(),\n",
    "        embed_model,\n",
    "    ], \n",
    "    vector_store=vector_store\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = pipeline.run(documents=web_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = VectorStoreIndex.from_vector_store(vector_store, embed_model=embed_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(model=\"gpt-4o-mini\")\n",
    "query_engine = index.as_query_engine(llm=llm)\n",
    "response = query_engine.query(\"How many languages there exactly? Quote the exact text as well.\")\n",
    "print(response.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use fuzzywuzzy to find the closest match in the source text\n",
    "from thefuzz import fuzz, process\n",
    "# Get the top matching line of text from the source_text_quote\n",
    "top_match, match_score = process.extractOne(response.response, response.source_nodes[0].text.splitlines(), scorer=fuzz.ratio)\n",
    "assert top_match in response.source_nodes[0].text\n",
    "print(f\"Quote from source: '{top_match}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OCR\n",
    "\n",
    "There are times where the PDF is not in a format that can be easily read into text.\n",
    "In these cases, we will need to use optical character recognition (OCR) to convert the images to text.\n",
    "There are many libraries and cloud services that can do this, but for this example, we will use the `docling` library since our example document is a PDF.\n",
    "We will then ingest the text into LlamaIndex and use it to answer a question.\n",
    "\n",
    "![](https://ds4sd.github.io/docling/assets/docling_processing.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from llama_index.readers.docling import DoclingReader\n",
    "from llama_index.node_parser.docling import DoclingNodeParser\n",
    "from llama_index.core.schema import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = DoclingReader(export_type=DoclingReader.ExportType.JSON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docling_docs = reader.load_data([fpath])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(docling_docs[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpath = \"data/2407.21783.pdf\"\n",
    "\n",
    "llama_collection = chromadb.EphemeralClient().create_collection(\"llama\", get_or_create=True)\n",
    "llama_vector_store = ChromaVectorStore(llama_collection)\n",
    "\n",
    "docling_pipeline = IngestionPipeline(\n",
    "    transformations=[\n",
    "        DoclingNodeParser(),\n",
    "        embed_model,\n",
    "    ], \n",
    "    vector_store=llama_vector_store\n",
    ")\n",
    "docling_nodes = docling_pipeline.run(documents=docling_docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(docling_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data splitting\n",
    "\n",
    "Many times, it's impractical to embed the entire document, and expensive to feed the entire document to the LLM.\n",
    "Instead, we can split the document into chunks, embed the chunks, and use a retrieval method to find the most relevant chunks.\n",
    "There are naÃ¯ve methods that split texts into chunks of a specific length with some overlap;\n",
    "there are methods that use the structure of the document to split it into sections (e.g. sections, figures, tables);\n",
    "and there are more advanced methods that use semantic similarity to group the text into chunks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the OCR'd text is just one very long Markdown string, we need to split it into chunks.\n",
    "One nice way to do that is use the inherent structure of Markdown to split it into sections.\n",
    "We do that here with LlamaIndex's `MarkdownNodeParser`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import MarkdownNodeParser\n",
    "from llama_index.core.schema import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "md_nodes = MarkdownNodeParser.from_defaults().get_nodes_from_documents([Document(text=full_text)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(md_nodes[0].text))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting simple: bm25\n",
    "\n",
    "BM25 is a simple retrieval method that uses the BM25 algorithm to score the relevance of each document to the query.\n",
    "The BM25 algorithm is a probabilistic retrieval model that uses the term frequency and inverse document frequency of the query terms to score the relevance of each document.\n",
    "You should be familiar with the basic idea of tf-idf from your NLP class - you can think of BM25 as a generalization of tf-idf that takes into account more factors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.retrievers import BaseRetriever\n",
    "from llama_index.core.schema import NodeWithScore, QueryBundle\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "import bm25s\n",
    "import Stemmer\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BM25Retriever(BaseRetriever):\n",
    "    def __init__(self, index: VectorStoreIndex, stemmer: Stemmer.Stemmer = Stemmer.Stemmer(\"english\"), k: int = 10):\n",
    "        self.index = index\n",
    "        self.stemmer = stemmer\n",
    "        self.k = k\n",
    "        self._ids = []\n",
    "        self._corpus = []\n",
    "        self._setup()\n",
    "\n",
    "    def _tokenize(self, text: str):\n",
    "        return bm25s.tokenize(text, stopwords=\"en\", stemmer=self.stemmer)\n",
    "\n",
    "    def _setup(self):\n",
    "        for doc_id, doc in self.index.docstore.docs.items():\n",
    "            self._ids.append(doc_id)\n",
    "            self._corpus.append(doc.text)\n",
    "        self._bm25 = bm25s.BM25()\n",
    "        corpus_tokens = self._tokenize(self._corpus)\n",
    "        self._bm25.index(corpus_tokens)\n",
    "\n",
    "    def _get_node_from_text(self, text: str, score: float) -> NodeWithScore:\n",
    "        text_idx = self._corpus.index(text)\n",
    "        id = self._ids[text_idx]\n",
    "        node = self.index.docstore.get_node(id)\n",
    "        return NodeWithScore(node=node, score=score)\n",
    "\n",
    "    def _retrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\n",
    "        query_tokens = self._tokenize(query_bundle.query_str)\n",
    "        # scores is a list of tuples (text, score)\n",
    "        documents, scores = self._bm25.retrieve(query_tokens, corpus=self._corpus, k=self.k)\n",
    "        \n",
    "        nodes_with_scores = []\n",
    "        for text, score in zip(documents[0], scores[0]):\n",
    "            node_with_score = self._get_node_from_text(text, float(score))\n",
    "            nodes_with_scores.append(node_with_score)\n",
    "            \n",
    "        return nodes_with_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25 = BM25Retriever(index, k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for node in bm25.retrieve(\"What is the main idea of the paper?\"):\n",
    "    print(f\"Score: {node.score:.4f}\\nText:\\n{node.node.text[:500]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dense retrieval (vector search)\n",
    "\n",
    "BM25 is a simple and fast method that depends on word matching.\n",
    "But if we want to do more complex retrieval, we can use dense retrieval.\n",
    "We represent both our query and documents as vectors and use a similarity metric to find the most relevant documents.\n",
    "This is what's been going on under the hood in the previous examples using `VectorStoreIndex`.\n",
    "\n",
    "Since most of the mechanics are taken care for us under the hood, let's examine what goes on under the hood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_documents = [\n",
    "    \"My favorite type of dog is a golden retriever.\",\n",
    "    \"I like to eat pizza with my friends.\",\n",
    "    \"I like to go to the gym in the morning.\",\n",
    "    \"I like to play basketball with my friends.\",\n",
    "]\n",
    "\n",
    "embeddings = np.array(embed_model.get_text_embedding_batch(sample_documents))\n",
    "\n",
    "query = \"What do I like to do with friends?\"\n",
    "query_embedding = np.array(embed_model.get_text_embedding(query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity(query_embedding.reshape(1, -1), embeddings).squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hybrid search: query rewriting and reciprocal ranking\n",
    "\n",
    "Sometimes, you may want several methods of searching over your data, then combining the results.\n",
    "This is known as hybrid search.\n",
    "\n",
    "Haveing multiple retrievers may not mean having separate objects - we may just have multiple queries.\n",
    "In this example, we'll use an LLM to rewrite our query into multiple queries, then use a dense retriever to find the most relevant documents.\n",
    "Finally, we'll use reciprocal ranking to re-rank the results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.retrievers import QueryFusionRetriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_retriever = index.as_retriever(similarity_top_k=5)\n",
    "hybrid_retriever = QueryFusionRetriever(\n",
    "    [dense_retriever],\n",
    "    num_queries=3,\n",
    "    use_async=False,\n",
    "    mode='reciprocal_rerank',\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = hybrid_retriever.retrieve(\"How many languages does wikipedia have?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise: Chat with PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the ingest_documents function:\n",
    "def ingest_documents(file_obj):\n",
    "    \"\"\"\n",
    "    Process an uploaded PDF file and create a query engine for it.\n",
    "    \n",
    "    Args:\n",
    "        file_obj: Gradio file upload object containing the PDF\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (chat_engine, cleared_file_upload, filename)\n",
    "    \"\"\"\n",
    "    if file_obj is None:\n",
    "        return None, None, \"\"\n",
    "        \n",
    "    with TemporaryDirectory() as temp_dir:\n",
    "        file_path = os.path.join(temp_dir, 'tmp.pdf')\n",
    "        # Get the file bytes from the Gradio upload object\n",
    "        file_bytes = open(file_obj, \"rb\").read()\n",
    "        \n",
    "        with open(file_path, \"wb\") as f:\n",
    "            f.write(file_bytes)\n",
    "            \n",
    "        documents = SimpleDirectoryReader(temp_dir).load_data()\n",
    "        llm = OpenAI(model=\"gpt-4o-mini\")\n",
    "        index = VectorStoreIndex.from_documents(documents, embed_model=embed_model)\n",
    "        chat_engine = index.as_chat_engine(llm=llm)\n",
    "        \n",
    "    return chat_engine, None, Path(file_obj.name).name, []\n",
    "\n",
    "def chat(message, history, chat_engine):\n",
    "    history.append(\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": message\n",
    "        }\n",
    "    )\n",
    "    response = chat_engine.chat(message)\n",
    "    history.append({\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": response.response\n",
    "    })\n",
    "    return history, None\n",
    "\n",
    "# In the Blocks interface:\n",
    "with gr.Blocks() as demo:\n",
    "    chat_engine_state = gr.State()  # Renamed for clarity\n",
    "    gr.Markdown(\"## RAG Demo: Question answering with a PDF\")\n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=1):   \n",
    "            file_upload = gr.File(label=\"Upload a PDF file\", file_types=[\".pdf\"], file_count=\"single\")\n",
    "            submit_button = gr.Button(\"Submit\")\n",
    "            pdf_name = gr.Textbox(label=\"You are asking about...\")\n",
    "        with gr.Column(scale=5):\n",
    "            chatbot = gr.Chatbot(label=\"Chat with the uploaded PDF\", type='messages')\n",
    "            input = gr.Textbox(label=\"Enter a question\", interactive=True)\n",
    "\n",
    "    # Update to only store the chat_engine in the State\n",
    "    submit_button.click(\n",
    "        fn=ingest_documents, \n",
    "        inputs=file_upload, \n",
    "        outputs=[chat_engine_state, file_upload, pdf_name, chatbot]\n",
    "    )\n",
    "    input.submit(\n",
    "        fn=chat, \n",
    "        inputs=[input, chatbot, chat_engine_state], \n",
    "        outputs=[chatbot, input]\n",
    "    )\n",
    "\n",
    "demo.launch(debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
