{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval Augmented Generation\n",
    "\n",
    "![](https://www.dailydoseofds.com/content/images/2024/10/rag.gif)\n",
    "Source: [Daily Dose of DS](https://www.dailydoseofds.com/a-crash-course-on-building-rag-systems-part-1-with-implementations/)\n",
    "\n",
    "Overview:\n",
    "- Introduction to RAG with LlamaIndex\n",
    "- Data ingestion\n",
    "  - PDF\n",
    "  - Web pages\n",
    "  - Code\n",
    "- Data splitting\n",
    "  - Token splitting\n",
    "  - Sentence splitting\n",
    "  - Structured data splitting\n",
    "  - Semantic chunking\n",
    "- Vectorization\n",
    "  - Embeddings\n",
    "  - Vector storage\n",
    "- Retrieval\n",
    "  - Keyword search\n",
    "  - Vector search\n",
    "  - Hybrid search\n",
    "- Advanced methods\n",
    "  - Query rewriting\n",
    "  - Multi-hop retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext rich\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to RAG with LlamaIndex\n",
    "\n",
    "LlamaIndex is a library for working with large language models.\n",
    "One of its main strengths is its ability to ingest documents into a vector index and use them to answer questions.\n",
    "This is known as Retrieval Augmented Generation (RAG).\n",
    "\n",
    "To start, we will use a low-code, high-level abstraction to build a basic PDF question-answering system.\n",
    "We will read in PDFs, split them into chunks, embed them, and store them in a vector database.\n",
    "Then, we will use an abstraction known as a `QueryEngine` that implements RAG to answer questions about the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Not in colab - using local environment variables.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Not in colab - using local environment variables.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# If we're in colab, use userdata to get the OPENAI_API_KEY\n",
    "import os\n",
    "from rich import print\n",
    "from pathlib import Path\n",
    "\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    print(\"Colab detected - setting up environment\")\n",
    "    os.environ[\"OPENAI_API_KEY\"] = userdata.get(\"OPENAI_API_KEY\")\n",
    "    %pip install llama-index \\\n",
    "        llama-index-readers-web \\\n",
    "        thefuzz \\\n",
    "        gradio \\\n",
    "        chromadb \\\n",
    "        llama-index-embeddings-huggingface \\\n",
    "        llama-index-vector-stores-chroma \\\n",
    "        llama-index-retrievers-bm25 \\\n",
    "        llama-index-llms-gemini \\\n",
    "        docling \\\n",
    "        llama-index-readers-docling \\\n",
    "        llama-index-node-parser-docling \\\n",
    "        PyStemmer\n",
    "        \n",
    "except:\n",
    "    print(\"Not in colab - using local environment variables.\")\n",
    "    from dotenv import load_dotenv\n",
    "    load_dotenv(\"../.env\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">PDF already exists at data/<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2407.21783</span>.pdf\n",
       "</pre>\n"
      ],
      "text/plain": [
       "PDF already exists at data/\u001b[1;36m2407.21783\u001b[0m.pdf\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "\n",
    "# Create data directory if it doesn't exist\n",
    "data_dir = \"data\"\n",
    "if not os.path.exists(data_dir):\n",
    "    os.makedirs(data_dir)\n",
    "\n",
    "# Download the PDF file\n",
    "pdf_url = \"https://arxiv.org/pdf/2407.21783\"\n",
    "pdf_path = os.path.join(data_dir, \"2407.21783.pdf\")\n",
    "\n",
    "if not os.path.exists(pdf_path):\n",
    "    response = requests.get(pdf_url)\n",
    "    with open(pdf_path, \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "    print(f\"Downloaded PDF to {pdf_path}\")\n",
    "else:\n",
    "    print(f\"PDF already exists at {pdf_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we need to do is to load the data.\n",
    "For general documents like PDFs, LlamaIndex provides a nice abstraction known as a `SimpleDirectoryReader` that can load data from a directory.\n",
    "In the cell below, we use it to load the data from the `data` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.readers import SimpleDirectoryReader\n",
    "documents = SimpleDirectoryReader(data_dir).load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the first document.\n",
    "Most prominently, we get the text of the document.\n",
    "By default, we also get a lot of useful information, like the page number file name, file path, type, size, etc.\n",
    "When we load lots of documents, this type of information becomes important to keep track of."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(documents[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've loaded the data, we need to vectorize it.\n",
    "We will use a combination of an embedding model and a vector database to store the vectors.\n",
    "In the cell below, we use a HuggingFace embedding model to embed the documents.\n",
    "We also use torch to determine the device to use for the embedding model (`mps` for Mac GPUs, `cuda` for Nvidia GPUs, and `cpu` otherwise)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from torch.backends.mps import is_available as is_mps_available\n",
    "from torch.cuda import is_available as is_cuda_available\n",
    "\n",
    "if is_mps_available():\n",
    "    device = \"mps\"\n",
    "elif is_cuda_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-large-en-v1.5\", device=device)\n",
    "index = VectorStoreIndex.from_documents(documents, embed_model=embed_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's set up our RAG query engine.\n",
    "If we want to perform simple question-answering, we can use the `as_query_engine` method.\n",
    "If we want to perform chat with history, we can use the `as_chat_engine` method.\n",
    "We can see both below ðŸ‘‡.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core import Settings\n",
    "llm = OpenAI(model=\"gpt-4o-mini\")\n",
    "Settings.llm = llm # set gpt-4o-mini as the default llm\n",
    "\n",
    "query_engine = index.as_query_engine(llm=llm)\n",
    "chat_engine = index.as_chat_engine(llm=llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how the query enine works.\n",
    "We start by passing it a question, then it uses the retriever to find the most relevant documents.\n",
    "Finally, it uses the LLM to answer the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_engine.query(\"How many new Llama models models are mentioned in the paper?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we see the response text.\n",
    "But there's also some additional information that we can access, including the source nodes.\n",
    "These are the nodes that the retriever used to answer the question.\n",
    "We can see that the retriever found several nodes that are relevant to the question, then the LLM used at least one of them to answer the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking a look at the first source node, we can see that it is a node that contains part of the document that is relevant to the question.\n",
    "It also contains the `score`, which is the similarity between the question and the node.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response.source_nodes[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see if our chat enigine comes up with the same answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_response = chat_engine.chat(\"How many new Llama models models are mentioned in the paper?\")\n",
    "print(chat_response.response)\n",
    "print(chat_response.source_nodes[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data ingestion\n",
    "\n",
    "Data often comes in many different formats.\n",
    "It may come in the form of a PDF, a web page, a code file, etc.\n",
    "We may need some specific processing pipelines to extract the text from these documents, split them correctly, and vectorize them.\n",
    "\n",
    "Luckily, LlamaIndex (and other libraries) provide lots of built-in and add-on tools to help you ingest almost any data type.\n",
    "Instead of loading a PDF, let's load a web page instead.\n",
    "We will use one of the classes provided by [`llama-index-readers-web`](https://llamahub.ai/l/readers/llama-index-readers-web?from=readers) to load data from a web page.\n",
    "\n",
    "In this section, we will:\n",
    "- Load a web page as Markdown\n",
    "- Split it into chunks following the structured format of the Markdown\n",
    "- Embed the chunks\n",
    "- Store the chunks in a vector database\n",
    "- Create a query engine from the vector database and use it to answer a question\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.readers.web import SimpleWebPageReader\n",
    "from llama_index.core.node_parser import MarkdownNodeParser\n",
    "from llama_index.core.ingestion import IngestionPipeline\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "import chromadb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "web_docs = SimpleWebPageReader(html_to_text=True).load_data(['https://en.wikipedia.org/wiki/Wikipedia'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = chromadb.EphemeralClient().create_collection(\"wikipedia\", get_or_create=True)\n",
    "vector_store = ChromaVectorStore(collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = IngestionPipeline(\n",
    "    transformations=[\n",
    "        MarkdownNodeParser.from_defaults(),\n",
    "        embed_model,\n",
    "    ], \n",
    "    vector_store=vector_store\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = pipeline.run(documents=web_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = VectorStoreIndex.from_vector_store(vector_store, embed_model=embed_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(model=\"gpt-4o-mini\")\n",
    "query_engine = index.as_query_engine(llm=llm)\n",
    "response = query_engine.query(\"How many languages there exactly? Quote the exact text as well.\")\n",
    "print(response.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use fuzzywuzzy to find the closest match in the source text\n",
    "from thefuzz import fuzz, process\n",
    "# Get the top matching line of text from the source_text_quote\n",
    "top_match, match_score = process.extractOne(response.response, response.source_nodes[0].text.splitlines(), scorer=fuzz.ratio)\n",
    "assert top_match in response.source_nodes[0].text\n",
    "print(f\"Quote from source: '{top_match}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OCR\n",
    "\n",
    "There are times where the PDF is not in a format that can be easily read into text.\n",
    "In these cases, we will need to use optical character recognition (OCR) to convert the images to text.\n",
    "There are many libraries and cloud services that can do this, but for this example, we will use the `docling` library since our example document is a PDF.\n",
    "We will then ingest the text into LlamaIndex and use it to answer a question.\n",
    "\n",
    "![](https://ds4sd.github.io/docling/assets/docling_processing.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.readers.docling import DoclingReader\n",
    "from llama_index.node_parser.docling import DoclingNodeParser\n",
    "from llama_index.core.schema import Document\n",
    "from llama_index.core import StorageContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = DoclingReader(export_type=DoclingReader.ExportType.JSON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docling_docs = reader.load_data(pdf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(docling_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to set the text template to \"{content}\" because the default is \"{metadata}\\n\\n{content}\",\n",
    "# and LlamaIndex will try to embed the metadata as well. The metadata is not useful at serach time.\n",
    "docling_docs[0].text_template\n",
    "docling_docs[0].text_template = \"{content}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docling_node_parser = DoclingNodeParser()\n",
    "docling_nodes = docling_node_parser.get_nodes_from_documents(docling_docs)\n",
    "len(docling_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docling_nodes[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = VectorStoreIndex(nodes=docling_nodes, embed_model=embed_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine(llm=llm)\n",
    "response = query_engine.query(\"How many new Llama models are mentioned in the paper?\")\n",
    "print(response.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response.source_nodes[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data splitting\n",
    "\n",
    "Many times, it's impractical to embed the entire document, and expensive to feed the entire document to the LLM.\n",
    "Instead, we can split the document into chunks, embed the chunks, and use a retrieval method to find the most relevant chunks.\n",
    "There are naÃ¯ve methods that split texts into chunks of a specific length with some overlap;\n",
    "there are methods that use the structure of the document to split it into sections (e.g. sections, figures, tables);\n",
    "and there are more advanced methods that use semantic similarity to group the text into chunks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the OCR'd text is just one very long Markdown string, we need to split it into chunks.\n",
    "One nice way to do that is use the inherent structure of Markdown to split it into sections.\n",
    "We do that here with LlamaIndex's `MarkdownNodeParser`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docling_md_docs = DoclingReader(export_type=DoclingReader.ExportType.MARKDOWN).load_data(pdf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import MarkdownNodeParser\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "md_nodes = MarkdownNodeParser.from_defaults().get_nodes_from_documents(documents=docling_md_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(md_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(md_nodes[9].text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md_index = VectorStoreIndex(nodes=md_nodes, embed_model=embed_model)\n",
    "md_query_engine = md_index.as_query_engine(llm=llm)\n",
    "response = md_query_engine.query(\"How many llama3 models are there?\")\n",
    "print(response.response)\n",
    "print(response.source_nodes[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting simple: bm25\n",
    "\n",
    "BM25 is a simple retrieval method that uses the BM25 algorithm to score the relevance of each document to the query.\n",
    "The BM25 algorithm is a probabilistic retrieval model that uses the term frequency and inverse document frequency of the query terms to score the relevance of each document.\n",
    "You should be familiar with the basic idea of tf-idf from your NLP class - you can think of BM25 as a generalization of tf-idf that takes into account more factors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.retrievers.bm25 import BM25Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25 = BM25Retriever.from_defaults(nodes=md_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for node in bm25.retrieve(\"How many llama3 models are there?\"):\n",
    "    print(f\"Score: {node.score:.4f}\\nText:\\n{node.node.text[:500]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dense retrieval (vector search)\n",
    "\n",
    "BM25 is a simple and fast method that depends on word matching.\n",
    "But if we want to do more complex retrieval, we can use dense retrieval.\n",
    "We represent both our query and documents as vectors and use a similarity metric to find the most relevant documents.\n",
    "This is what's been going on under the hood in the previous examples using `VectorStoreIndex`.\n",
    "\n",
    "Since most of the mechanics are taken care for us under the hood, let's examine what goes on under the hood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_documents = [\n",
    "    \"My favorite type of dog is a golden retriever.\",\n",
    "    \"I like to eat pizza with my friends.\",\n",
    "    \"I like to go to the gym in the morning.\",\n",
    "    \"I like to play basketball with my friends.\",\n",
    "]\n",
    "\n",
    "embeddings = np.array(embed_model.get_text_embedding_batch(sample_documents))\n",
    "\n",
    "query = \"What do I like to do with friends?\"\n",
    "query_embedding = np.array(embed_model.get_text_embedding(query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity(query_embedding.reshape(1, -1), embeddings).squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have a `VectorStoreIndex`, we can use the `as_retriever` method to get a retriever object.\n",
    "This uses dense retrieval under the hood, since it already has an embedding model as a part of the class.\n",
    "Here, we use the `similarity_top_k` parameter to limit the number of results to 2 and show the cosine similarity score along with the beginning of the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How many llama3 models are there?\"\n",
    "top_results = md_index.as_retriever(similarity_top_k=2).retrieve(query)\n",
    "for result in top_results:\n",
    "    print(f\"Score: {result.score:.4f}\\nText:\\n{result.node.text[:500]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hybrid search: query rewriting and reciprocal ranking\n",
    "\n",
    "Sometimes, you may want several methods of searching over your data, then combining the results.\n",
    "This is known as hybrid search.\n",
    "\n",
    "Haveing multiple retrievers may not mean having separate objects - we may just have multiple queries.\n",
    "In this example, we'll use an LLM to rewrite our query into multiple queries, then use a dense retriever to find the most relevant documents.\n",
    "Finally, we'll use reciprocal ranking to re-rank the results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.retrievers import QueryFusionRetriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_retriever = md_index.as_retriever(similarity_top_k=5)\n",
    "hybrid_retriever = QueryFusionRetriever(\n",
    "    [dense_retriever],\n",
    "    num_queries=3,\n",
    "    use_async=False,\n",
    "    mode='reciprocal_rerank',\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = hybrid_retriever.retrieve(\"How many llama3 models are there?\")\n",
    "for result in results:\n",
    "    print(f\"Score: {result.score:.4f}\\nText:\\n{result.node.text[:500]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reranking\n",
    "\n",
    "Often, retrieval only gives us a good first pass at finding relevant documents.\n",
    "We can use re-ranking to improve the results.\n",
    "There are several re-ranking methods, but in this case we'll use a cross-encoder to re-rank the results.\n",
    "We'll also introduce a more low-level method for organizing LlamaIndex flows called workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.postprocessor import SentenceTransformerRerank\n",
    "md_query_engine = md_index.as_query_engine(llm=llm, node_postprocessors=[\n",
    "    SentenceTransformerRerank(top_n=5)\n",
    "], retriever_top_k=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = md_query_engine.query(\"How many llama3 models are there?\")\n",
    "print(response.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.schema import NodeWithScore\n",
    "from llama_index.core.llms import ChatMessage\n",
    "from llama_index.core.workflow import (\n",
    "    Workflow,\n",
    "    Context,\n",
    "    step,\n",
    "    StartEvent,\n",
    "    StopEvent,\n",
    "    Event\n",
    ")\n",
    "from typing import List\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's walk throught the code below step by step.\n",
    "First, we define the events that will be passed between the steps.\n",
    "This includes a retrieval result, a re-ranking result, and a prompt for the LLM.\n",
    "The start and stop are already taken care of for us by `StartEvent` and `StopEvent`.\n",
    "\n",
    "Next, we define our `RAGWithReRank` workflow.\n",
    "We initialize it with an index and LLM, which we will use for retrieval and answering and the cross-encoder for re-ranking.\n",
    "We also define exactly what to do for each step of the workflow.\n",
    "It knows what to do because of the `@step` decorator and the types annotations of each method.\n",
    "\n",
    "Finally, we can run the workflow with a query.\n",
    "The workflow will automatically handle passing the events between the steps, so we don't have to worry about that.\n",
    "We can observe that the results are just what we expect as we saw from the previous examples.\n",
    "One advantage of this method is that it's much more flexible - you can include arbitrary code, loops, conditionals, etc.\n",
    "\n",
    "For more detailed documentation on workflows, see [here](https://docs.llamaindex.ai/en/stable/understanding/workflows/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the events that will be passed between the steps\n",
    "class RetrievalResult(Event):\n",
    "    results: List[NodeWithScore]\n",
    "\n",
    "class ReRankResult(Event):\n",
    "    results: List[NodeWithScore]\n",
    "\n",
    "class RagPrompt(Event):\n",
    "    prompt: str\n",
    "\n",
    "# Define the workflow\n",
    "class RAGWithReRank(Workflow):\n",
    "    # Initialize the workflow with an index and LLM\n",
    "    def __init__(self, index: VectorStoreIndex = md_index, llm: OpenAI = OpenAI(model=\"gpt-4o-mini\")):\n",
    "        super().__init__(timeout=10, verbose=False)\n",
    "        self.index = index\n",
    "        self.llm = llm\n",
    "        self.reranker = SentenceTransformerRerank(top_n=5)\n",
    "\n",
    "    # The start method is called when the workflow is run.\n",
    "    # This is the first step of the workflow that takes in a query and returns a retrieval result.\n",
    "    # Since we want to use a course retriever, we use a top k of 20.\n",
    "    @step\n",
    "    async def start(self, ctx: Context, ev: StartEvent) -> RetrievalResult:\n",
    "        query = ev.query\n",
    "        await ctx.set('query', query)\n",
    "        results = self.index.as_retriever(similarity_top_k=20).retrieve(query)\n",
    "        return RetrievalResult(results=results)\n",
    "    \n",
    "    # This step takes the retrieval results and re-ranks them.\n",
    "    # Notice that the similarity top k is set to 5, so we only keep the top 5 results.\n",
    "    # This is smaller than the retrieval step since this is about refining a smaller number of results.\n",
    "    @step\n",
    "    async def rerank(self, ctx: Context, ev: RetrievalResult) -> ReRankResult:\n",
    "        results = ev.results\n",
    "        query = await ctx.get('query')\n",
    "        reranked = self.reranker.postprocess_nodes(nodes=results, query_str=query)\n",
    "        return ReRankResult(results=reranked)\n",
    "    \n",
    "    # This step creates a prompt for the LLM.\n",
    "    # It does this by joining the re-ranked results into a single string and formatting it with the query.\n",
    "    @step\n",
    "    async def create_prompt(self, ctx: Context, ev: ReRankResult) -> RagPrompt:\n",
    "        reranked_results = ev.results\n",
    "        await ctx.set('reranked_results', reranked_results)\n",
    "        query = await ctx.get('query')\n",
    "        reranked_str = '\\n\\n'.join(i.text for i in reranked_results)\n",
    "        prompt = f\"\"\"\\\n",
    "Here is some relevant context:\n",
    "--------------------------------\n",
    "{reranked_str}\n",
    "--------------------------------\n",
    "Based on the above information and not prior knowledge, please answer the question.\n",
    "Question:\n",
    "{query}\n",
    "\"\"\"\n",
    "        return RagPrompt(prompt=prompt)\n",
    "\n",
    "    # This step takes the prompt and uses the LLM to answer the question.\n",
    "    # It also takes the re-ranked results and attaches them to the response in case we want to see the nodes.\n",
    "    @step\n",
    "    async def answer(self, ctx: Context, ev: RagPrompt) -> StopEvent:\n",
    "        prompt = ev.prompt\n",
    "        ranked_results = await ctx.get('reranked_results')\n",
    "        messages = [\n",
    "            ChatMessage.from_str(prompt)\n",
    "        ]\n",
    "        answer = await self.llm.achat(messages)\n",
    "        result = {\n",
    "            \"response\": answer,\n",
    "            \"source_nodes\": ranked_results\n",
    "        }\n",
    "        return StopEvent(result=result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag = RAGWithReRank()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = await rag.run(query=\"How many llama3 models are there?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.utils.workflow import draw_all_possible_flows\n",
    "from IPython.core.display import HTML, display\n",
    "draw_all_possible_flows(RAGWithReRank, filename=\"flow.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise: Chat with PDF (30 minutes)\n",
    "\n",
    "Your goal in this exercise is to create a Gradio interface for a question-answering system.\n",
    "Your application should:\n",
    "- Use the query engine created above to answer questions about the uploaded PDF\n",
    "- Display the question and answer in the UI. If using QueryEngine, use a question and answer format. If using ChatEngine, use a chat format.\n",
    "\n",
    "If you need a challenge:\n",
    "- Use the `gr.File` component to allow the user to upload ANY pdf and ask question about it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "from tempfile import TemporaryDirectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the ingest_documents function:\n",
    "def ingest_documents(file_obj):\n",
    "    \"\"\"\n",
    "    Process an uploaded PDF file and create a query engine for it.\n",
    "    \n",
    "    Args:\n",
    "        file_obj: Gradio file upload object containing the PDF\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (chat_engine, cleared_file_upload, filename)\n",
    "    \"\"\"\n",
    "    if file_obj is None:\n",
    "        return None, None, \"\"\n",
    "        \n",
    "    with TemporaryDirectory() as temp_dir:\n",
    "        file_path = os.path.join(temp_dir, 'tmp.pdf')\n",
    "        # Get the file bytes from the Gradio upload object\n",
    "        file_bytes = open(file_obj, \"rb\").read()\n",
    "        \n",
    "        with open(file_path, \"wb\") as f:\n",
    "            f.write(file_bytes)\n",
    "            \n",
    "        documents = SimpleDirectoryReader(temp_dir).load_data()\n",
    "        llm = OpenAI(model=\"gpt-4o-mini\")\n",
    "        index = VectorStoreIndex.from_documents(documents, embed_model=embed_model)\n",
    "        chat_engine = index.as_chat_engine(llm=llm, node_postprocessors=[\n",
    "            SentenceTransformerRerank(top_n=5)\n",
    "        ], retriever_top_k=20)\n",
    "        \n",
    "    return chat_engine, None, Path(file_obj.name).name, []\n",
    "\n",
    "def chat(message, history, chat_engine):\n",
    "    history.append(\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": message\n",
    "        }\n",
    "    )\n",
    "    response = chat_engine.chat(message)\n",
    "    history.append({\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": response.response\n",
    "    })\n",
    "    return history, None\n",
    "\n",
    "# In the Blocks interface:\n",
    "with gr.Blocks() as demo:\n",
    "    chat_engine_state = gr.State()  # Renamed for clarity\n",
    "    gr.Markdown(\"## RAG Demo: Question answering with a PDF\")\n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=1):   \n",
    "            file_upload = gr.File(label=\"Upload a PDF file\", file_types=[\".pdf\"], file_count=\"single\")\n",
    "            submit_button = gr.Button(\"Submit\")\n",
    "            pdf_name = gr.Textbox(label=\"You are asking about...\")\n",
    "        with gr.Column(scale=5):\n",
    "            chatbot = gr.Chatbot(label=\"Chat with the uploaded PDF\", type='messages')\n",
    "            input = gr.Textbox(label=\"Enter a question\", interactive=True)\n",
    "\n",
    "    # Update to only store the chat_engine in the State\n",
    "    submit_button.click(\n",
    "        fn=ingest_documents, \n",
    "        inputs=file_upload, \n",
    "        outputs=[chat_engine_state, file_upload, pdf_name, chatbot]\n",
    "    )\n",
    "    input.submit(\n",
    "        fn=chat, \n",
    "        inputs=[input, chatbot, chat_engine_state], \n",
    "        outputs=[chatbot, input]\n",
    "    )\n",
    "\n",
    "demo.launch(debug=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function calling\n",
    "\n",
    "LLMs are fine tuned to call functions with parameters based on the prompt.\n",
    "This means you can provide information about one ore more functions you want to call and have the LLM decide which one to call.\n",
    "This is powerful tool when working with LLMs because it can provide a lot of external data and skills not available to the LLM itself.\n",
    "\n",
    "Let's go through a simple function calling example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.tools import FunctionTool\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core.llms import ChatMessage\n",
    "import numpy as np\n",
    "from rich import print\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below, we define a function that rolls a number of dice and returns the results.\n",
    "LlamaIndex provides a `FunctionTool` class that makes it easy to convert a function into a tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "def roll_dice(num_dice:int) -> str:\n",
    "    \"\"\"\n",
    "    Rolls a number of dice and returns the results.\n",
    "\n",
    "    Args:\n",
    "        num_dice: The number of dice to roll\n",
    "\n",
    "    Returns:\n",
    "        str: The results of the dice roll (comma separated if multiple dice are rolled)\n",
    "    \"\"\"\n",
    "    return ', '.join(str(np.random.randint(1, 6)) for _ in range(num_dice))\n",
    "\n",
    "roll_dice_tool = FunctionTool.from_defaults(fn=roll_dice, description=\"Useful when you need to roll dice.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's ask our LLM to roll 5 dice and see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(model=\"gpt-4o-mini\")\n",
    "messages = [\n",
    "    ChatMessage.from_str(\"Roll 5 dice\")\n",
    "]\n",
    "response = llm.chat_with_tools(tools=[roll_dice_tool], messages=messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It doesn't actually call the function yet - it just says which function to call and which arguments to pass to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ChatResponse</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">message</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ChatMessage</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">role</span>=<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">MessageRole.ASSISTANT:</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #008000; text-decoration-color: #008000\">'assistant'</span><span style=\"font-weight: bold\">&gt;</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">content</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">additional_kwargs</span>=<span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'tool_calls'</span>: <span style=\"font-weight: bold\">[</span>\n",
       "                <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ChatCompletionMessageToolCall</span><span style=\"font-weight: bold\">(</span>\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'call_m0BGmqoqLx9zrPXTzSPYD5oX'</span>,\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">function</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Function</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">arguments</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'{\"num_dice\":5}'</span>, <span style=\"color: #808000; text-decoration-color: #808000\">name</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'roll_dice'</span><span style=\"font-weight: bold\">)</span>,\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">type</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'function'</span>\n",
       "                <span style=\"font-weight: bold\">)</span>\n",
       "            <span style=\"font-weight: bold\">]</span>\n",
       "        <span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">raw</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ChatCompletion</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'chatcmpl-AaWgtiml9ctk3mu6dN5vdafVz2EeS'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">choices</span>=<span style=\"font-weight: bold\">[</span>\n",
       "            <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Choice</span><span style=\"font-weight: bold\">(</span>\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">finish_reason</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'tool_calls'</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">index</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">logprobs</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">message</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ChatCompletionMessage</span><span style=\"font-weight: bold\">(</span>\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">content</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">refusal</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">role</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'assistant'</span>,\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">audio</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">function_call</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">tool_calls</span>=<span style=\"font-weight: bold\">[</span>\n",
       "                        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ChatCompletionMessageToolCall</span><span style=\"font-weight: bold\">(</span>\n",
       "                            <span style=\"color: #808000; text-decoration-color: #808000\">id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'call_m0BGmqoqLx9zrPXTzSPYD5oX'</span>,\n",
       "                            <span style=\"color: #808000; text-decoration-color: #808000\">function</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Function</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">arguments</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'{\"num_dice\":5}'</span>, <span style=\"color: #808000; text-decoration-color: #808000\">name</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'roll_dice'</span><span style=\"font-weight: bold\">)</span>,\n",
       "                            <span style=\"color: #808000; text-decoration-color: #808000\">type</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'function'</span>\n",
       "                        <span style=\"font-weight: bold\">)</span>\n",
       "                    <span style=\"font-weight: bold\">]</span>\n",
       "                <span style=\"font-weight: bold\">)</span>\n",
       "            <span style=\"font-weight: bold\">)</span>\n",
       "        <span style=\"font-weight: bold\">]</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">created</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1733268835</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">model</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'gpt-4o-mini-2024-07-18'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">object</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'chat.completion'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">service_tier</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">system_fingerprint</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'fp_0705bf87c0'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">usage</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">CompletionUsage</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">completion_tokens</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">17</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">prompt_tokens</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">56</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">total_tokens</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">73</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">completion_tokens_details</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">CompletionTokensDetails</span><span style=\"font-weight: bold\">(</span>\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">accepted_prediction_tokens</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">audio_tokens</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">reasoning_tokens</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">rejected_prediction_tokens</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>\n",
       "            <span style=\"font-weight: bold\">)</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">prompt_tokens_details</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">PromptTokensDetails</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">audio_tokens</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #808000; text-decoration-color: #808000\">cached_tokens</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">)</span>\n",
       "        <span style=\"font-weight: bold\">)</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">delta</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">logprobs</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">additional_kwargs</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'prompt_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">56</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'completion_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">17</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'total_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">73</span><span style=\"font-weight: bold\">}</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mChatResponse\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33mmessage\u001b[0m=\u001b[1;35mChatMessage\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mrole\u001b[0m=\u001b[1m<\u001b[0m\u001b[1;95mMessageRole.ASSISTANT:\u001b[0m\u001b[39m \u001b[0m\u001b[32m'assistant'\u001b[0m\u001b[1m>\u001b[0m,\n",
       "        \u001b[33mcontent\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "        \u001b[33madditional_kwargs\u001b[0m=\u001b[1m{\u001b[0m\n",
       "            \u001b[32m'tool_calls'\u001b[0m: \u001b[1m[\u001b[0m\n",
       "                \u001b[1;35mChatCompletionMessageToolCall\u001b[0m\u001b[1m(\u001b[0m\n",
       "                    \u001b[33mid\u001b[0m=\u001b[32m'call_m0BGmqoqLx9zrPXTzSPYD5oX'\u001b[0m,\n",
       "                    \u001b[33mfunction\u001b[0m=\u001b[1;35mFunction\u001b[0m\u001b[1m(\u001b[0m\u001b[33marguments\u001b[0m=\u001b[32m'\u001b[0m\u001b[32m{\u001b[0m\u001b[32m\"num_dice\":5\u001b[0m\u001b[32m}\u001b[0m\u001b[32m'\u001b[0m, \u001b[33mname\u001b[0m=\u001b[32m'roll_dice'\u001b[0m\u001b[1m)\u001b[0m,\n",
       "                    \u001b[33mtype\u001b[0m=\u001b[32m'function'\u001b[0m\n",
       "                \u001b[1m)\u001b[0m\n",
       "            \u001b[1m]\u001b[0m\n",
       "        \u001b[1m}\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[33mraw\u001b[0m=\u001b[1;35mChatCompletion\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mid\u001b[0m=\u001b[32m'chatcmpl-AaWgtiml9ctk3mu6dN5vdafVz2EeS'\u001b[0m,\n",
       "        \u001b[33mchoices\u001b[0m=\u001b[1m[\u001b[0m\n",
       "            \u001b[1;35mChoice\u001b[0m\u001b[1m(\u001b[0m\n",
       "                \u001b[33mfinish_reason\u001b[0m=\u001b[32m'tool_calls'\u001b[0m,\n",
       "                \u001b[33mindex\u001b[0m=\u001b[1;36m0\u001b[0m,\n",
       "                \u001b[33mlogprobs\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "                \u001b[33mmessage\u001b[0m=\u001b[1;35mChatCompletionMessage\u001b[0m\u001b[1m(\u001b[0m\n",
       "                    \u001b[33mcontent\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "                    \u001b[33mrefusal\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "                    \u001b[33mrole\u001b[0m=\u001b[32m'assistant'\u001b[0m,\n",
       "                    \u001b[33maudio\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "                    \u001b[33mfunction_call\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "                    \u001b[33mtool_calls\u001b[0m=\u001b[1m[\u001b[0m\n",
       "                        \u001b[1;35mChatCompletionMessageToolCall\u001b[0m\u001b[1m(\u001b[0m\n",
       "                            \u001b[33mid\u001b[0m=\u001b[32m'call_m0BGmqoqLx9zrPXTzSPYD5oX'\u001b[0m,\n",
       "                            \u001b[33mfunction\u001b[0m=\u001b[1;35mFunction\u001b[0m\u001b[1m(\u001b[0m\u001b[33marguments\u001b[0m=\u001b[32m'\u001b[0m\u001b[32m{\u001b[0m\u001b[32m\"num_dice\":5\u001b[0m\u001b[32m}\u001b[0m\u001b[32m'\u001b[0m, \u001b[33mname\u001b[0m=\u001b[32m'roll_dice'\u001b[0m\u001b[1m)\u001b[0m,\n",
       "                            \u001b[33mtype\u001b[0m=\u001b[32m'function'\u001b[0m\n",
       "                        \u001b[1m)\u001b[0m\n",
       "                    \u001b[1m]\u001b[0m\n",
       "                \u001b[1m)\u001b[0m\n",
       "            \u001b[1m)\u001b[0m\n",
       "        \u001b[1m]\u001b[0m,\n",
       "        \u001b[33mcreated\u001b[0m=\u001b[1;36m1733268835\u001b[0m,\n",
       "        \u001b[33mmodel\u001b[0m=\u001b[32m'gpt-4o-mini-2024-07-18'\u001b[0m,\n",
       "        \u001b[33mobject\u001b[0m=\u001b[32m'chat.completion'\u001b[0m,\n",
       "        \u001b[33mservice_tier\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "        \u001b[33msystem_fingerprint\u001b[0m=\u001b[32m'fp_0705bf87c0'\u001b[0m,\n",
       "        \u001b[33musage\u001b[0m=\u001b[1;35mCompletionUsage\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mcompletion_tokens\u001b[0m=\u001b[1;36m17\u001b[0m,\n",
       "            \u001b[33mprompt_tokens\u001b[0m=\u001b[1;36m56\u001b[0m,\n",
       "            \u001b[33mtotal_tokens\u001b[0m=\u001b[1;36m73\u001b[0m,\n",
       "            \u001b[33mcompletion_tokens_details\u001b[0m=\u001b[1;35mCompletionTokensDetails\u001b[0m\u001b[1m(\u001b[0m\n",
       "                \u001b[33maccepted_prediction_tokens\u001b[0m=\u001b[1;36m0\u001b[0m,\n",
       "                \u001b[33maudio_tokens\u001b[0m=\u001b[1;36m0\u001b[0m,\n",
       "                \u001b[33mreasoning_tokens\u001b[0m=\u001b[1;36m0\u001b[0m,\n",
       "                \u001b[33mrejected_prediction_tokens\u001b[0m=\u001b[1;36m0\u001b[0m\n",
       "            \u001b[1m)\u001b[0m,\n",
       "            \u001b[33mprompt_tokens_details\u001b[0m=\u001b[1;35mPromptTokensDetails\u001b[0m\u001b[1m(\u001b[0m\u001b[33maudio_tokens\u001b[0m=\u001b[1;36m0\u001b[0m, \u001b[33mcached_tokens\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m)\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[33mdelta\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "    \u001b[33mlogprobs\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "    \u001b[33madditional_kwargs\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'prompt_tokens'\u001b[0m: \u001b[1;36m56\u001b[0m, \u001b[32m'completion_tokens'\u001b[0m: \u001b[1;36m17\u001b[0m, \u001b[32m'total_tokens'\u001b[0m: \u001b[1;36m73\u001b[0m\u001b[1m}\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's parse the response to get the function call and arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Function</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">arguments</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'{\"num_dice\":5}'</span>, <span style=\"color: #808000; text-decoration-color: #808000\">name</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'roll_dice'</span><span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mFunction\u001b[0m\u001b[1m(\u001b[0m\u001b[33marguments\u001b[0m=\u001b[32m'\u001b[0m\u001b[32m{\u001b[0m\u001b[32m\"num_dice\":5\u001b[0m\u001b[32m}\u001b[0m\u001b[32m'\u001b[0m, \u001b[33mname\u001b[0m=\u001b[32m'roll_dice'\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function_call = response.raw.choices[0].message.tool_calls[0].function\n",
    "print(function_call)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's call the function with the arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ToolOutput</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'2, 4, 3, 3, 5'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">tool_name</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'roll_dice'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">raw_input</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'args'</span>: <span style=\"font-weight: bold\">()</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'kwargs'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'num_dice'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span><span style=\"font-weight: bold\">}}</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">raw_output</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'2, 4, 3, 3, 5'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">is_error</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mToolOutput\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33mcontent\u001b[0m=\u001b[32m'2, 4, 3, 3, 5'\u001b[0m,\n",
       "    \u001b[33mtool_name\u001b[0m=\u001b[32m'roll_dice'\u001b[0m,\n",
       "    \u001b[33mraw_input\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'args'\u001b[0m: \u001b[1m(\u001b[0m\u001b[1m)\u001b[0m, \u001b[32m'kwargs'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'num_dice'\u001b[0m: \u001b[1;36m5\u001b[0m\u001b[1m}\u001b[0m\u001b[1m}\u001b[0m,\n",
       "    \u001b[33mraw_output\u001b[0m=\u001b[32m'2, 4, 3, 3, 5'\u001b[0m,\n",
       "    \u001b[33mis_error\u001b[0m=\u001b[3;91mFalse\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tool_output = roll_dice_tool.call(**json.loads(function_call.arguments))\n",
    "print(tool_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now we have a basic idea of how function calling works under the hood.\n",
    "Agents use function calling to call tools, but generally use them in loops combined with reasoning steps to accomplish complex tasks.\n",
    "Next, we'll see how to use LlamaIndex's implementation of agents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Llama index agents\n",
    "\n",
    "We've seen how to use function calling with an LLM, but it was kind of difficult to go from the LLM response to the function call.\n",
    "We didn't even close the loop and pass the tool call result back to the LLM.\n",
    "LlamaIndex's `FunctionCallingAgent` and `ReActAgent` classes make this easier.\n",
    "Let's go through an example of using the `FunctionCallingAgent` to perform a more complex tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.agent import ReActAgent, FunctionCallingAgent\n",
    "from llama_index.core.tools import FunctionTool\n",
    "from llama_index.tools.yahoo_finance import YahooFinanceToolSpec\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from yfinance import download as yf_download\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's really useful to be able to define your own tools.\n",
    "Luckliy, LlamaIndex makes this easy with the `FunctionTool` class.\n",
    "Here, we'll define a tool that gets stock data from Yahoo Finance and returns it as a markdown table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stock_data(ticker:str, start_date:str, end_date:str) -> str:\n",
    "    \"\"\"\n",
    "    Gets stock data using yfinance. All dates should be in YYYY-MM-DD format.\n",
    "\n",
    "    Args:\n",
    "        ticker: The ticker symbol of the stock to get data for\n",
    "        start_date: The start date of the data to get\n",
    "        end_date: The end date of the data to get\n",
    "\n",
    "    Returns:\n",
    "        str: A markdown table of the stock data\n",
    "    \"\"\"\n",
    "    df = yf_download(ticker, start=start_date, end=end_date)\n",
    "    return df.to_markdown()\n",
    "\n",
    "get_stock_data_tool = FunctionTool.from_defaults(fn=get_stock_data, description=\"Useful when you want to pull trended data about a stock.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can pass this tool to an agent to have it answer questions about stocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Running step f4e4fdd4-dbc7-4f32-a09a-72dcd1065789. Step input: What was TSLA's high and low over the last 7 days?\n",
      "Added user message to memory: What was TSLA's high and low over the last 7 days?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Calling Function ===\n",
      "Calling function: get_stock_data with args: {\"ticker\": \"TSLA\", \"start_date\": \"2023-10-16\", \"end_date\": \"2023-10-23\"}\n",
      "=== Function Output ===\n",
      "| Date                |   ('Adj Close', 'TSLA') |   ('Close', 'TSLA') |   ('High', 'TSLA') |   ('Low', 'TSLA') |   ('Open', 'TSLA') |   ('Volume', 'TSLA') |\n",
      "|:--------------------|------------------------:|--------------------:|-------------------:|------------------:|-------------------:|---------------------:|\n",
      "| 2023-10-16 00:00:00 |                  253.92 |              253.92 |             255.4  |            248.48 |             250.05 |          8.89172e+07 |\n",
      "| 2023-10-17 00:00:00 |                  254.85 |              254.85 |             257.18 |            247.08 |             250.1  |          9.35629e+07 |\n",
      "| 2023-10-18 00:00:00 |                  242.68 |              242.68 |             254.63 |            242.08 |             252.7  |          1.25148e+08 |\n",
      "| 2023-10-19 00:00:00 |                  220.11 |              220.11 |             230.61 |            216.78 |             225.95 |          1.70773e+08 |\n",
      "| 2023-10-20 00:00:00 |                  211.99 |              211.99 |             218.86 |            210.42 |             217.01 |          1.37734e+08 |\n",
      "> Running step d1ea8661-606e-4050-98b3-485d11245640. Step input: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== LLM Response ===\n",
      "Over the last 7 days, TSLA's stock data shows the following high and low prices:\n",
      "\n",
      "- **High**: $257.18 (on October 17, 2023)\n",
      "- **Low**: $210.42 (on October 20, 2023)\n"
     ]
    }
   ],
   "source": [
    "agent = FunctionCallingAgent.from_tools(tools=[get_stock_data_tool], llm=OpenAI(model=\"gpt-4o-mini\"), verbose=True)\n",
    "response = agent.chat(f\"What was TSLA's high and low over the last 7 days?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for understanding\n",
    "\n",
    "Is there anything strange about the response to this question above?\n",
    "What do you think we could do to fix it?\n",
    "Let's define another tool that addresses this problem.\n",
    "Does this help? Is there a different way to solve the problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using pre-packaged tools\n",
    "\n",
    "LlamaIndex has a number of tools that are pre-defined and can be installed.\n",
    "In this case, we will use the `YahooFinanceToolSpec` to get additional information about a stock beyond just the trended data.\n",
    "We will also add our `get_stock_data_tool` to the list of tools, since getting trended data is not a part of the `YahooFinanceToolSpec`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "yfinance_tools = YahooFinanceToolSpec().to_tool_list()\n",
    "yfinance_tools.append(get_stock_data_tool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Running step bbf82ada-f013-48d9-8476-99c32b16c17f. Step input: What was TSLA's high and low over the last 7 days? Today's date is 2024-12-03.\n",
      "Added user message to memory: What was TSLA's high and low over the last 7 days? Today's date is 2024-12-03.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Calling Function ===\n",
      "Calling function: get_stock_data with args: {\"ticker\": \"TSLA\", \"start_date\": \"2024-11-26\", \"end_date\": \"2024-12-03\"}\n",
      "=== Function Output ===\n",
      "| Date                |   ('Adj Close', 'TSLA') |   ('Close', 'TSLA') |   ('High', 'TSLA') |   ('Low', 'TSLA') |   ('Open', 'TSLA') |   ('Volume', 'TSLA') |\n",
      "|:--------------------|------------------------:|--------------------:|-------------------:|------------------:|-------------------:|---------------------:|\n",
      "| 2024-11-26 00:00:00 |                  338.23 |              338.23 |             346.96 |            335.66 |             341    |          6.22959e+07 |\n",
      "| 2024-11-27 00:00:00 |                  332.89 |              332.89 |             342.55 |            326.59 |             341.8  |          5.78964e+07 |\n",
      "| 2024-11-29 00:00:00 |                  345.16 |              345.16 |             345.45 |            334.65 |             336.08 |          3.71676e+07 |\n",
      "| 2024-12-02 00:00:00 |                  357.09 |              357.09 |             360    |            351.15 |             352.38 |          7.79865e+07 |\n",
      "> Running step 9d5543d4-1c9e-4f6c-9d66-bc5ef204af28. Step input: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== LLM Response ===\n",
      "Over the last 7 days, Tesla (TSLA) had the following high and low prices:\n",
      "\n",
      "- **High:** $360.00 (on December 2, 2024)\n",
      "- **Low:** $326.59 (on November 27, 2024)\n"
     ]
    }
   ],
   "source": [
    "agent = FunctionCallingAgent.from_tools(tools=yfinance_tools, llm=OpenAI(model=\"gpt-4o-mini\"), verbose=True)\n",
    "response = agent.chat(f\"What was TSLA's high and low over the last 7 days? Today's date is {datetime.now().strftime('%Y-%m-%d')}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Running step d140eeff-7170-4b3c-b7b2-df47504a706a. Step input: What does TSLA make?\n",
      "Added user message to memory: What does TSLA make?\n",
      "=== Calling Function ===\n",
      "Calling function: stock_basic_info with args: {\"ticker\": \"TSLA\"}\n",
      "=== Function Output ===\n",
      "Info: \n",
      "{'address1': '1 Tesla Road', 'city': 'Austin', 'state': 'TX', 'zip': '78725', 'country': 'United States', 'phone': '512 516 8177', 'website': 'https://www.tesla.com', 'industry': 'Auto Manufacturers', 'industryKey': 'auto-manufacturers', 'industryDisp': 'Auto Manufacturers', 'sector': 'Consumer Cyclical', 'sectorKey': 'consumer-cyclical', 'sectorDisp': 'Consumer Cyclical', 'longBusinessSummary': 'Tesla, Inc. designs, develops, manufactures, leases, and sells electric vehicles, and energy generation and storage systems in the United States, China, and internationally. The company operates in two segments, Automotive, and Energy Generation and Storage. The Automotive segment offers electric vehicles, as well as sells automotive regulatory credits; and non-warranty after-sales vehicle, used vehicles, body shop and parts, supercharging, retail merchandise, and vehicle insurance services. This segment also provides sedans and sport utility vehicles through direct and used vehicle sales, a network of Tesla Superchargers, and in-app upgrades; purchase financing and leasing services; services for electric vehicles through its company-owned service locations and Tesla mobile service technicians; and vehicle limited warranties and extended service plans. The Energy Generation and Storage segment engages in the design, manufacture, installation, sale, and leasing of solar energy generation and energy storage products, and related services to residential, commercial, and industrial customers and utilities through its website, stores, and galleries, as well as through a network of channel partners; and provision of service and repairs to its energy product customers, including under warranty, as well as various financing options to its solar customers. The company was formerly known as Tesla Motors, Inc. and changed its name to Tesla, Inc. in February 2017. Tesla, Inc. was incorporated in 2003 and is headquartered in Austin, Texas.', 'fullTimeEmployees': 140473, 'companyOfficers': [{'maxAge': 1, 'name': 'Mr. Elon R. Musk', 'age': 51, 'title': 'Co-Founder, Technoking of Tesla, CEO & Director', 'yearBorn': 1972, 'fiscalYear': 2023, 'exercisedValue': 0, 'unexercisedValue': 0}, {'maxAge': 1, 'name': 'Mr. Vaibhav  Taneja', 'age': 45, 'title': 'Chief Financial Officer', 'yearBorn': 1978, 'fiscalYear': 2023, 'totalPay': 278000, 'exercisedValue': 8517957, 'unexercisedValue': 202075632}, {'maxAge': 1, 'name': 'Mr. Xiaotong  Zhu', 'age': 43, 'title': 'Senior Vice President of Automotive', 'yearBorn': 1980, 'fiscalYear': 2023, 'totalPay': 926877, 'exercisedValue': 0, 'unexercisedValue': 344144320}, {'maxAge': 1, 'name': 'Travis  Axelrod', 'title': 'Head of Investor Relations', 'fiscalYear': 2023, 'exercisedValue': 0, 'unexercisedValue': 0}, {'maxAge': 1, 'name': 'Brian  Scelfo', 'title': 'Senior Director of Corporate Development', 'fiscalYear': 2023, 'exercisedValue': 0, 'unexercisedValue': 0}, {'maxAge': 1, 'name': 'Mr. Franz  von Holzhausen', 'title': 'Chief Designer', 'fiscalYear': 2023, 'exercisedValue': 0, 'unexercisedValue': 0}, {'maxAge': 1, 'name': 'Mr. John  Walker', 'age': 60, 'title': 'Vice President of Sales - North America', 'yearBorn': 1963, 'fiscalYear': 2023, 'totalPay': 121550, 'exercisedValue': 0, 'unexercisedValue': 0}, {'maxAge': 1, 'name': 'Mr. Peter  Bannon', 'title': 'Chip Architect', 'fiscalYear': 2023, 'exercisedValue': 0, 'unexercisedValue': 0}, {'maxAge': 1, 'name': 'Mr. Turner  Caldwell', 'title': 'Engineering Manager', 'fiscalYear': 2023, 'exercisedValue': 0, 'unexercisedValue': 0}, {'maxAge': 1, 'name': 'Mr. Rodney D. Westmoreland Jr.', 'title': 'Director of Construction Management', 'fiscalYear': 2023, 'exercisedValue': 0, 'unexercisedValue': 0}], 'auditRisk': 7, 'boardRisk': 9, 'compensationRisk': 10, 'shareHolderRightsRisk': 9, 'overallRisk': 10, 'governanceEpochDate': 1733011200, 'compensationAsOfEpochDate': 1703980800, 'maxAge': 86400, 'priceHint': 2, 'previousClose': 357.09, 'open': 351.575, 'dayLow': 348.2, 'dayHigh': 355.68, 'regularMarketPreviousClose': 357.09, 'regularMarketOpen': 351.575, 'regularMarketDayLow': 348.2, 'regularMarketDayHigh': 355.68, 'beta': 2.295, 'trailingPE': 96.54396, 'forwardPE': 106.73641, 'volume': 56540126, 'regularMarketVolume': 56540126, 'averageVolume': 89123946, 'averageVolume10days': 75967170, 'averageDailyVolume10Day': 75967170, 'bid': 351.37, 'ask': 351.92, 'bidSize': 100, 'askSize': 200, 'marketCap': 1128079294464, 'fiftyTwoWeekLow': 138.8, 'fiftyTwoWeekHigh': 361.93, 'priceToSalesTrailing12Months': 11.611727, 'fiftyDayAverage': 275.0206, 'twoHundredDayAverage': 215.6159, 'currency': 'USD', 'enterpriseValue': 1126194216960, 'profitMargins': 0.13075, 'floatShares': 2793105010, 'sharesOutstanding': 3210060032, 'sharesShort': 76444266, 'sharesShortPriorMonth': 79364249, 'sharesShortPreviousMonthDate': 1728950400, 'dateShortInterest': 1731628800, 'sharesPercentSharesOut': 0.0238, 'heldPercentInsiders': 0.12911999, 'heldPercentInstitutions': 0.47877997, 'shortRatio': 0.74, 'shortPercentOfFloat': 0.0273, 'impliedSharesOutstanding': 3210060032, 'bookValue': 21.806, 'priceToBook': 16.11575, 'lastFiscalYearEnd': 1703980800, 'nextFiscalYearEnd': 1735603200, 'mostRecentQuarter': 1727654400, 'earningsQuarterlyGrowth': 0.169, 'netIncomeToCommon': 12743000064, 'trailingEps': 3.64, 'forwardEps': 3.24, 'lastSplitFactor': '3:1', 'lastSplitDate': 1661385600, 'enterpriseToRevenue': 11.592, 'enterpriseToEbitda': 85.034, '52WeekChange': 0.49585283, 'SandP52WeekChange': 0.3240446, 'exchange': 'NMS', 'quoteType': 'EQUITY', 'symbol': 'TSLA', 'underlyingSymbol': 'TSLA', 'shortName': 'Tesla, Inc.', 'longName': 'Tesla, Inc.', 'firstTradeDateEpochUtc': 1277818200, 'timeZoneFullName': 'America/New_York', 'timeZoneShortName': 'EST', 'uuid': 'ec367bc4-f92c-397c-ac81-bf7b43cffaf7', 'messageBoardId': 'finmb_27444752', 'gmtOffSetMilliseconds': -18000000, 'currentPrice': 351.42, 'targetHighPrice': 411.0, 'targetLowPrice': 116.0, 'targetMeanPrice': 243.13881, 'targetMedianPrice': 239.0, 'recommendationMean': 2.77083, 'recommendationKey': 'hold', 'numberOfAnalystOpinions': 42, 'totalCash': 33648001024, 'totalCashPerShare': 10.482, 'ebitda': 13244000256, 'totalDebt': 12782999552, 'quickRatio': 1.214, 'currentRatio': 1.844, 'totalRevenue': 97150001152, 'debtToEquity': 18.078, 'revenuePerShare': 30.457, 'returnOnAssets': 0.04759, 'returnOnEquity': 0.20389, 'freeCashflow': 676625024, 'operatingCashflow': 14478999552, 'earningsGrowth': 0.17, 'revenueGrowth': 0.078, 'grossMargins': 0.18229, 'ebitdaMargins': 0.13633001, 'operatingMargins': 0.107889995, 'financialCurrency': 'USD', 'trailingPegRatio': 9.9006}\n",
      "> Running step f5d5fcfe-abad-4811-96b6-cd5c7afcc44a. Step input: None\n",
      "=== LLM Response ===\n",
      "Tesla, Inc. designs, develops, manufactures, leases, and sells electric vehicles and energy generation and storage systems. The company operates in two main segments:\n",
      "\n",
      "1. **Automotive Segment**: \n",
      "   - Offers electric vehicles (EVs) including sedans and sport utility vehicles (SUVs).\n",
      "   - Sells automotive regulatory credits and non-warranty after-sales vehicle services.\n",
      "   - Provides used vehicles, body shop and parts services, supercharging, retail merchandise, and vehicle insurance services.\n",
      "   - Operates a network of Tesla Superchargers and offers in-app upgrades, purchase financing, and leasing services.\n",
      "\n",
      "2. **Energy Generation and Storage Segment**: \n",
      "   - Engages in the design, manufacture, installation, sale, and leasing of solar energy generation and energy storage products.\n",
      "   - Provides related services to residential, commercial, and industrial customers, as well as utilities.\n",
      "   - Offers service and repairs for energy products, including warranty services and financing options for solar customers.\n",
      "\n",
      "Tesla is headquartered in Austin, Texas, and has a significant presence in the United States, China, and internationally. The company was formerly known as Tesla Motors, Inc. and changed its name to Tesla, Inc. in February 2017.\n"
     ]
    }
   ],
   "source": [
    "response = agent.chat(\"What does TSLA make?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autogen\n",
    "\n",
    "Autogen is a library for building agents that can interact with each other.\n",
    "It's a little more low-level than LlamaIndex's agents, but is very flexible and a great choice for more complex workflows.\n",
    "One feature is that each type of interaction is mediated by a specific agent.\n",
    "Autogen enables interactions between different types of agents in order to orchestrate complex workflows.\n",
    "We'll walk through several examples of how to use Autogen to build agents, but for more details go to the [documentation](https://microsoft.github.io/autogen)\n",
    "\n",
    "First, let's create a simple agent that can converse with a user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen import ConversableAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_config = {\n",
    "    \"config_list\": [\n",
    "        {\n",
    "            \"model\": \"gpt-4o-mini\",\n",
    "            \"api_key\": os.environ[\"OPENAI_API_KEY\"]\n",
    "        }\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Why do Python programmers prefer dark mode?\n",
       "\n",
       "Because light attracts bugs!\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Why do Python programmers prefer dark mode?\n",
       "\n",
       "Because light attracts bugs!\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "agent = ConversableAgent(name=\"My Agent\", llm_config=llm_config)\n",
    "print(agent.generate_reply(messages=[{\"role\": \"user\", \"content\": \"Tell me a joke about Python\"}]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, this wasn't much more than an LLM call.\n",
    "Let's take it a step further and create two agents that can interact with each other.\n",
    "Here is an example straight from the Autogen documentation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mcathy\u001b[0m (to joe):\n",
      "\n",
      "Tell me a joke about Python\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mjoe\u001b[0m (to cathy):\n",
      "\n",
      "Why do Python programmers prefer dark mode?\n",
      "\n",
      "Because light attracts bugs!\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mcathy\u001b[0m (to joe):\n",
      "\n",
      "That's a good one, Joe! How about this: \n",
      "\n",
      "Why did the Python programmer break up with their partner?\n",
      "\n",
      "Because they had too many \"issues\" and couldn't handle their \"exceptions\"!\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mjoe\u001b[0m (to cathy):\n",
      "\n",
      "That's a great one, Cathy! Here's another for you:\n",
      "\n",
      "Why do Python programmers love nature?\n",
      "\n",
      "Because they enjoy working with lists and trees!\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "cathy = ConversableAgent(\n",
    "    \"cathy\",\n",
    "    system_message=\"Your name is Cathy and you are a part of a duo of comedians.\",\n",
    "    llm_config={\"config_list\": [{\"model\": \"gpt-4o-mini\", \"temperature\": 0.9, \"api_key\": os.environ.get(\"OPENAI_API_KEY\")}]},\n",
    "    human_input_mode=\"NEVER\",  # Never ask for human input.\n",
    ")\n",
    "\n",
    "joe = ConversableAgent(\n",
    "    \"joe\",\n",
    "    system_message=\"Your name is Joe and you are a part of a duo of comedians.\",\n",
    "    llm_config={\"config_list\": [{\"model\": \"gpt-4o-mini\", \"temperature\": 0.7, \"api_key\": os.environ.get(\"OPENAI_API_KEY\")}]},\n",
    "    human_input_mode=\"NEVER\",  # Never ask for human input.\n",
    ")\n",
    "\n",
    "response = cathy.initiate_chat(joe, message=\"Tell me a joke about Python\", max_turns=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another nice thing about Autogen is its ability to execute code.\n",
    "In the cell below, we define a code executor that will execute code on the local machine.\n",
    "We give it a rather long system message to help it understand what to do.\n",
    "We will also have a multi-agent interaction here, but one agent is a code writer and the other is a code executor.\n",
    "The code writer agent will write code and the code executor agent will execute it.\n",
    "Let's see how this works in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mcode_executor_agent\u001b[0m (to code_writer):\n",
      "\n",
      "What are the files in the current directory?\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mcode_writer\u001b[0m (to code_executor_agent):\n",
      "\n",
      "# filename: list_files.py\n",
      "```python\n",
      "import os\n",
      "\n",
      "# Get the list of files in the current directory\n",
      "files = os.listdir('.')\n",
      "\n",
      "# Print the list of files\n",
      "print(\"Files in the current directory:\")\n",
      "for file in files:\n",
      "    print(file)\n",
      "```\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> EXECUTING CODE BLOCK (inferred language is python)...\u001b[0m\n",
      "\u001b[33mcode_executor_agent\u001b[0m (to code_writer):\n",
      "\n",
      "exitcode: 0 (execution succeeded)\n",
      "Code output: Files in the current directory:\n",
      "bayesian_optimization.gif\n",
      ".DS_Store\n",
      "tmp_code_a9b2887cb4447ffc40d0267d5221988a.py\n",
      "01_getting_started_with_llms_20241123.ipynb\n",
      "02_rag_and_agents_solution.ipynb\n",
      "output\n",
      "flow.html\n",
      "01_getting_started_with_llms.ipynb\n",
      "01_getting_started_with_llms_solution.ipynb\n",
      "create_fastapi_app.py\n",
      "lib\n",
      "api\n",
      "_resources.ipynb\n",
      "data\n",
      ".cache\n",
      "mipro_optimized_few_shot.json\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mcode_writer\u001b[0m (to code_executor_agent):\n",
      "\n",
      "Now that we have the list of files in the current directory, please let me know what specific task you would like to perform with these files.\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from autogen.coding import LocalCommandLineCodeExecutor\n",
    "executor = LocalCommandLineCodeExecutor(\n",
    "    timeout=10\n",
    ")\n",
    "\n",
    "code_writer_system_message = \"\"\"\n",
    "You have been given coding capability to solve tasks using Python code.\n",
    "In the following cases, suggest python code (in a python coding block) or shell script (in a sh coding block) for the user to execute.\n",
    "    1. When you need to collect info, use the code to output the info you need, for example, browse or search the web, download/read a file, print the content of a webpage or a file, get the current date/time, check the operating system. After sufficient info is printed and the task is ready to be solved based on your language skill, you can solve the task by yourself.\n",
    "    2. When you need to perform some task with code, use the code to perform the task and output the result. Finish the task smartly.\n",
    "Solve the task step by step if you need to. If a plan is not provided, explain your plan first. Be clear which step uses code, and which step uses your language skill.\n",
    "When using code, you must indicate the script type in the code block. The user cannot provide any other feedback or perform any other action beyond executing the code you suggest. The user can't modify your code. So do not suggest incomplete code which requires users to modify. Don't use a code block if it's not intended to be executed by the user.\n",
    "If you want the user to save the code in a file before executing it, put # filename: <filename> inside the code block as the first line. Don't include multiple code blocks in one response. Do not ask users to copy and paste the result. Instead, use 'print' function for the output when relevant. Check the execution result returned by the user.\n",
    "\"\"\"\n",
    "\n",
    "code_writer_agent = ConversableAgent(\n",
    "    \"code_writer\",\n",
    "    system_message=code_writer_system_message,\n",
    "    llm_config={\"config_list\": [{\"model\": \"gpt-4o-mini\", \"api_key\": os.environ[\"OPENAI_API_KEY\"]}]},\n",
    "    code_execution_config=False,  # Turn off code execution for this agent.\n",
    "    max_consecutive_auto_reply=2,\n",
    "    human_input_mode=\"NEVER\",\n",
    ")\n",
    "\n",
    "code_executor_agent = ConversableAgent(\n",
    "    name=\"code_executor_agent\",\n",
    "    llm_config=False,\n",
    "    code_execution_config={\n",
    "        \"executor\": executor,\n",
    "    },\n",
    "    human_input_mode=\"NEVER\",\n",
    ")  \n",
    "\n",
    "response = code_executor_agent.initiate_chat(code_writer_agent, message=\"What are the files in the current directory?\", max_turns=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, just like LlamaIndex agents, you can use function calling with Autogen as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen import AssistantAgent, UserProxyAgent, register_function\n",
    "\n",
    "assistant = AssistantAgent(name=\"assistant\", llm_config=llm_config, human_input_mode=\"NEVER\")\n",
    "user_proxy = UserProxyAgent(name=\"user_proxy\", llm_config=False, human_input_mode=\"NEVER\")\n",
    "\n",
    "def roll_dice(num_dice:int) -> str:\n",
    "    \"\"\"\n",
    "    Rolls a number of dice and returns the results.\n",
    "    \"\"\"\n",
    "    return ', '.join(str(np.random.randint(1, 6)) for _ in range(num_dice))\n",
    "\n",
    "register_function(roll_dice, caller=assistant, executor=user_proxy, description=\"Useful when you need to roll dice.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33muser_proxy\u001b[0m (to assistant):\n",
      "\n",
      "Roll 3 dice\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33massistant\u001b[0m (to user_proxy):\n",
      "\n",
      "\u001b[32m***** Suggested tool call (call_FCLl3UffiTOv7FDbZHjCAd1V): roll_dice *****\u001b[0m\n",
      "Arguments: \n",
      "{\"num_dice\":3}\n",
      "\u001b[32m**************************************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[35m\n",
      ">>>>>>>> EXECUTING FUNCTION roll_dice...\u001b[0m\n",
      "\u001b[33muser_proxy\u001b[0m (to assistant):\n",
      "\n",
      "\u001b[33muser_proxy\u001b[0m (to assistant):\n",
      "\n",
      "\u001b[32m***** Response from calling tool (call_FCLl3UffiTOv7FDbZHjCAd1V) *****\u001b[0m\n",
      "3, 2, 1\n",
      "\u001b[32m**********************************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33massistant\u001b[0m (to user_proxy):\n",
      "\n",
      "The result of rolling 3 dice is: 3, 2, 1.\n",
      "\n",
      "TERMINATE\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "response = user_proxy.initiate_chat(assistant, message=\"Roll 3 dice\", max_turns=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise: Build your own agent"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
