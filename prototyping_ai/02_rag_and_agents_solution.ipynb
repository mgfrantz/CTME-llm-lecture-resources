{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval Augmented Generation\n",
    "\n",
    "Overview:\n",
    "- Introduction to RAG with LlamaIndex\n",
    "- Data ingestion\n",
    "  - PDF\n",
    "  - Web pages\n",
    "  - Code\n",
    "- Data splitting\n",
    "  - Token splitting\n",
    "  - Sentence splitting\n",
    "  - Structured data splitting\n",
    "  - Semantic chunking\n",
    "- Vectorization\n",
    "  - Embeddings\n",
    "  - Vector storage\n",
    "- Retrieval\n",
    "  - Keyword search\n",
    "  - Vector search\n",
    "  - Hybrid search\n",
    "- Advanced methods\n",
    "  - Query rewriting\n",
    "  - Multi-hop retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to RAG with LlamaIndex\n",
    "\n",
    "LlamaIndex is a library for working with large language models.\n",
    "One of its main strengths is its ability to ingest documents into a vector index and use them to answer questions.\n",
    "This is known as Retrieval Augmented Generation (RAG).\n",
    "\n",
    "To start, we will use a low-code, high-level abstraction to build a basic PDF question-answering system.\n",
    "We will read in PDFs, split them into chunks, embed them, and store them in a vector database.\n",
    "Then, we will use an abstraction known as a `QueryEngine` that implements RAG to answer questions about the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Colab detected - using colab secrets.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Colab detected - using colab secrets.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Not in colab - using local environment variables.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Not in colab - using local environment variables.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# If we're in colab, use userdata to get the OPENAI_API_KEY\n",
    "import os\n",
    "from rich import print\n",
    "from pathlib import Path\n",
    "\n",
    "try:\n",
    "    print(\"Colab detected - using colab secrets.\")\n",
    "    from google.colab import userdata\n",
    "    os.environ[\"OPENAI_API_KEY\"] = userdata.get(\"OPENAI_API_KEY\")\n",
    "except:\n",
    "    print(\"Not in colab - using local environment variables.\")\n",
    "    from dotenv import load_dotenv\n",
    "    load_dotenv(\"../.env\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">PDF already exists at data/<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2407.21783</span>.pdf\n",
       "</pre>\n"
      ],
      "text/plain": [
       "PDF already exists at data/\u001b[1;36m2407.21783\u001b[0m.pdf\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "\n",
    "# Create data directory if it doesn't exist\n",
    "data_dir = \"data\"\n",
    "if not os.path.exists(data_dir):\n",
    "    os.makedirs(data_dir)\n",
    "\n",
    "# Download the PDF file\n",
    "pdf_url = \"https://arxiv.org/pdf/2407.21783\"\n",
    "pdf_path = os.path.join(data_dir, \"2407.21783.pdf\")\n",
    "\n",
    "if not os.path.exists(pdf_path):\n",
    "    response = requests.get(pdf_url)\n",
    "    with open(pdf_path, \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "    print(f\"Downloaded PDF to {pdf_path}\")\n",
    "else:\n",
    "    print(f\"PDF already exists at {pdf_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.readers import SimpleDirectoryReader\n",
    "documents = SimpleDirectoryReader(data_dir).load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from torch.backends.mps import is_available as is_mps_available\n",
    "from torch.cuda import is_available as is_cuda_available\n",
    "\n",
    "if is_mps_available():\n",
    "    device = \"mps\"\n",
    "elif is_cuda_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-large-en-v1.5\", device=device)\n",
    "index = VectorStoreIndex.from_documents(documents, embed_model=embed_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "llm = OpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "query_engine = index.as_query_engine(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_engine.query(\"How many new Llama models models are mentioned in the paper?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Create a Gradio interface for a question-answering system\n",
    "\n",
    "Your goal in this exercise is to create a Gradio interface for a question-answering system.\n",
    "Your application should:\n",
    "- Use the query engine created above to answer questions about the uploaded PDF\n",
    "- Display the question and answer in the UI\n",
    "\n",
    "If you need a challenge:\n",
    "- Use the `gr.File` component to allow the user to upload ANY pdf and ask question about it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tempfile import TemporaryDirectory\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "def ingest_documents(file_obj):\n",
    "    \"\"\"\n",
    "    Process an uploaded PDF file and create a query engine for it.\n",
    "    \n",
    "    Args:\n",
    "        file_obj: Gradio file upload object containing the PDF\n",
    "        \n",
    "    Returns:\n",
    "        QueryEngine: Configured query engine for the ingested document\n",
    "    \"\"\"\n",
    "    if file_obj is None:\n",
    "        return None\n",
    "        \n",
    "    with TemporaryDirectory() as temp_dir:\n",
    "        file_path = os.path.join(temp_dir, 'tmp.pdf')\n",
    "        # Get the file bytes from the Gradio upload object\n",
    "        file_bytes = open(file_obj, \"rb\").read()\n",
    "        \n",
    "        with open(file_path, \"wb\") as f:\n",
    "            f.write(file_bytes)\n",
    "            \n",
    "        documents = SimpleDirectoryReader(temp_dir).load_data()\n",
    "        llm = OpenAI(model=\"gpt-4o-mini\")\n",
    "        index = VectorStoreIndex.from_documents(documents, embed_model=embed_model)\n",
    "        query_engine = index.as_query_engine(llm=llm)\n",
    "        \n",
    "    return query_engine, None, Path(file_obj.name).name\n",
    "\n",
    "def predict(query_engine, question):\n",
    "    return query_engine.query(question)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gr.Blocks() as demo:\n",
    "    query_engine = gr.State()\n",
    "    gr.Markdown(\"## RAG Demo: Question answering with a PDF\")\n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=1):   \n",
    "            file_upload = gr.File(label=\"Upload a PDF file\", file_types=[\".pdf\"], file_count=\"single\")\n",
    "            submit_button = gr.Button(\"Submit\")\n",
    "            pdf_name = gr.Textbox(label=\"You are asking about...\")\n",
    "        with gr.Column(scale=3):\n",
    "            input = gr.Textbox(label=\"Enter a question\")\n",
    "            output = gr.Textbox(label=\"Answer\")\n",
    "\n",
    "    submit_button.click(fn=ingest_documents, inputs=file_upload, outputs=[query_engine, file_upload, pdf_name])\n",
    "    input.submit(fn=predict, inputs=[query_engine, input], outputs=output)\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data ingestion\n",
    "\n",
    "Data often comes in many different formats.\n",
    "It may come in the form of a PDF, a web page, a code file, etc.\n",
    "We may need some specific processing pipelines to extract the text from these documents, split them correctly, and vectorize them.\n",
    "\n",
    "Luckily, LlamaIndex (and other libraries) provide lots of built-in and add-on tools to help you ingest almost any data type.\n",
    "Instead of loading a PDF, let's load a web page instead.\n",
    "We will use one of the classes provided by [`llama-index-readers-web`](https://llamahub.ai/l/readers/llama-index-readers-web?from=readers) to load data from a web page.\n",
    "\n",
    "In this section, we will:\n",
    "- Load a web page as Markdown\n",
    "- Split it into chunks following the structured format of the Markdown\n",
    "- Embed the chunks\n",
    "- Store the chunks in a vector database\n",
    "- Create a query engine from the vector database and use it to answer a question\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.readers.web import SimpleWebPageReader\n",
    "from llama_index.core.node_parser import MarkdownNodeParser\n",
    "from llama_index.core.ingestion import IngestionPipeline\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "import chromadb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "web_docs = SimpleWebPageReader(html_to_text=True).load_data(['https://en.wikipedia.org/wiki/Wikipedia'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "chromadb.EphemeralClient().delete_collection(\"wikipedia\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = chromadb.EphemeralClient().create_collection(\"wikipedia\", get_or_create=True)\n",
    "vector_store = ChromaVectorStore(collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = IngestionPipeline(\n",
    "    transformations=[\n",
    "        MarkdownNodeParser.from_defaults(),\n",
    "        embed_model,\n",
    "    ], \n",
    "    vector_store=vector_store\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = pipeline.run(documents=web_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = VectorStoreIndex.from_vector_store(vector_store, embed_model=embed_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">There are currently <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">339</span> language editions of Wikipedia.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "There are currently \u001b[1;36m339\u001b[0m language editions of Wikipedia.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "llm = OpenAI(model=\"gpt-4o-mini\")\n",
    "query_engine = index.as_query_engine(llm=llm)\n",
    "response = query_engine.query(\"How many languages there exactly? Quote the exact text as well.\")\n",
    "print(response.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Quote from source: <span style=\"color: #008000; text-decoration-color: #008000\">'There are currently 339 language editions of Wikipedia (also called _language'</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Quote from source: \u001b[32m'There are currently 339 language editions of Wikipedia \u001b[0m\u001b[32m(\u001b[0m\u001b[32malso called _language'\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Use fuzzywuzzy to find the closest match in the source text\n",
    "from thefuzz import fuzz, process\n",
    "# Get the top matching line of text from the source_text_quote\n",
    "top_match, match_score = process.extractOne(response.response, response.source_nodes[0].text.splitlines(), scorer=fuzz.ratio)\n",
    "assert top_match in response.source_nodes[0].text\n",
    "print(f\"Quote from source: '{top_match}'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
